import os
import time
import logging
import argparse
import numpy as np
# import cv2 as cv
import trimesh
import torch
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
# try:
#     from torch.utils.tensorboard import SummaryWriter
# except:
#     SummaryWriter = None
#     pass
from shutil import copyfile
from icecream import ic
from tqdm import tqdm
from pyhocon import ConfigFactory
# from models.dataset import Dataset
# from models.dataset_wtime import Dataset
from models.fields import RenderingNetwork, SDFNetwork, SingleVarianceNetwork, NeRF, BendingNetwork
# from models.renderer import NeuSRenderer
# from models.renderer_def import NeuSRenderer
# from models.renderer_def_multi_objs import NeuSRenderer
# from models.renderer_def_multi_objs_compositional import NeuSRenderer
import models.data_utils_torch as data_utils
# import models.dyn_model_utils as dyn_utils
import torch.nn as nn
# import models.renderer_def_multi_objs as render_utils
import models.fields as fields
# from torch.distributions.uniform import Uniform
from torch.distributions.categorical import Categorical
# import random
try:
    import redmax_py as redmax
except:
    pass
import open3d as o3d
import models.dyn_model_act as dyn_model_act
import models.dyn_model_act_v2 as dyn_model_act_mano
from scipy.spatial.transform import Rotation as R
import models.dyn_model_act_v2_deformable as dyn_model_act_mano_deformable
import engine.sim_modules as sim_modules
import traceback

import pickle as pkl

##
def load_GT_vertices(GT_meshes_folder):
    tot_meshes_fns = os.listdir(GT_meshes_folder)
    tot_meshes_fns = [fn for fn in tot_meshes_fns if fn.endswith(".obj")]
    tot_mesh_verts = []
    for fn in tot_meshes_fns:
        cur_mesh_fn = os.path.join(GT_meshes_folder, fn)
        obj_mesh = trimesh.load(cur_mesh_fn, process=False)
        verts_obj = np.array(obj_mesh.vertices)
        tot_mesh_verts.append(verts_obj)
    tot_mesh_verts = np.concatenate(tot_mesh_verts, axis=0)
    return tot_mesh_verts


def plane_rotation_matrix_from_angle_xz(angle):
    sin_ = torch.sin(angle)
    cos_ = torch.cos(angle)
    zero_padding = torch.zeros_like(cos_)
    one_padding = torch.ones_like(cos_)
    col_a = torch.stack(
        [cos_, zero_padding, sin_], dim=0
    )
    col_b = torch.stack(
        [zero_padding, one_padding, zero_padding], dim=0
    )
    col_c = torch.stack(
        [-1. * sin_, zero_padding, cos_], dim=0
    )
    rot_mtx = torch.stack(
        [col_a, col_b, col_c], dim=-1
    )
    return rot_mtx


# # runner # # # runner # 
class Runner:
    def __init__(self, conf_path, mode='train', case='CASE_NAME', is_continue=False):
        self.device = torch.device('cuda')

        # if 'ks' in conf_path:
        #     import taichi as ti
            
        #     ti.init(arch=ti.cuda, device_memory_GB=40.0)
            
        #     # uuu = ti.field(dtype=ti.f32, shape=(4e10, 1))
            
        #     uuu = ti.field(ti.f32)
        #     ti.root.dense(ti.i, 4e5).place(uuu)
        
        # Configuration
        self.conf_path = conf_path
        f = open(self.conf_path)
        conf_text = f.read()
        conf_text = conf_text.replace('CASE_NAME', case)
        f.close()

        self.conf = ConfigFactory.parse_string(conf_text)
        self.conf['dataset.data_dir'] = self.conf['dataset.data_dir'].replace('CASE_NAME', case)
        self.base_exp_dir = self.conf['general.base_exp_dir']
        
        local_exp_dir = "/data2/xueyi/quasisim/exp/"
        if os.path.exists(local_exp_dir):
            self.base_exp_dir = local_exp_dir
        
        
        self.base_exp_dir = self.base_exp_dir + f"_reverse_value_totviews_tag_{self.conf['general.tag']}"
        os.makedirs(self.base_exp_dir, exist_ok=True)
        # self.dataset = Dataset(self.conf['dataset']) # base exp dirs # # base exp dirs # ---- base exp dirs # # base exp dirs # 
        
        # self.n_timesteps = 10
        self.n_timesteps = self.conf['model.n_timesteps'] ## n_timesteps -> give the n_timesteps # 
        # self.time_idx_to_dataset = {}
        # for i_time_idx in range(self.n_timesteps):
        #     cur_time_dataset = Dataset(self.conf['dataset'], time_idx=i_time_idx, mode=mode)
        #     self.time_idx_to_dataset[i_time_idx] = cur_time_dataset
        
        
        self.iter_step = 0

        # Training parameters # # NeuS #
        self.end_iter = self.conf.get_int('train.end_iter')
        self.save_freq = self.conf.get_int('train.save_freq')
        self.report_freq = self.conf.get_int('train.report_freq')
        self.val_freq = self.conf.get_int('train.val_freq')
        self.val_mesh_freq = self.conf.get_int('train.val_mesh_freq')
        self.batch_size = self.conf.get_int('train.batch_size')
        self.validate_resolution_level = self.conf.get_int('train.validate_resolution_level')
        self.learning_rate = self.conf.get_float('train.learning_rate')
        self.learning_rate_alpha = self.conf.get_float('train.learning_rate_alpha')
        self.learning_rate_actions = self.conf.get_float('train.learning_rate_actions', default=self.learning_rate)
        self.use_white_bkgd = self.conf.get_bool('train.use_white_bkgd')
        self.warm_up_end = self.conf.get_float('train.warm_up_end', default=0.0)
        self.anneal_end = self.conf.get_float('train.anneal_end', default=0.0)
        
        self.use_bending_network = True
        # use_split_network
        self.use_selector = True


        # Weights # 
        self.igr_weight = self.conf.get_float('train.igr_weight') # what is igr_weight? # # what is igr_weight # 
        self.mask_weight = self.conf.get_float('train.mask_weight') # what is mask weight? # # perhaps.. #
        self.is_continue = is_continue # 
        self.mode = mode
        self.model_list = []
        self.writer = None
        
        # self.bending_latent = nn.Embedding(
        #     num_embeddings=self.bending_n_timesteps, embedding_dim=self.bending_latent_size
        # ) # 
        self.bending_latent_size = self.conf['model.bending_network']['bending_latent_size']
        # self.bending_latent_size = 

        # Networks
        params_to_train = []
        self.nerf_outside = NeRF(**self.conf['model.nerf']).to(self.device)
        self.sdf_network = SDFNetwork(**self.conf['model.sdf_network']).to(self.device)
        self.deviation_network = SingleVarianceNetwork(**self.conf['model.variance_network']).to(self.device)
        self.color_network = RenderingNetwork(**self.conf['model.rendering_network']).to(self.device) # rendering network # 
        
        # self.use_bending_network = self.conf['model.use_bending_network']
        # # bending network size #
        # if self.use_bending_network:  # add the bendingnetwork #
        self.bending_network = BendingNetwork(**self.conf['model.bending_network']).to(self.device)
        
        
        self.use_split_network = self.conf.get_bool('model.use_split_network', False)
        if self.use_split_network:
            self.bending_network.set_split_bending_network()
        self.bending_network.n_timesteps = self.n_timesteps
        
        
        self.extract_delta_mesh = self.conf['model.extract_delta_mesh']
        
        
        # # bending_net_type = "pts_def" # 
        # bending_net_type = "rigid_acc" # 
        
        self.use_passive_nets = self.conf['model.use_passive_nets']
        if 'model.bending_net_type' in self.conf:
            self.bending_net_type = self.conf['model.bending_net_type']
        else:
            self.bending_net_type = "pts_def"
        
        
        if 'model.train_multi_seqs' in self.conf and self.conf['model.train_multi_seqs']:
            self.rhand_verts, self.hand_faces, self.obj_faces, self.obj_normals, self.ts_to_contact_pts, self.hand_verts = self.load_active_passive_timestep_to_mesh_multi_seqs()
            self.train_multi_seqs = True
            self.nn_instances = len(self.rhand_verts)
        else:
            self.train_multi_seqs = False
            self.nn_instances = 1
            
        if 'model.minn_dist_threshold' in self.conf:
            self.minn_dist_threshold = self.conf['model.minn_dist_threshold']
        else:
            self.minn_dist_threshold = 0.05
            
        if 'model.optimize_with_intermediates' in self.conf:
            self.optimize_with_intermediates = self.conf['model.optimize_with_intermediates']
        else:
            self.optimize_with_intermediates = False
            
        if 'model.no_friction_constraint' in self.conf:
            self.no_friction_constraint = self.conf['model.no_friction_constraint']
        else:
            self.no_friction_constraint = False
            
        if 'model.optimize_active_object' in self.conf:
            self.optimize_active_object = self.conf['model.optimize_active_object']
        else:
            self.optimize_active_object = False
            
        if 'model.optimize_glb_transformations' in self.conf:
            self.optimize_glb_transformations = self.conf['model.optimize_glb_transformations']
        else:
            self.optimize_glb_transformations = False
            
        if 'model.with_finger_tracking_loss' in self.conf:
            self.with_finger_tracking_loss = self.conf['model.with_finger_tracking_loss']
        else:
            self.with_finger_tracking_loss = True


        #### loss types ###
        # finger_cd_loss_coef, finger_tracking_loss_coef, tracking_loss_coef, penetrating_depth_penalty_coef # 
        if 'model.finger_cd_loss' in self.conf:
            self.finger_cd_loss_coef = self.conf['model.finger_cd_loss']
        else:
            self.finger_cd_loss_coef = 0.

        if 'model.finger_tracking_loss' in self.conf:
            self.finger_tracking_loss_coef = self.conf['model.finger_tracking_loss']
        else:
            self.finger_tracking_loss_coef = 0.
        
        if 'model.tracking_loss' in self.conf:
            self.tracking_loss_coef = self.conf['model.tracking_loss']
        else:
            self.tracking_loss_coef = 0.
            
        if 'model.penetrating_depth_penalty' in self.conf:
            self.penetrating_depth_penalty_coef = self.conf['model.penetrating_depth_penalty']
        else:
            self.penetrating_depth_penalty_coef = 0.
            
            
        if 'model.ragged_dist' in self.conf:
            self.ragged_dist_coef = self.conf['model.ragged_dist']
        else:
            self.ragged_dist_coef = 1.


        if 'mode.load_only_glb' in self.conf:
            self.load_only_glb = self.conf['model.load_only_glb']
        else:
            self.load_only_glb = False
            
        # optimize_rules; optimize_robot # 
        if 'model.optimize_robot' in self.conf:
            self.optimize_robot = self.conf['model.optimize_robot']
        else:
            self.optimize_robot = True
            
        if 'model.optimize_rules' in self.conf:
            self.optimize_rules = self.conf['model.optimize_rules']
        else:
            self.optimize_rules = False
            
            
        if 'model.optimize_expanded_pts' in self.conf:
            # print(f"Setting optimizing expanded pts...") # optimize expanded pts or optimize 
            # params_to_train = []
            # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
            # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
            # self.optimize_expanded_pts = True
            self.optimize_expanded_pts = self.conf['model.optimize_expanded_pts']
        else:
            self.optimize_expanded_pts = True
            
        if 'model.optimize_expanded_ragged_pts' in self.conf:
            self.optimize_expanded_ragged_pts = self.conf['model.optimize_expanded_ragged_pts']
        else:
            self.optimize_expanded_ragged_pts = False
        
        if 'model.add_delta_state_constraints' in self.conf:
            self.add_delta_state_constraints = self.conf['model.add_delta_state_constraints']
        else:
            self.add_delta_state_constraints = True
            
        # 
        if 'model.train_actions_with_states' in self.conf:
            self.train_actions_with_states = self.conf['model.train_actions_with_states']
        else:
            self.train_actions_with_states = False
            
            
        if 'model.train_with_forces_to_active' in self.conf:
            self.train_with_forces_to_active = self.conf['model.train_with_forces_to_active']
        else:
            self.train_with_forces_to_active = False
            
            
        if 'model.loss_weight_diff_states' in self.conf: # conf #
            self.loss_weight_diff_states = self.conf['model.loss_weight_diff_states']
        else:
            self.loss_weight_diff_states = 1.
            
        if 'model.loss_tangential_diff_coef' in self.conf:
            self.loss_tangential_diff_coef = float(self.conf['model.loss_tangential_diff_coef'])
        else:
            self.loss_tangential_diff_coef = 1.
            
        if 'model.use_penalty_based_friction' in self.conf:
            self.use_penalty_based_friction = self.conf['model.use_penalty_based_friction']
        else:
            self.use_penalty_based_friction = False
            
        if 'model.use_disp_based_friction' in self.conf:
            self.use_disp_based_friction = self.conf['model.use_disp_based_friction']
        else:
            self.use_disp_based_friction = False 
            
        if 'model.use_sqrt_dist' in self.conf:
            self.use_sqrt_dist = self.conf['model.use_sqrt_dist']
        else:
            self.use_sqrt_dist = False
            
        if 'model.reg_loss_coef' in self.conf:
            self.reg_loss_coef = float(self.conf['model.reg_loss_coef'])
        else:
            self.reg_loss_coef = 0.
            
        if 'model.contact_maintaining_dist_thres' in self.conf:
            self.contact_maintaining_dist_thres = float(self.conf['model.contact_maintaining_dist_thres'])
        else:
            self.contact_maintaining_dist_thres = 0.1
            
        if 'model.penetration_proj_k_to_robot' in self.conf:
            self.penetration_proj_k_to_robot = float(self.conf['model.penetration_proj_k_to_robot'])
        else:
            self.penetration_proj_k_to_robot = 0.0
            
        if 'model.use_mano_inputs' in self.conf:
            self.use_mano_inputs = self.conf['model.use_mano_inputs']
        else:
            self.use_mano_inputs = False
            
        
        if 'model.use_split_params' in self.conf:
            self.use_split_params = self.conf['model.use_split_params']
        else:
            self.use_split_params = False
            
        
        if 'model.use_split_params' in self.conf:
            self.use_split_params = self.conf['model.use_split_params']
        else:
            self.use_split_params = False
            
        
        if 'model.use_sqr_spring_stiffness' in self.conf:
            self.use_sqr_spring_stiffness = self.conf['model.use_sqr_spring_stiffness']
        else:
            self.use_sqr_spring_stiffness = False
            
        
        if 'model.use_pre_proj_frictions' in self.conf:
            self.use_pre_proj_frictions = self.conf['model.use_pre_proj_frictions']
        else:
            self.use_pre_proj_frictions = False
            
        if 'model.use_static_mus' in self.conf:
            self.use_static_mus = self.conf['model.use_static_mus']
        else:
            self.use_static_mus = False
        
        if 'model.contact_friction_static_mu' in self.conf:
            self.contact_friction_static_mu = self.conf['model.contact_friction_static_mu']
        else:
            self.contact_friction_static_mu = 1.0
            
        if 'model.debug' in self.conf:
            self.debug = self.conf['model.debug']
        else:
            self.debug  = False
        
        if 'model.robot_actions_diff_coef' in self.conf:
            self.robot_actions_diff_coef = self.conf['model.robot_actions_diff_coef']
        else:
            self.robot_actions_diff_coef = 0.1
        
        if 'model.use_sdf_as_contact_dist' in self.conf:
            self.use_sdf_as_contact_dist = self.conf['model.use_sdf_as_contact_dist']
        else:
            self.use_sdf_as_contact_dist = False
        
        if 'model.use_contact_dist_as_sdf' in self.conf:
            self.use_contact_dist_as_sdf = self.conf['model.use_contact_dist_as_sdf']
        else:
            self.use_contact_dist_as_sdf = False
        
        if 'model.use_same_contact_spring_k' in self.conf:
            self.use_same_contact_spring_k = self.conf['model.use_same_contact_spring_k']
        else:
            self.use_same_contact_spring_k = False
        
        if 'model.minn_dist_threshold_robot_to_obj' in self.conf:
            self.minn_dist_threshold_robot_to_obj = float(self.conf['model.minn_dist_threshold_robot_to_obj'])
        else:
            self.minn_dist_threshold_robot_to_obj = 0.0
            
        if 'model.obj_mass' in self.conf:
            self.obj_mass = float(self.conf['model.obj_mass'])
        else:
            self.obj_mass = 100.0
            
        if 'model.diff_hand_tracking_coef' in self.conf:
            self.diff_hand_tracking_coef = float(self.conf['model.diff_hand_tracking_coef'])
        else: # 
            self.diff_hand_tracking_coef = 0.0
            
        if 'model.use_mano_hand_for_test' in self.conf:
            self.use_mano_hand_for_test = self.conf['model.use_mano_hand_for_test']
        else:
            self.use_mano_hand_for_test = False
        
        if 'model.train_residual_friction' in self.conf:
            self.train_residual_friction = self.conf['model.train_residual_friction']
        else:
            self.train_residual_friction = False
            
        if 'model.use_LBFGS' in self.conf:
            self.use_LBFGS = self.conf['model.use_LBFGS']
        else:
            self.use_LBFGS = False
            
        if 'model.use_optimizable_params' in self.conf:
            self.use_optimizable_params = self.conf['model.use_optimizable_params']
        else:
            self.use_optimizable_params = False
             
        if 'model.penetration_determining' in self.conf:
            self.penetration_determining = self.conf['model.penetration_determining']
        else:
            self.penetration_determining = "sdf_of_canon"
            
        if 'model.sdf_sv_fn' in self.conf:
            self.sdf_sv_fn = self.conf['model.sdf_sv_fn']
        else:
            self.sdf_sv_fn = None
            
        if 'model.loss_scale_coef' in self.conf:
            self.loss_scale_coef = float(self.conf['model.loss_scale_coef'])
        else:
            self.loss_scale_coef = 1.0
        
        if 'model.penetration_proj_k_to_robot_friction' in self.conf:
            self.penetration_proj_k_to_robot_friction = float(self.conf['model.penetration_proj_k_to_robot_friction'])
        else:
            self.penetration_proj_k_to_robot_friction = self.penetration_proj_k_to_robot
            
        if 'model.retar_only_glb' in self.conf:
            self.retar_only_glb = self.conf['model.retar_only_glb']
        else:
            self.retar_only_glb = False
            
        if 'model.optim_sim_model_params_from_mano' in self.conf:
            self.optim_sim_model_params_from_mano = self.conf['model.optim_sim_model_params_from_mano']
        else:
            self.optim_sim_model_params_from_mano = False
            
            
        # opt_robo_states, opt_robo_glb_trans, opt_robo_glb_rot #
        if 'model.opt_robo_states' in self.conf:
            self.opt_robo_states = self.conf['model.opt_robo_states']
        else:
            self.opt_robo_states = True
            
        if 'model.opt_robo_glb_trans' in self.conf:
            self.opt_robo_glb_trans = self.conf['model.opt_robo_glb_trans']
        else:
            self.opt_robo_glb_trans = False
            
        if 'model.opt_robo_glb_rot' in self.conf:
            self.opt_robo_glb_rot = self.conf['model.opt_robo_glb_rot'] 
        else:
            self.opt_robo_glb_rot = False
            
        # motion_reg_loss_coef
        
        if 'model.motion_reg_loss_coef' in self.conf:
            self.motion_reg_loss_coef = self.conf['model.motion_reg_loss_coef'] 
        else:
            self.motion_reg_loss_coef = 1.0
            
        if 'model.drive_robot' in self.conf:
            self.drive_robot = self.conf['model.drive_robot']
        else:
            self.drive_robot = 'states'
            
        if 'model.use_scaled_urdf' in self.conf:
            self.use_scaled_urdf =self.conf['model.use_scaled_urdf']
        else:
            self.use_scaled_urdf = False
            
        if 'model.window_size' in self.conf:
            self.window_size = self.conf['model.window_size']
        else:
            self.window_size = 60
            
        if 'model.use_taco' in self.conf:
            self.use_taco = self.conf['model.use_taco']
        else:
            self.use_taco = False
            
        if 'model.ang_vel_damping' in self.conf:
            self.ang_vel_damping = float(self.conf['model.ang_vel_damping'])
        else:
            self.ang_vel_damping = 0.0
            
        if 'model.drive_glb_delta' in self.conf:
            self.drive_glb_delta = self.conf['model.drive_glb_delta']
        else:
            self.drive_glb_delta = False
            
        if 'model.fix_obj' in self.conf:
            self.fix_obj = self.conf['model.fix_obj']
        else:
            self.fix_obj = False
            
        if 'model.diff_reg_coef' in self.conf:
            self.diff_reg_coef = self.conf['model.diff_reg_coef']
        else:
            self.diff_reg_coef = 0.01
            
        if 'model.use_damping_params_vel' in self.conf:
            self.use_damping_params_vel = self.conf['model.use_damping_params_vel']
        else:
            self.use_damping_params_vel = False
        
        if 'train.ckpt_sv_freq' in self.conf:
            self.ckpt_sv_freq = int(self.conf['train.ckpt_sv_freq'])
        else:
            self.ckpt_sv_freq = 100
        
        # optm_alltime_ks
        #  # optimizable_spring_ks_normal, optimizable_spring_ks_friction #
        if 'model.optm_alltime_ks' in self.conf:
            self.optm_alltime_ks = self.conf['model.optm_alltime_ks']
        else:
            self.optm_alltime_ks = False
            
        if 'model.retar_dense_corres' in self.conf:
            self.retar_dense_corres = self.conf['model.retar_dense_corres']
        else:
            self.retar_dense_corres = False
            
        # retar_delta_glb_trans
            
        if 'model.retar_delta_glb_trans' in self.conf:
            self.retar_delta_glb_trans = self.conf['model.retar_delta_glb_trans']
        else:
            self.retar_delta_glb_trans = False
            
        if 'model.use_multi_stages' in self.conf:
            self.use_multi_stages = self.conf['model.use_multi_stages']
        else:
            self.use_multi_stages = False
            
        if 'model.seq_start_idx' in self.conf:
            self.seq_start_idx = self.conf['model.seq_start_idx']
        else:
            self.seq_start_idx = 40
            
        self.minn_init_passive_mesh = None
        self.maxx_init_passive_mesh = None
        
        
        if 'model.optimize_dyn_actions' in self.conf:
            self.optimize_dyn_actions = self.conf['model.optimize_dyn_actions']
        else:
            self.optimize_dyn_actions = False
            
        print(f"optimize_dyn_actions: {self.optimize_dyn_actions}")
            
        if 'dataset.obj_idx' in self.conf:
            print(f"dataset.obj_idx:", self.conf['dataset.obj_idx'])
            self.obj_idx = self.conf['dataset.obj_idx']
            
            ###### only for the grab dataset only currently ########
            GRAB_data_root = "/data1/xueyi/GRAB_extracted_test/train"
            # /data/xueyi/GRAB/GRAB_extracted_test/train/102_obj.npy
            if not os.path.exists(GRAB_data_root):
                GRAB_data_root = "/data/xueyi/GRAB/GRAB_extracted_test/train"
            
            # self.conf['model.obj_sdf_fn'] = os.path.join(GRAB_data_root, f"{self.obj_idx}_obj.npy")
            # self.conf['model.kinematic_mano_gt_sv_fn'] = os.path.join(GRAB_data_root, f"{self.obj_idx}_sv_dict.npy")
            # self.conf['model.scaled_obj_mesh_fn'] = os.path.join(GRAB_data_root, f"{self.obj_idx}_obj.obj")
            # self.conf['model.ckpt_fn'] = ""
            # self.conf['model.load_optimized_init_transformations'] = ""
            
            ## grab data root ##
            
            self.obj_sdf_fn = os.path.join(GRAB_data_root, f"{self.obj_idx}_obj.npy")
            self.kinematic_mano_gt_sv_fn =  os.path.join(GRAB_data_root, f"{self.obj_idx}_sv_dict.npy")
            self.scaled_obj_mesh_fn = os.path.join(GRAB_data_root, f"{self.obj_idx}_obj.obj")
            self.ckpt_fn =  self.conf['model.ckpt_fn']
            self.load_optimized_init_transformations =  self.conf['model.load_optimized_init_transformations']
            
            print(f"obj_sdf_fn:", self.obj_sdf_fn)
            print(f"kinematic_mano_gt_sv_fn:", self.kinematic_mano_gt_sv_fn)
            print(f"scaled_obj_mesh_fn:", self.scaled_obj_mesh_fn)
            
        
        
        self.mano_nn_substeps = 1
        
        self.canon_passive_obj_verts = None
        self.canon_passive_obj_normals = None
        # active_force_field_v15
        if self.bending_net_type == "active_force_field_v18":
            self.other_bending_network = fields.BendingNetworkActiveForceFieldForwardLagV18(**self.conf['model.bending_network'], nn_instances=self.nn_instances, minn_dist_threshold=self.minn_dist_threshold).to(self.device)
            
            
            if mode in ["train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states_grab", "train_expanded_set_motions", "train_finger_retargeting", "train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab", "train_expanded_set_motions_retar", "train_finger_kinematics_retargeting_arctic_twohands", "train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_arctic_twohands", "train_diffhand_model", "train_manip_acts_params", "train_redmax_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab", "train_dyn_mano_model_states"]:
                
                
                # object orienation ##
                # self.load_active_passive_timestep_to_mesh()
                
                if mode in ['train_finger_kinematics_retargeting_arctic_twohands', 'train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_arctic_twohands']:
                    self.timestep_to_passive_mesh, self.timestep_to_active_mesh, self.timestep_to_passive_mesh_normals = self.load_active_passive_timestep_to_mesh_twohands_arctic()
                else:
                    if self.use_taco:
                        self.timestep_to_passive_mesh, self.timestep_to_active_mesh, self.timestep_to_passive_mesh_normals = self.load_active_passive_timestep_to_mesh_v3_taco()
                    else:
                        self.timestep_to_passive_mesh, self.timestep_to_active_mesh, self.timestep_to_passive_mesh_normals = self.load_active_passive_timestep_to_mesh_v3()
                
                if self.conf['model.penetration_determining'] == "ball_primitives":
                    self.center_verts, self.ball_r = self.get_ball_primitives()
                
                
                ## load obj sdf grad #
                ## set something in the bending network field ##
                self.other_bending_network.canon_passive_obj_verts = self.obj_verts
                self.other_bending_network.canon_passive_obj_normals = self.obj_normals
                
                self.canon_passive_obj_verts = self.obj_verts
                self.canon_passive_obj_normals = self.obj_normals
                
                # self.other_bending_network.canon_passive_obj_verts = None
                # self.other_bending_network.canon_passive_obj_normals = None
                
                # self.canon_passive_obj_verts = None
                # self.canon_passive_obj_normals = None 
                
                
                # tot_obj_quat, tot_reversed_obj_rot_mtx #
                self.obj_sdf_fn = self.conf['model.obj_sdf_fn'] ## get the sdf file of the initial verts here ##
                self.other_bending_network.sdf_space_center = self.sdf_space_center
                self.other_bending_network.sdf_space_scale = self.sdf_space_scale
                self.obj_sdf = np.load(self.obj_sdf_fn, allow_pickle=True)
                # self.obj_sdf = torch.from_numpy(self.obj_sdf).float() ## 
                self.sdf_res = self.obj_sdf.shape[0]
                self.other_bending_network.obj_sdf = self.obj_sdf
                self.other_bending_network.sdf_res = self.sdf_res
                
                
                if self.conf['model.penetration_determining'] == "sdf_of_canon":
                    print(f"Setting the penetration determining method to sdf_of_canon")
                    self.other_bending_network.penetration_determining = "sdf_of_canon" ## sdf of canon ##
                elif self.conf['model.penetration_determining'] == 'plane_primitives':
                    # print(f"Setting the penetration determining method to plane_primitives with maxx_xyz: {self.maxx_init_passive_mesh}, minn_xyz: {self.minn_init_passive_mesh}")
                    ## penetration determining ... # 
                    print(f"setting the penetration determining method to plane_primitives with maxx_xyz: {self.maxx_init_passive_mesh}, minn_xyz: {self.minn_init_passive_mesh}")
                    self.other_bending_network.penetration_determining = "plane_primitives" #
                elif self.conf['model.penetration_determining'] == 'ball_primitives':
                    print(f"Setting the penetration determining method to ball_primitives with ball_r: {self.ball_r}, center: {self.center_verts}")
                    self.other_bending_network.penetration_determining = "ball_primitives" #
                    self.other_bending_network.center_verts = self.center_verts
                    self.other_bending_network.ball_r = self.ball_r ## get the ball primitives here? ##
                else:
                    raise NotImplementedError(f"penetration determining method {self.conf['model.penetration_determining']} not implemented")
                
                
            elif mode == "train_dyn_mano_model_states": # ## active force field 18  # train 
                self.load_active_passive_timestep_to_mesh()
                self.timestep_to_passive_mesh, self.timestep_to_active_mesh, self.timestep_to_passive_mesh_normals = self.load_active_passive_timestep_to_mesh_v3()
                self.obj_sdf_grad = None
            else:
                
                if not self.train_multi_seqs:
                    self.timestep_to_passive_mesh, self.timestep_to_active_mesh, self.timestep_to_passive_mesh_normals = self.load_active_passive_timestep_to_mesh()
                    
                    # self.sdf_space_center, self.sdf_space_scale, self.obj_sdf #
                    self.other_bending_network.sdf_space_center = self.sdf_space_center
                    self.other_bending_network.sdf_space_scale = self.sdf_space_scale
                    self.other_bending_network.obj_sdf = self.obj_sdf # 
                    self.other_bending_network.sdf_res = self.sdf_res #
        # 
        ###### initialize the dyn model ######
        elif self.bending_net_type == "active_force_field_v14": 
            # timestep_to_passive_mesh_normals; if self.bending_net_type == 'active_force_field_v11'
            self.other_bending_network = fields.BendingNetworkActiveForceFieldForwardLagV14(**self.conf['model.bending_network']).to(self.device)
            ## 
            self.timestep_to_passive_mesh, self.timestep_to_active_mesh, self.timestep_to_passive_mesh_normals = self.load_active_passive_timestep_to_mesh()
        elif self.bending_net_type == "active_force_field_v13_lageu":
            self.other_bending_network = fields.BendingNetworkActiveForceFieldForwardLagEuV13(**self.conf['model.bending_network']).to(self.device)
            self.timestep_to_passive_mesh, self.timestep_to_active_mesh, self.timestep_to_passive_mesh_normals = self.load_active_passive_timestep_to_mesh()
            
            if mode in ['train_actions_from_model_rules', 'train_mano_actions_from_model_rules']:
                ### load grid forces from the checkpoint ###
                self.load_grid_forces_via_fn(self.conf['model.grid_forces_fn']) # to the self.ref_grid_forces a dict # # 

        if self.maxx_init_passive_mesh is None and self.minn_init_passive_mesh is None:
            self.calculate_collision_geometry_bounding_boxes()
            
            ###### initialize the dyn model ######
        for i_time_idx in range(self.n_timesteps):
            self.other_bending_network.timestep_to_vel[i_time_idx] = torch.zeros((3,), dtype=torch.float32).cuda()
            self.other_bending_network.timestep_to_point_accs[i_time_idx] = torch.zeros((3,), dtype=torch.float32).cuda()
            self.other_bending_network.timestep_to_total_def[i_time_idx] = torch.zeros((3,), dtype=torch.float32).cuda()
            self.other_bending_network.timestep_to_angular_vel[i_time_idx] = torch.zeros((3,), dtype=torch.float32).cuda()
            self.other_bending_network.timestep_to_quaternion[i_time_idx] = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda()
            self.other_bending_network.timestep_to_torque[i_time_idx] = torch.zeros((3,), dtype=torch.float32).cuda()
            
            # if self.fix_obj:

        # calculate_collision_geometry_bounding_boxes, self.maxx_init_passive_mesh, self.minn_init_passive_mesh #  # the best performed DGrasp-tracking? ##
        
        ### set initial transformations ###
        if mode in ["train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states_grab", "train_expanded_set_motions", "train_expanded_set_motions_retar", "train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab", "train_finger_kinematics_retargeting_arctic_twohands", "train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_arctic_twohands", "train_diffhand_model", "train_manip_acts_params", "train_redmax_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab"] and self.bending_net_type == "active_force_field_v18":
            # timestep_to_optimizable_rot_mtx, timestep_to_optimizable_total_def, timestep_to_optimizable_quaternion
            self.other_bending_network.timestep_to_total_def[0] = self.object_transl[0]
            self.other_bending_network.timestep_to_quaternion[0] = self.tot_obj_quat[0]
            self.other_bending_network.timestep_to_optimizable_offset[0] = self.object_transl[0].detach()
            self.other_bending_network.timestep_to_optimizable_quaternion[0] = self.tot_obj_quat[0].detach()
            self.other_bending_network.timestep_to_optimizable_rot_mtx[0] = self.tot_reversed_obj_rot_mtx[0].detach()
            self.other_bending_network.timestep_to_optimizable_total_def[0] = self.object_transl[0].detach()
            
            if self.fix_obj:
                for i_fr in range(self.object_transl.size(0)):
                    self.other_bending_network.timestep_to_total_def[i_fr] = self.object_transl[i_fr]
                    self.other_bending_network.timestep_to_quaternion[i_fr] = self.tot_obj_quat[i_fr]
                    self.other_bending_network.timestep_to_optimizable_offset[i_fr] = self.object_transl[i_fr].detach()
                    self.other_bending_network.timestep_to_optimizable_quaternion[i_fr] = self.tot_obj_quat[i_fr].detach()
                    self.other_bending_network.timestep_to_optimizable_rot_mtx[i_fr] = self.tot_reversed_obj_rot_mtx[i_fr].detach()
                    self.other_bending_network.timestep_to_optimizable_total_def[i_fr] = self.object_transl[i_fr].detach()
            


        self.calculate_obj_inertia()

        self.other_bending_network.use_penalty_based_friction = self.use_penalty_based_friction
        self.other_bending_network.use_disp_based_friction = self.use_disp_based_friction
        self.other_bending_network.use_sqrt_dist = self.use_sqrt_dist
        self.other_bending_network.contact_maintaining_dist_thres = self.contact_maintaining_dist_thres
        self.other_bending_network.penetration_proj_k_to_robot = self.penetration_proj_k_to_robot
        self.other_bending_network.use_split_params = self.use_split_params
        # self.other_bending_network.use_split_params = self.use_split_params
        self.other_bending_network.use_sqr_spring_stiffness = self.use_sqr_spring_stiffness
        self.other_bending_network.use_pre_proj_frictions = self.use_pre_proj_frictions
        self.other_bending_network.use_static_mus = self.use_static_mus
        self.other_bending_network.contact_friction_static_mu = self.contact_friction_static_mu
        self.other_bending_network.debug = self.debug
        self.obj_sdf_grad = None
        self.other_bending_network.obj_sdf_grad = self.obj_sdf_grad ## set obj_sdf #
        self.other_bending_network.use_sdf_as_contact_dist = self.use_sdf_as_contact_dist
        self.other_bending_network.use_contact_dist_as_sdf = self.use_contact_dist_as_sdf
        self.other_bending_network.minn_dist_threshold_robot_to_obj = self.minn_dist_threshold_robot_to_obj
        self.other_bending_network.use_same_contact_spring_k = self.use_same_contact_spring_k
        self.other_bending_network.I_ref = self.I_ref
        self.other_bending_network.I_inv_ref = self.I_inv_ref
        self.other_bending_network.obj_mass = self.obj_mass
        
        # self.maxx_init_passive_mesh, self.minn_init_passive_mesh
        self.other_bending_network.maxx_init_passive_mesh = self.maxx_init_passive_mesh
        self.other_bending_network.minn_init_passive_mesh = self.minn_init_passive_mesh # ### init maximum passive meshe #
        self.other_bending_network.train_residual_friction = self.train_residual_friction
        ### use optimizable params ###
        self.other_bending_network.use_optimizable_params = self.use_optimizable_params
        self.other_bending_network.penetration_proj_k_to_robot_friction = self.penetration_proj_k_to_robot_friction
        self.other_bending_network.ang_vel_damping = self.ang_vel_damping
        self.other_bending_network.use_damping_params_vel = self.use_damping_params_vel ## use_damping_params
        self.other_bending_network.optm_alltime_ks = self.optm_alltime_ks
        
        # self.ts_to_mesh_offset = self.load_calcu_timestep_to_passive_mesh_offset()
        # self.ts_to_mesh_offset_for_opt = self.load_calcu_timestep_to_passive_mesh_offset()


        params_to_train += list(self.other_bending_network.parameters()) # 
        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        
        if 'model.ckpt_fn' in self.conf and len(self.conf['model.ckpt_fn']) > 0:
            cur_ckpt_fn = self.conf['model.ckpt_fn']
            self.load_checkpoint_via_fn(cur_ckpt_fn)
            if self.train_multi_seqs:
                damping_coefs = self.other_bending_network.damping_constant[0].weight.data
                spring_ks_values = self.other_bending_network.spring_ks_values[0].weight.data
            else:
                damping_coefs = self.other_bending_network.damping_constant.weight.data
                spring_ks_values = self.other_bending_network.spring_ks_values.weight.data
            print(f"loaded ckpt has damping_coefs: {damping_coefs}, and spring ks values: {spring_ks_values}")
            try:
                friction_spring_ks = self.other_bending_network.spring_friction_ks_values.weight.data
                print(f"friction_spring_ks:")
                print(friction_spring_ks)
                
                obj_inertia_val = self.other_bending_network.obj_inertia.weight.data
                optimizable_obj_mass = self.other_bending_network.optimizable_obj_mass.weight.data
                print(f"obj_inertia_val: {obj_inertia_val ** 2}, optimizable_obj_mass: {optimizable_obj_mass ** 2}")
            except:
                pass
            
            time_constant = self.other_bending_network.time_constant.weight.data 
            print(f"time_constant: {time_constant}")
        
        # ### gravity ### #
        self.gravity_acc = 9.8
        self.gravity_dir = torch.tensor([0., 0., -1]).float().cuda()
        self.passive_obj_mass = 1.

        if not self.bending_net_type == "active_force_field_v13":
            #### init passive mesh center and I_ref # # 
            self.init_passive_mesh_center, self.I_ref = self.calculate_passive_mesh_center_intertia()
            self.inv_I_ref = torch.linalg.inv(self.I_ref)
            self.other_bending_network.passive_obj_inertia = self.I_ref
            self.other_bending_network.passive_obj_inertia_inv = self.inv_I_ref

    def get_robohand_type_from_conf_fn(self, conf_model_fn):
        if "redmax" in conf_model_fn:
            hand_type = "redmax_hand"
        elif "shadow" in conf_model_fn:
            hand_type = "shadow_hand"
        else:
            raise ValueError(f"Cannot identify robot hand type from the conf_model file: {conf_model_fn}")
        return hand_type
        
    def calculate_passive_mesh_center_intertia(self, ): # passive 
        # self.timestep_to_passive_mesh # ## passvie mesh center ### 
        init_passive_mesh = self.timestep_to_passive_mesh[0] ### nn_mesh_pts x 3 ###
        init_passive_mesh_center = torch.mean(init_passive_mesh, dim=0) ### init_center ###
        per_vert_mass = self.passive_obj_mass / float(init_passive_mesh.size(0))
        # (center to the vertex)
        # assume the mass is uniformly distributed across all vertices ##
        I_ref = torch.zeros((3, 3), dtype=torch.float32).cuda()
        for i_v in range(init_passive_mesh.size(0)):
            cur_vert = init_passive_mesh[i_v]
            cur_r = cur_vert - init_passive_mesh_center
            dot_r_r = torch.sum(cur_r * cur_r)
            cur_eye_mtx = torch.eye(3, dtype=torch.float32).cuda()
            r_mult_rT = torch.matmul(cur_r.unsqueeze(-1), cur_r.unsqueeze(0))
            I_ref += (dot_r_r * cur_eye_mtx - r_mult_rT) * per_vert_mass
        return init_passive_mesh_center, I_ref
    
    def load_grid_forces_via_fn(self, grid_forces_fn):
        sv_values = np.load(grid_forces_fn, allow_pickle=True).item()
        self.ref_grid_forces = sv_values['timestep_to_grid_pts_forces']
        
        self.ref_grid_weight = sv_values['timestep_to_grid_pts_weight']
        
        self.ref_grid_forces = {
            ts: torch.from_numpy(self.ref_grid_forces[ts]).float().cuda() for ts in self.ref_grid_forces
        }
        self.ref_grid_weight = {
            ts: torch.from_numpy(self.ref_grid_weight[ts]).float().cuda() for ts in self.ref_grid_weight
        }
        
    def calculate_obj_inertia(self, ):
        if self.canon_passive_obj_verts is None:
            cur_init_passive_mesh_verts = self.timestep_to_passive_mesh[0].clone()
        else:
            cur_init_passive_mesh_verts = self.canon_passive_obj_verts.clone()
        cur_init_passive_mesh_center = torch.mean(cur_init_passive_mesh_verts, dim=0)
        cur_init_passive_mesh_verts = cur_init_passive_mesh_verts - cur_init_passive_mesh_center
        # per_vert_mass=  cur_init_passive_mesh_verts.size(0) / self.obj_mass
        per_vert_mass = self.obj_mass / cur_init_passive_mesh_verts.size(0) 
        ## 
        print(f"[Calculating obj inertia] per_vert_mass: {per_vert_mass}")
        I_ref = torch.zeros((3, 3), dtype=torch.float32).cuda() ## caclulate I_ref; I_inv_ref; ##
        for i_v in range(cur_init_passive_mesh_verts.size(0)):
            cur_vert = cur_init_passive_mesh_verts[i_v]
            cur_r = cur_vert # - cur_init_passive_mesh_center
            cur_v_inertia = per_vert_mass * (torch.sum(cur_r * cur_r) - torch.matmul(cur_r.unsqueeze(-1), cur_r.unsqueeze(0)))
            # cur_v_inertia = torch.cross(cur_r, cur_r) * per_vert_mass3 # # 
            I_ref += cur_v_inertia

        print(f"In calculating inertia")
        print(I_ref)
        self.I_inv_ref = torch.linalg.inv(I_ref)
        
        self.I_ref = I_ref
    
    
    
    # self.rhand_verts, self.hand_faces, self.obj_faces, self.obj_normals, self.ts_to_contact_pts = self.load_active_passive_timestep_to_mesh_multi_seqs()
    def load_active_passive_timestep_to_mesh_multi_seqs(self, ):
        data_folder = "/data/xueyi/arctic/arctic_processed_data/processed_seqs/s01"
        # extracted_dict.npy"
        tot_fns = os.listdir(data_folder)
        tot_fns = [cur_fn for cur_fn in tot_fns if cur_fn.endswith("extracted_dict.npy")]
        
        self.rhand_verts = []
        self.hand_faces = []
        self.obj_faces = []
        self.obj_normals = []
        self.ts_to_contact_pts = []
        self.hand_verts = []
        self.obj_verts = []
        for cur_fn in tot_fns:
            tot_sv_fn = os.path.join(data_folder, cur_fn)
            cur_active_passive_sv_dict = np.load(tot_sv_fn, allow_pickle=True).item()
            obj_verts = cur_active_passive_sv_dict['obj_verts']
            obj_faces = cur_active_passive_sv_dict['obj_faces']
            
            init_obj_verts = obj_verts[0]
            init_trimesh = trimesh.Trimesh(vertices=init_obj_verts, faces=obj_faces)
            mesh_exported_path = 'init_box_mesh.ply' # export and load # 
            init_trimesh.export(mesh_exported_path)
            o3d_mesh = o3d.io.read_triangle_mesh(mesh_exported_path)
            o3d_mesh.compute_vertex_normals()
            init_verts_normals = o3d_mesh.vertex_normals
            init_verts_normals = np.array(init_verts_normals, dtype=np.float32)
            init_verts_normals = torch.from_numpy(init_verts_normals).float().cuda() # init box meshes # # timestep to mesh multi seqs #
            
            rhand_verts = cur_active_passive_sv_dict['rhand_verts']
            lhand_verts = cur_active_passive_sv_dict['lhand_verts']
            hand_verts = np.concatenate([rhand_verts, lhand_verts], axis=1)
            obj_verts = torch.from_numpy(obj_verts).float().cuda()
            hand_verts = torch.from_numpy(hand_verts).float().cuda()
            # rhand_verts = ``
            hand_faces = cur_active_passive_sv_dict['hand_faces']
            
            rhand_verts = torch.from_numpy(rhand_verts).float().cuda()
            hand_faces = torch.from_numpy(hand_faces).long().cuda()
            obj_faces = torch.tensor(obj_faces).long().cuda()
            obj_normals = {0: init_verts_normals}
            
            # rhand verts and hand faces and obj vertices # 
            dist_rhand_verts_to_obj_vertts = torch.sum(
                (rhand_verts.unsqueeze(2) - obj_verts.unsqueeze(1)) ** 2, dim=-1
            ) ## nn_hand_verts x nn_obj_verts
            dist_rhand_verts_to_obj_vertts, _ = torch.min(dist_rhand_verts_to_obj_vertts, dim=-1) ### nn_hand_verts ##
            ts_to_contact_pts = {}
            thresold_dist = 0.001
            for ts in range(rhand_verts.size(0)):
                cur_contact_pts_threshold_indicator = dist_rhand_verts_to_obj_vertts[ts] <= thresold_dist
                if torch.sum(cur_contact_pts_threshold_indicator.float()).item() > 0.1:
                    ts_to_contact_pts[ts] = rhand_verts[ts][cur_contact_pts_threshold_indicator]
            
            self.rhand_verts.append(rhand_verts)
            self.hand_faces.append(hand_faces)
            self.obj_faces.append(obj_faces)
            self.obj_normals.append(obj_normals)
            self.ts_to_contact_pts.append(ts_to_contact_pts)
            self.hand_verts.append(hand_verts)
            self.obj_verts.append(obj_verts)
            
        # print(f"obj_verts: {obj_verts.size()}, hand_verts: {hand_verts.size()}, rhand_verts: {rhand_verts.shape}, lhand_verts: {lhand_verts.shape}, hand_faces: {self.hand_faces.size()}, obj_faces: {self.obj_faces.size()}")
        return self.rhand_verts, self.hand_faces, self.obj_faces, self.obj_normals, self.ts_to_contact_pts, self.hand_verts
    
    # self.obj_verts, self.hand_verts = self.load_active_passive_timestep_to_mesh() # 
    
    # the collison geometry should be able to locate the contact points #
    # the contact points should be able to locate the collision geometry #
    # collision geometry -> sdf of the pnetrated points or a point in the air #
    # collision geometry -> query the narest point of an arbitrary point #
    
    # calculate_collision_geometry_bounding_boxes, self.maxx_init_passive_mesh, self.minn_init_passive_mesh # 
    def calculate_collision_geometry_bounding_boxes(self, ):    
        # #
        # nearest ppont ? 
        init_passive_mesh = self.timestep_to_passive_mesh[0]
        maxx_init_passive_mesh, _ = torch.max(init_passive_mesh, dim=0) ## (3, )
        minn_init_passive_mesh, _ = torch.min(init_passive_mesh, dim=0) ## (3, )
        # maxx init passive mesh; minn init passvie mesh ##
        # contact passive mesh #
        self.maxx_init_passive_mesh = maxx_init_passive_mesh
        self.minn_init_passive_mesh = minn_init_passive_mesh # 
        
        pass
    
    ## gethtjobject verties and callibrate the collision geometry ###
    
    def load_active_passive_timestep_to_mesh_v3(self, ):
        # train_dyn_mano_model_states ## rhand 
        sv_fn = "/data1/xueyi/GRAB_extracted_test/test/30_sv_dict.npy"
        # /data1/xueyi/GRAB_extracted_test/train/20_sv_dict_real_obj.obj # data1
        
        if 'model.kinematic_mano_gt_sv_fn' in self.conf:
            sv_fn = self.conf['model.kinematic_mano_gt_sv_fn']
        
        ### get hand faces ###
        # sv_fn = "/data2/xueyi/arctic_processed_data/processed_sv_dicts/s01/box_grab_01_extracted_dict.npy"
        ''' Loading mano template '''
        mano_hand_template_fn = 'assets/mano_hand_template.obj'
        if not os.path.exists(mano_hand_template_fn):
            box_sv_fn = "/data2/xueyi/arctic_processed_data/processed_sv_dicts/s01/box_grab_01_extracted_dict.npy"
            box_sv_dict = np.load(box_sv_fn, allow_pickle=True).item()
            mano_hand_faces = box_sv_dict['hand_faces']
            mano_hand_verts = box_sv_dict['rhand_verts'][0]
            mano_hand_mesh = trimesh.Trimesh(mano_hand_verts, mano_hand_faces)
            mano_hand_mesh.export(mano_hand_template_fn)
        mano_hand_temp = trimesh.load(mano_hand_template_fn, force='mesh')
        hand_faces = mano_hand_temp.faces
        self.hand_faces = torch.from_numpy(hand_faces).long().to(self.device)
        
        print(f"Loading data from {sv_fn}")
        
        sv_dict = np.load(sv_fn, allow_pickle=True).item()
        
        print(f"sv_dict: {sv_dict.keys()}")
        
        obj_pcs = sv_dict['object_pc']
        obj_pcs = torch.from_numpy(obj_pcs).float().cuda()
        # self.obj_pcs = obj_pcs
        
        
        
        obj_vertex_normals = sv_dict['obj_vertex_normals']
        obj_vertex_normals = torch.from_numpy(obj_vertex_normals).float().cuda()
        self.obj_normals = obj_vertex_normals
        
        object_global_orient = sv_dict['object_global_orient'] # glboal orient 
        object_transl = sv_dict['object_transl']
        
        
        obj_faces = sv_dict['obj_faces']
        obj_faces = torch.from_numpy(obj_faces).long().cuda()
        self.obj_faces = obj_faces
        
        obj_verts = sv_dict['obj_verts']
        minn_verts = np.min(obj_verts, axis=0)
        maxx_verts = np.max(obj_verts, axis=0)
        extent = maxx_verts - minn_verts
        center_ori = (maxx_verts + minn_verts) / 2
        scale_ori = np.sqrt(np.sum(extent ** 2))
        obj_verts = torch.from_numpy(obj_verts).float().cuda()
        
        
        
        self.obj_verts = obj_verts
        
        
        
        mesh_scale = 0.8
        bbmin, _ = obj_verts.min(0) #
        bbmax, _ = obj_verts.max(0) #
        
        center = (bbmin + bbmax) * 0.5
        scale = 2.0 * mesh_scale / (bbmax - bbmin).max() # bounding box's max #
        # vertices = (vertices - center) * scale # (vertices - center) * scale # #
        
        self.sdf_space_center = center
        self.sdf_space_scale = scale
        # sdf_sv_fn = self.sdf_sv_fn
        # self.obj_sdf = np.load(sdf_sv_fn, allow_pickle=True)
        # self.sdf_res = self.obj_sdf.shape[0]
        
        
        # init_obj_pcs = obj_pcs[0].detach().cpu().numpy()
        # init_glb_rot = object_global_orient[0]
        # init_glb_trans = object_transl[0]
        # init_glb_rot_struct = R.from_rotvec(init_glb_rot)
        # init_glb_rot_mtx = init_glb_rot_struct.as_matrix()
        # self.obj_verts = np.matmul((init_obj_pcs - init_glb_trans[None]), init_glb_rot_mtx.T)
        # obj_verts = self.obj_verts
        # minn_verts = np.min(obj_verts, axis=0)
        # maxx_verts = np.max(obj_verts, axis=0)
        # extent = maxx_verts - minn_verts
        # scale_cur = np.sqrt(np.sum(extent ** 2))
        
        # center_cur= (minn_verts + maxx_verts) / 2
        
        # obj_verts = (sv_dict['obj_verts'] - center_ori[None]) / scale_ori * scale_cur + center_cur[None]
        
        # obj_verts = torch.from_numpy(obj_verts).float().cuda()
        # self.obj_verts = obj_verts
        
        # sv_fn_obj_fn = sv_fn[:-4] + "_real_obj.obj"
        # scaled_obj = trimesh.Trimesh(vertices=self.obj_verts.detach().cpu().numpy(), faces=self.obj_faces.detach().cpu().numpy(), vertex_normals=self.obj_normals.detach().cpu().numpy())
        # scaled_obj.export(sv_fn_obj_fn)
        # print(f"Scaled obj saved to {scaled_obj}")
        
        
        
        tot_reversed_obj_rot_mtx = []
        tot_obj_quat = [] ## rotation matrix 
        
        
        transformed_obj_verts = []
        for i_fr in range(object_global_orient.shape[0]):
            cur_glb_rot = object_global_orient[i_fr]
            cur_transl = object_transl[i_fr]
            cur_transl = torch.from_numpy(cur_transl).float().cuda()
            cur_glb_rot_struct = R.from_rotvec(cur_glb_rot)
            cur_glb_rot_mtx = cur_glb_rot_struct.as_matrix()
            cur_glb_rot_mtx = torch.from_numpy(cur_glb_rot_mtx).float().cuda()
            
            cur_transformed_verts = torch.matmul(
                self.obj_verts, cur_glb_rot_mtx
            ) + cur_transl.unsqueeze(0)
            
            cur_glb_rot_mtx_reversed = cur_glb_rot_mtx.contiguous().transpose(1, 0).contiguous()
            tot_reversed_obj_rot_mtx.append(cur_glb_rot_mtx_reversed)
            
            cur_glb_rot_struct = R.from_matrix(cur_glb_rot_mtx_reversed.cpu().numpy())
            cur_obj_quat = cur_glb_rot_struct.as_quat()
            cur_obj_quat = cur_obj_quat[[3, 0, 1, 2]]
            cur_obj_quat = torch.from_numpy(cur_obj_quat).float().cuda()
            tot_obj_quat.append(cur_obj_quat)

            # center_obj_verts = torch.mean(self.obj_verts, dim=0, keepdim=True)
            # cur_transformed_verts = torch.matmul(
            #     (self.obj_verts - center_obj_verts), cur_glb_rot_mtx
            # ) + cur_transl.unsqueeze(0) + center_obj_verts
            
            # cur_transformed_verts = torch.matmul(
            #     cur_glb_rot_mtx, self.obj_verts.transpose(1, 0)
            # ).contiguous().transpose(1, 0).contiguous() + cur_transl.unsqueeze(0)
            transformed_obj_verts.append(cur_transformed_verts)
        transformed_obj_verts = torch.stack(transformed_obj_verts, dim=0)
        
        self.obj_pcs = transformed_obj_verts
        
        rhand_verts = sv_dict['rhand_verts']
        rhand_verts = torch.from_numpy(rhand_verts).float().cuda()
        self.rhand_verts = rhand_verts ## rhand verts ## 
        
        
        
        if '30_sv_dict' in sv_fn:
            bbox_selected_verts_idxes = torch.tensor([1511, 1847, 2190, 2097, 2006, 2108, 1604], dtype=torch.long).cuda()
            obj_selected_verts = self.obj_verts[bbox_selected_verts_idxes]
        else:
            obj_selected_verts = self.obj_verts.clone()
        
        maxx_init_passive_mesh, _ = torch.max(obj_selected_verts, dim=0)
        minn_init_passive_mesh, _ = torch.min(obj_selected_verts, dim=0)
        self.maxx_init_passive_mesh = maxx_init_passive_mesh
        self.minn_init_passive_mesh = minn_init_passive_mesh
        
        
        init_obj_verts = obj_verts # [0] # cannnot rotate it at all # frictional forces in the pybullet? # 
        
        mesh_scale = 0.8
        bbmin, _ = init_obj_verts.min(0) #
        bbmax, _ = init_obj_verts.max(0) #
        print(f"bbmin: {bbmin}, bbmax: {bbmax}")
        center = (bbmin + bbmax) * 0.5
        
        scale = 2.0 * mesh_scale / (bbmax - bbmin).max() # bounding box's max #
        # vertices = (vertices - center) * scale # (vertices - center) * scale # #
        
        self.sdf_space_center = center.detach().cpu().numpy()
        self.sdf_space_scale = scale.detach().cpu().numpy()
        # sdf_sv_fn = "/data/xueyi/diffsim/NeuS/init_box_mesh.npy"
        # if not os.path.exists(sdf_sv_fn):
        #     sdf_sv_fn = "/home/xueyi/diffsim/NeuS/init_box_mesh.npy"
        # self.obj_sdf = np.load(sdf_sv_fn, allow_pickle=True)
        # self.sdf_res = self.obj_sdf.shape[0]
        # print(f"obj_sdf loaded from {sdf_sv_fn} with shape {self.obj_sdf.shape}")
        
        
        
        
        
        # tot_obj_quat, tot_reversed_obj_rot_mtx #
        tot_obj_quat = torch.stack(tot_obj_quat, dim=0)
        tot_reversed_obj_rot_mtx = torch.stack(tot_reversed_obj_rot_mtx, dim=0)
        self.tot_obj_quat = tot_obj_quat
        self.tot_reversed_obj_rot_mtx = tot_reversed_obj_rot_mtx
        
        ## should save self.object_global_orient and self.object_transl ##
        # object_global_orient, object_transl #
        self.object_global_orient = torch.from_numpy(object_global_orient).float().cuda()
        self.object_transl = torch.from_numpy(object_transl).float().cuda()
        return transformed_obj_verts, rhand_verts, self.obj_normals
    
    
    def load_active_passive_timestep_to_mesh_v3_taco(self, ):
        # train_dyn_mano_model_states ## rhand 
        sv_fn = "/data1/xueyi/GRAB_extracted_test/test/30_sv_dict.npy"
        # /data1/xueyi/GRAB_extracted_test/train/20_sv_dict_real_obj.obj # data1
        
        # start_idx = 40
        
        start_idx = self.seq_start_idx
        maxx_ws = 150
        # maxx_ws = 90
        
        print(f"TACO loading data with start_idx: {start_idx}, maxx_ws: {maxx_ws}")
        
        
        if 'model.kinematic_mano_gt_sv_fn' in self.conf:
            sv_fn = self.conf['model.kinematic_mano_gt_sv_fn']
        
        ### get hand faces ###
        # sv_fn = "/data2/xueyi/arctic_processed_data/processed_sv_dicts/s01/box_grab_01_extracted_dict.npy"
        ''' Loading mano template '''
        mano_hand_template_fn = 'assets/mano_hand_template.obj'
        if not os.path.exists(mano_hand_template_fn):
            box_sv_fn = "/data2/xueyi/arctic_processed_data/processed_sv_dicts/s01/box_grab_01_extracted_dict.npy"
            box_sv_dict = np.load(box_sv_fn, allow_pickle=True).item()
            mano_hand_faces = box_sv_dict['hand_faces']
            mano_hand_verts = box_sv_dict['rhand_verts'][0]
            mano_hand_mesh = trimesh.Trimesh(mano_hand_verts, mano_hand_faces)
            mano_hand_mesh.export(mano_hand_template_fn)
        mano_hand_temp = trimesh.load(mano_hand_template_fn, force='mesh')
        hand_faces = mano_hand_temp.faces
        self.hand_faces = torch.from_numpy(hand_faces).long().to(self.device)
        
        
        
        print(f"Loading data from {sv_fn}")
        
        # sv_dict = np.load(sv_fn, allow_pickle=True).item()
        
        sv_dict = pkl.load(open(sv_fn, "rb"))
        
        self.hand_faces = torch.from_numpy(sv_dict['hand_faces']).float().cuda()
        
        print(f"sv_dict: {sv_dict.keys()}")
        
        maxx_ws = min(maxx_ws, len(sv_dict['obj_verts']) - start_idx)
        
        obj_pcs = sv_dict['obj_verts'][start_idx: start_idx + maxx_ws]
        obj_pcs = torch.from_numpy(obj_pcs).float().cuda()
        
        self.obj_pcs = obj_pcs
        # obj_vertex_normals = sv_dict['obj_vertex_normals']
        # obj_vertex_normals = torch.from_numpy(obj_vertex_normals).float().cuda()
        self.obj_normals = torch.zeros_like(obj_pcs[0]) ### get the obj naormal vectors ##
        
        object_pose = sv_dict['obj_pose'][start_idx: start_idx + maxx_ws]
        object_pose = torch.from_numpy(object_pose).float().cuda() ### nn_frames x 4 x 4 ###
        object_global_orient_mtx = object_pose[:, :3, :3 ] ## nn_frames x 3 x 3 ##
        object_transl = object_pose[:, :3, 3] ## nn_frmaes x 3 ##
        
        
        # object_global_orient = sv_dict['object_global_orient'] # glboal orient 
        # object_transl = sv_dict['object_transl']
        
        
        obj_faces = sv_dict['obj_faces']
        obj_faces = torch.from_numpy(obj_faces).long().cuda()
        self.obj_faces = obj_faces # [0] ### obj faces ##
        
        # obj_verts = sv_dict['obj_verts']
        # minn_verts = np.min(obj_verts, axis=0)
        # maxx_verts = np.max(obj_verts, axis=0)
        # extent = maxx_verts - minn_verts
        # center_ori = (maxx_verts + minn_verts) / 2
        # scale_ori = np.sqrt(np.sum(extent ** 2))
        # obj_verts = torch.from_numpy(obj_verts).float().cuda()
        
        init_obj_verts = obj_pcs[0]
        init_obj_ornt_mtx = object_global_orient_mtx[0]
        init_obj_transl = object_transl[0]
        
        canon_obj_verts = torch.matmul(
            init_obj_ornt_mtx.contiguous().transpose(1, 0).contiguous(), (init_obj_verts - init_obj_transl.unsqueeze(0)).transpose(1, 0).contiguous()
        ).transpose(1, 0).contiguous() ### 
        self.obj_verts = canon_obj_verts.clone()
        obj_verts = canon_obj_verts.clone()
        
        
        # self.obj_verts = obj_verts
        
        
        
        # mesh_scale = 0.8
        # bbmin, _ = obj_verts.min(0) #
        # bbmax, _ = obj_verts.max(0) #
        
        # center = (bbmin + bbmax) * 0.5
        # scale = 2.0 * mesh_scale / (bbmax - bbmin).max() # bounding box's max #
        # # vertices = (vertices - center) * scale # (vertices - center) * scale # #
        
        # self.sdf_space_center = center
        # self.sdf_space_scale = scale
        
        
        sdf_sv_fn = self.conf['model.obj_sdf_fn']
        print(f'sdf_sv_fn: {sdf_sv_fn}')
        self.obj_sdf = np.load(sdf_sv_fn, allow_pickle=True)
        self.sdf_res = self.obj_sdf.shape[0]
        
        self.obj_sdf = torch.from_numpy(self.obj_sdf).float().cuda()
        # init_obj_pcs = obj_pcs[0].detach().cpu().numpy()
        # init_glb_rot = object_global_orient[0]
        # init_glb_trans = object_transl[0]
        # init_glb_rot_struct = R.from_rotvec(init_glb_rot)
        # init_glb_rot_mtx = init_glb_rot_struct.as_matrix()
        # self.obj_verts = np.matmul((init_obj_pcs - init_glb_trans[None]), init_glb_rot_mtx.T)
        # obj_verts = self.obj_verts
        # minn_verts = np.min(obj_verts, axis=0)
        # maxx_verts = np.max(obj_verts, axis=0)
        # extent = maxx_verts - minn_verts
        # scale_cur = np.sqrt(np.sum(extent ** 2))
        
        # center_cur= (minn_verts + maxx_verts) / 2
        
        # obj_verts = (sv_dict['obj_verts'] - center_ori[None]) / scale_ori * scale_cur + center_cur[None]
        
        # obj_verts = torch.from_numpy(obj_verts).float().cuda()
        # self.obj_verts = obj_verts
        
        # sv_fn_obj_fn = sv_fn[:-4] + "_real_obj.obj"
        # scaled_obj = trimesh.Trimesh(vertices=self.obj_verts.detach().cpu().numpy(), faces=self.obj_faces.detach().cpu().numpy(), vertex_normals=self.obj_normals.detach().cpu().numpy())
        # scaled_obj.export(sv_fn_obj_fn)
        # print(f"Scaled obj saved to {scaled_obj}")
        
        tot_obj_quat = []
        
        for i_fr in range(object_global_orient_mtx.shape[0]):
            cur_ornt_mtx = object_global_orient_mtx[i_fr]
            cur_ornt_mtx_np = cur_ornt_mtx.detach().cpu().numpy() ### cur ornt mtx ## 
            cur_ornt_rot_struct = R.from_matrix(cur_ornt_mtx_np)
            cur_ornt_quat = cur_ornt_rot_struct.as_quat()
            cur_ornt_quat = cur_ornt_quat[[3, 0, 1, 2]]
            tot_obj_quat.append(torch.from_numpy(cur_ornt_quat).float().cuda()) ### float cuda ##
        
        # tot_obj_quat = np.stack(tot_obj_quat, axis=0) ## obj quat ##
        tot_obj_quat = torch.stack(tot_obj_quat, dim=0)
            
        
        # tot_reversed_obj_rot_mtx = []
        # tot_obj_quat = [] ## rotation matrix 
        
        
        # transformed_obj_verts = []
        # for i_fr in range(object_global_orient.shape[0]):
        #     cur_glb_rot = object_global_orient[i_fr]
        #     cur_transl = object_transl[i_fr]
        #     cur_transl = torch.from_numpy(cur_transl).float().cuda()
        #     cur_glb_rot_struct = R.from_rotvec(cur_glb_rot)
        #     cur_glb_rot_mtx = cur_glb_rot_struct.as_matrix()
        #     cur_glb_rot_mtx = torch.from_numpy(cur_glb_rot_mtx).float().cuda()
            
        #     cur_transformed_verts = torch.matmul(
        #         self.obj_verts, cur_glb_rot_mtx
        #     ) + cur_transl.unsqueeze(0)
            
        #     cur_glb_rot_mtx_reversed = cur_glb_rot_mtx.contiguous().transpose(1, 0).contiguous()
        #     tot_reversed_obj_rot_mtx.append(cur_glb_rot_mtx_reversed)
            
        #     cur_glb_rot_struct = R.from_matrix(cur_glb_rot_mtx_reversed.cpu().numpy())
        #     cur_obj_quat = cur_glb_rot_struct.as_quat()
        #     cur_obj_quat = cur_obj_quat[[3, 0, 1, 2]]
        #     cur_obj_quat = torch.from_numpy(cur_obj_quat).float().cuda()
        #     tot_obj_quat.append(cur_obj_quat)

        #     # center_obj_verts = torch.mean(self.obj_verts, dim=0, keepdim=True)
        #     # cur_transformed_verts = torch.matmul(
        #     #     (self.obj_verts - center_obj_verts), cur_glb_rot_mtx
        #     # ) + cur_transl.unsqueeze(0) + center_obj_verts
            
        #     # cur_transformed_verts = torch.matmul(
        #     #     cur_glb_rot_mtx, self.obj_verts.transpose(1, 0)
        #     # ).contiguous().transpose(1, 0).contiguous() + cur_transl.unsqueeze(0)
        #     transformed_obj_verts.append(cur_transformed_verts)
        # transformed_obj_verts = torch.stack(transformed_obj_verts, dim=0)
        
        
        rhand_verts = sv_dict['hand_verts'][start_idx: start_idx + maxx_ws]
        rhand_verts = torch.from_numpy(rhand_verts).float().cuda()
        self.rhand_verts = rhand_verts ## rhand verts ## 
        
        
        
        # if '30_sv_dict' in sv_fn:
        #     bbox_selected_verts_idxes = torch.tensor([1511, 1847, 2190, 2097, 2006, 2108, 1604], dtype=torch.long).cuda()
        #     obj_selected_verts = self.obj_verts[bbox_selected_verts_idxes]
        # else:
        #     obj_selected_verts = self.obj_verts.clone()
        
        # maxx_init_passive_mesh, _ = torch.max(obj_selected_verts, dim=0)
        # minn_init_passive_mesh, _ = torch.min(obj_selected_verts, dim=0)
        # self.maxx_init_passive_mesh = maxx_init_passive_mesh
        # self.minn_init_passive_mesh = minn_init_passive_mesh
        
        
        init_obj_verts = obj_verts # [0] # cannnot rotate it at all # frictional forces in the pybullet? # 
        
        mesh_scale = 0.8
        bbmin, _ = init_obj_verts.min(0) #
        bbmax, _ = init_obj_verts.max(0) #
        print(f"bbmin: {bbmin}, bbmax: {bbmax}")
        center = (bbmin + bbmax) * 0.5
        
        scale = 2.0 * mesh_scale / (bbmax - bbmin).max() # bounding box's max #
        # vertices = (vertices - center) * scale # (vertices - center) * scale # #
        
        self.sdf_space_center = center.detach().cpu().numpy()
        self.sdf_space_scale = scale.detach().cpu().numpy()
        # # sdf_sv_fn = "/data/xueyi/diffsim/NeuS/init_box_mesh.npy"
        # if not os.path.exists(sdf_sv_fn):
        #     sdf_sv_fn = "/home/xueyi/diffsim/NeuS/init_box_mesh.npy"
        # self.obj_sdf = np.load(sdf_sv_fn, allow_pickle=True)
        # self.sdf_res = self.obj_sdf.shape[0]
        # print(f"obj_sdf loaded from {sdf_sv_fn} with shape {self.obj_sdf.shape}")
        
        
        
        
        
        # tot_obj_quat, tot_reversed_obj_rot_mtx #
        # tot_obj_quat = torch.stack(tot_obj_quat, dim=0)
        tot_reversed_obj_rot_mtx = object_global_orient_mtx.clone() # torch.stack(tot_reversed_obj_rot_mtx, dim=0)
        self.tot_obj_quat = tot_obj_quat
        self.tot_reversed_obj_rot_mtx = tot_reversed_obj_rot_mtx
        
        ## should save self.object_global_orient and self.object_transl ##
        # object_global_orient, object_transl #
        # self.object_global_orient = torch.from_numpy(object_global_orient).float().cuda()
        self.object_transl = object_transl.clone() #  torch.from_numpy(object_transl).float().cuda()
        return self.obj_pcs, rhand_verts, self.obj_normals
    
    
    
    def load_active_passive_timestep_to_mesh_twohands_arctic(self, ):
        # train_dyn_mano_model_states ## rhand 
        # sv_fn = "/data1/xueyi/GRAB_extracted_test/test/30_sv_dict.npy"
        # /data1/xueyi/GRAB_extracted_test/train/20_sv_dict_real_obj.obj # data1
        import utils.utils as utils
        from manopth.manolayer import ManoLayer
        
        # mano_hand_template_fn = 'assets/mano_hand_template.obj'
        # # if not os.path.exists(mano_hand_template_fn):
        # #     box_sv_fn = "/data2/xueyi/arctic_processed_data/processed_sv_dicts/s01/box_grab_01_extracted_dict.npy"
        # #     box_sv_dict = np.load(box_sv_fn, allow_pickle=True).item()
        # #     mano_hand_faces = box_sv_dict['hand_faces']
        # #     mano_hand_verts = box_sv_dict['rhand_verts'][0]
        # #     mano_hand_mesh = trimesh.Trimesh(mano_hand_verts, mano_hand_faces)
        # #     mano_hand_mesh.export(mano_hand_template_fn)
        # mano_hand_temp = trimesh.load(mano_hand_template_fn, force='mesh')
        # hand_faces = mano_hand_temp.faces
        
        
        rgt_hand_pkl_fn = "assets/right_20230917_004.pkl"
        data_dict = pkl.load(open(rgt_hand_pkl_fn, "rb"))
        hand_faces = data_dict['hand_faces'] # ['faces']
        
        self.hand_faces = torch.from_numpy(hand_faces).long().to(self.device)
        
        self.start_idx = 20
        # self.window_size = 60
        self.window_size = self.window_size
        start_idx = self.start_idx
        window_size = self.window_size
        
        # if 'model.kinematic_mano_gt_sv_fn' in self.conf:
        sv_fn = self.conf['model.kinematic_mano_gt_sv_fn']
        
        # gt_data_folder = "/".join(sv_fn.split("/")[:-1]) ## 
        gt_data_fn_name = sv_fn.split("/")[-1].split(".")[0]
        arctic_processed_data_sv_folder = "/home/xueyi/diffsim/NeuS/raw_data/arctic_processed_canon_obj"
        if not os.path.exists(arctic_processed_data_sv_folder):
            arctic_processed_data_sv_folder = "/root/diffsim/quasi-dyn/raw_data/arctic_processed_canon_obj"
        gt_data_canon_obj_sv_fn = f"{arctic_processed_data_sv_folder}/{gt_data_fn_name}_canon_obj.obj"
            
        print(f"Loading data from {sv_fn}")
        
        sv_dict = np.load(sv_fn, allow_pickle=True).item()
        
        tot_frames_nn  = sv_dict["obj_rot"].shape[0]
        window_size = min(tot_frames_nn - self.start_idx, window_size)
        self.window_size = window_size
        
        
        object_global_orient = sv_dict["obj_rot"][start_idx: start_idx + window_size] # num_frames x 3 
        object_transl = sv_dict["obj_trans"][start_idx: start_idx + window_size] * 0.001 # num_frames x 3
        obj_pcs = sv_dict["verts.object"][start_idx: start_idx + window_size]
        
        # obj_pcs = sv_dict['object_pc']
        obj_pcs = torch.from_numpy(obj_pcs).float().cuda()
        
        
        obj_vertex_normals = torch.zeros_like(obj_pcs)
        obj_tot_normals = obj_vertex_normals
        print(f"obj_normals: {obj_tot_normals.size()}")
        # /data/xueyi/sim/arctic_processed_data/processed_seqs/s01/espressomachine_use_01.npy
        
        # obj_vertex_normals = sv_dict['obj_vertex_normals']
        # obj_vertex_normals = torch.from_numpy(obj_vertex_normals).float().cuda()
        # self.obj_normals = obj_vertex_normals
        
        # object_global_orient = sv_dict['object_global_orient'] # glboal orient 
        # object_transl = sv_dict['object_transl']
        
        
        obj_faces = sv_dict['f'][0]
        obj_faces = torch.from_numpy(obj_faces).long().cuda()
        self.obj_faces = obj_faces
        
        # obj_verts = sv_dict['verts.object']
        # minn_verts = np.min(obj_verts, axis=0)
        # maxx_verts = np.max(obj_verts, axis=0)
        # extent = maxx_verts - minn_verts
        # # center_ori = (maxx_verts + minn_verts) / 2
        # # scale_ori = np.sqrt(np.sum(extent ** 2))
        # obj_verts = torch.from_numpy(obj_verts).float().cuda()
        
        
        # obj_sv_path = "/data3/datasets/xueyi/arctic/arctic_data/data/meta/object_vtemplates"
        # obj_name = sv_fn.split("/")[-1].split("_")[0]
        # obj_mesh_fn = os.path.join(obj_sv_path, obj_name, "mesh.obj")
        # print(f"loading from {obj_mesh_fn}")
        # # template_obj_vs, template_obj_fs = trimesh.load(obj_mesh_fn, force='mesh')
        # template_obj_vs, template_obj_fs = utils.read_obj_file_ours(obj_mesh_fn, sub_one=True)
        
        
        
        
        # self.obj_verts = obj_verts
        init_obj_verts = obj_pcs[0]
        init_obj_rot_vec = object_global_orient[0]
        init_obj_transl = object_transl[0]
        
        init_obj_transl = torch.from_numpy(init_obj_transl).float().cuda()
        init_rot_struct = R.from_rotvec(init_obj_rot_vec)
        
        init_glb_rot_mtx = init_rot_struct.as_matrix()
        init_glb_rot_mtx = torch.from_numpy(init_glb_rot_mtx).float().cuda()
        # ## reverse the global rotation matrix ##
        init_glb_rot_mtx_reversed = init_glb_rot_mtx.contiguous().transpose(1, 0).contiguous()
        # nn_obj_verts x  3 ##
        #####  ## initial tarns of the object  and the hand ##
        # canon_obj_verts = torch.matmul(
        #     (init_obj_verts - init_obj_transl.unsqueeze(0)), init_glb_rot_mtx.contiguous().transpose(1, 0).contiguous()
        # )
        
        ## (R (v - t)^T)^T = (v - t) R^T
        canon_obj_verts = torch.matmul(
            init_glb_rot_mtx_reversed.transpose(1, 0).contiguous(), (init_obj_verts - init_obj_transl.unsqueeze(0)).contiguous().transpose(1, 0).contiguous()
        ).contiguous().transpose(1, 0).contiguous()

        ## get canon obj verts ##
        
        # canon_obj_verts = obj_pcs[0].clone()
        self.obj_verts = canon_obj_verts.clone()
        obj_verts = canon_obj_verts.clone()
        
        
        #### save canonical obj mesh ####
        print(f"canon_obj_verts: {canon_obj_verts.size()}, obj_faces: {obj_faces.size()}")
        canon_obj_mesh = trimesh.Trimesh(vertices=canon_obj_verts.detach().cpu().numpy(), faces=obj_faces.detach().cpu().numpy())   
        canon_obj_mesh.export(gt_data_canon_obj_sv_fn)
        print(f"canonical obj exported to {gt_data_canon_obj_sv_fn}")
        #### save canonical obj mesh ####
        
        
        
        # # glb_rot xx obj_verts + obj_trans = cur_obj_verts 
        # canon_obj_verts = torch.matmul(
        #     init_glb_rot_mtx.transpose(1, 0).contiguous(), self.obj_verts[0] - init_obj_transl.unsqueeze(0)
        # )
        
        
        # obj_verts = torch.from_numpy(template_obj_vs).float().cuda()
        
        self.obj_verts = obj_verts.clone()
        
        
        mesh_scale = 0.8
        bbmin, _ = obj_verts.min(0) #
        bbmax, _ = obj_verts.max(0) #
        
        center = (bbmin + bbmax) * 0.5
        scale = 2.0 * mesh_scale / (bbmax - bbmin).max() # bounding box's max #
        # vertices = (vertices - center) * scale # (vertices - center) * scale # #
        
        self.sdf_space_center = center
        self.sdf_space_scale = scale
        # sdf_sv_fn = self.sdf_sv_fn
        # self.obj_sdf = np.load(sdf_sv_fn, allow_pickle=True)
        # self.sdf_res = self.obj_sdf.shape[0]
        
        
        # init_obj_pcs = obj_pcs[0].detach().cpu().numpy()
        # init_glb_rot = object_global_orient[0]
        # init_glb_trans = object_transl[0]
        # init_glb_rot_struct = R.from_rotvec(init_glb_rot)
        # init_glb_rot_mtx = init_glb_rot_struct.as_matrix()
        # self.obj_verts = np.matmul((init_obj_pcs - init_glb_trans[None]), init_glb_rot_mtx.T)
        # obj_verts = self.obj_verts
        # minn_verts = np.min(obj_verts, axis=0)
        # maxx_verts = np.max(obj_verts, axis=0)
        # extent = maxx_verts - minn_verts
        # scale_cur = np.sqrt(np.sum(extent ** 2))
        
        # center_cur= (minn_verts + maxx_verts) / 2
        
        # obj_verts = (sv_dict['obj_verts'] - center_ori[None]) / scale_ori * scale_cur + center_cur[None]
        
        # obj_verts = torch.from_numpy(obj_verts).float().cuda()
        # self.obj_verts = obj_verts
        
        # sv_fn_obj_fn = sv_fn[:-4] + "_real_obj.obj"
        # scaled_obj = trimesh.Trimesh(vertices=self.obj_verts.detach().cpu().numpy(), faces=self.obj_faces.detach().cpu().numpy(), vertex_normals=self.obj_normals.detach().cpu().numpy())
        # scaled_obj.export(sv_fn_obj_fn)
        # print(f"Scaled obj saved to {scaled_obj}")
        
        
        
        tot_reversed_obj_rot_mtx = []
        tot_obj_quat = [] ## rotation matrix 
        
        re_transformed_obj_verts = []
        
        # transformed_obj_verts = []
        for i_fr in range(object_global_orient.shape[0]):
            cur_glb_rot = object_global_orient[i_fr]
            cur_transl = object_transl[i_fr]
            cur_transl = torch.from_numpy(cur_transl).float().cuda()
            cur_glb_rot_struct = R.from_rotvec(cur_glb_rot)
            cur_glb_rot_mtx = cur_glb_rot_struct.as_matrix()
            cur_glb_rot_mtx = torch.from_numpy(cur_glb_rot_mtx).float().cuda()
            
            # transformed verts ## canon_verts x R + t = transformed_verts #
            # (transformed_verts - t) x R^T = canon_verts #
            # cur_transformed_verts = torch.matmul(
            #     self.obj_verts, cur_glb_rot_mtx
            # ) + cur_transl.unsqueeze(0)
            
            cur_glb_rot_mtx_reversed = cur_glb_rot_mtx.contiguous().transpose(1, 0).contiguous()
            tot_reversed_obj_rot_mtx.append(cur_glb_rot_mtx_reversed)
            
            cur_glb_rot_struct = R.from_matrix(cur_glb_rot_mtx_reversed.cpu().numpy())
            cur_obj_quat = cur_glb_rot_struct.as_quat()
            cur_obj_quat = cur_obj_quat[[3, 0, 1, 2]]
            cur_obj_quat = torch.from_numpy(cur_obj_quat).float().cuda()
            tot_obj_quat.append(cur_obj_quat)
            
            cur_re_transformed_obj_verts = torch.matmul(
                cur_glb_rot_mtx_reversed, self.obj_verts.transpose(1, 0)
            ).transpose(1, 0) + cur_transl.unsqueeze(0)
            re_transformed_obj_verts.append(cur_re_transformed_obj_verts)
            
            # cur_re_transformed_obj_verts = torch.matmul(
            #     cur_glb_rot_mtx, self.obj_verts.transpose(1, 0)
            # ).transpose(1, 0) + cur_transl.unsqueeze(0)
            # re_transformed_obj_verts.append(cur_re_transformed_obj_verts)

            # center_obj_verts = torch.mean(self.obj_verts, dim=0, keepdim=True)
            # cur_transformed_verts = torch.matmul(
            #     (self.obj_verts - center_obj_verts), cur_glb_rot_mtx
            # ) + cur_transl.unsqueeze(0) + center_obj_verts
            
            # cur_transformed_verts = torch.matmul(
            #     cur_glb_rot_mtx, self.obj_verts.transpose(1, 0)
            # ).contiguous().transpose(1, 0).contiguous() + cur_transl.unsqueeze(0)
            # transformed_obj_verts.append(self.obj_)
        # transformed_obj_verts = torch.stack(transformed_obj_verts, dim=0)
        
        transformed_obj_verts = obj_pcs.clone()
        
        
        
        # rhand_verts = sv_dict['rhand_verts']
        # rhand_verts = torch.from_numpy(rhand_verts).float().cuda()
        # self.rhand_verts = rhand_verts ## rhand verts ## 
        
        
        self.mano_path = "/data1/xueyi/mano_models/mano/models" ### mano_path
        if not os.path.exists(self.mano_path):
            self.mano_path = '/data/xueyi/mano_v1_2/models'
        self.rgt_mano_layer = ManoLayer(
            flat_hand_mean=False,
            side='right',
            mano_root=self.mano_path,
            ncomps=45,
            use_pca=False,
        ).cuda()
        
        self.lft_mano_layer = ManoLayer(
            flat_hand_mean=False,
            side='left',
            mano_root=self.mano_path,
            ncomps=45,
            use_pca=False,
        ).cuda()
        
        
        ##### rhand parameters #####
        rhand_global_orient_gt, rhand_pose_gt = sv_dict["rot_r"], sv_dict["pose_r"]
        # print(f"rhand_global_orient_gt: {rhand_global_orient_gt.shape}")
        rhand_global_orient_gt = rhand_global_orient_gt[start_idx: start_idx + self.window_size]
        # print(f"rhand_global_orient_gt: {rhand_global_orient_gt.shape}, start_idx: {start_idx}, window_size: {self.window_size}, len: {self.len}")
        rhand_pose_gt = rhand_pose_gt[start_idx: start_idx + self.window_size]
        
        rhand_global_orient_gt = rhand_global_orient_gt.reshape(self.window_size, -1).astype(np.float32)
        rhand_pose_gt = rhand_pose_gt.reshape(self.window_size, -1).astype(np.float32)
        
        rhand_transl, rhand_betas = sv_dict["trans_r"], sv_dict["shape_r"][0]
        rhand_transl, rhand_betas = rhand_transl[start_idx: start_idx + self.window_size], rhand_betas
        
        # print(f"rhand_transl: {rhand_transl.shape}, rhand_betas: {rhand_betas.shape}")
        rhand_transl = rhand_transl.reshape(self.window_size, -1).astype(np.float32)
        rhand_betas = rhand_betas.reshape(-1).astype(np.float32)
        
        rhand_global_orient_var = torch.from_numpy(rhand_global_orient_gt).float().cuda()
        rhand_pose_var = torch.from_numpy(rhand_pose_gt).float().cuda()
        rhand_beta_var = torch.from_numpy(rhand_betas).float().cuda()
        rhand_transl_var = torch.from_numpy(rhand_transl).float().cuda()
        # R.from_rotvec(obj_rot).as_matrix()
        ##### rhand parameters #####
        
        
        ##### lhand parameters #####
        lhand_global_orient_gt, lhand_pose_gt = sv_dict["rot_l"], sv_dict["pose_l"]
        # print(f"rhand_global_orient_gt: {rhand_global_orient_gt.shape}")
        lhand_global_orient_gt = lhand_global_orient_gt[start_idx: start_idx + self.window_size]
        # print(f"rhand_global_orient_gt: {rhand_global_orient_gt.shape}, start_idx: {start_idx}, window_size: {self.window_size}, len: {self.len}")
        lhand_pose_gt = lhand_pose_gt[start_idx: start_idx + self.window_size]
        
        lhand_global_orient_gt = lhand_global_orient_gt.reshape(self.window_size, -1).astype(np.float32)
        lhand_pose_gt = lhand_pose_gt.reshape(self.window_size, -1).astype(np.float32)
        
        lhand_transl, lhand_betas = sv_dict["trans_l"], sv_dict["shape_l"][0]
        lhand_transl, lhand_betas = lhand_transl[start_idx: start_idx + self.window_size], lhand_betas
        
        # print(f"rhand_transl: {rhand_transl.shape}, rhand_betas: {rhand_betas.shape}")
        lhand_transl = lhand_transl.reshape(self.window_size, -1).astype(np.float32)
        lhand_betas = lhand_betas.reshape(-1).astype(np.float32)
        
        lhand_global_orient_var = torch.from_numpy(lhand_global_orient_gt).float().cuda()
        lhand_pose_var = torch.from_numpy(lhand_pose_gt).float().cuda()
        lhand_beta_var = torch.from_numpy(lhand_betas).float().cuda()
        lhand_transl_var = torch.from_numpy(lhand_transl).float().cuda() # self.window_size x 3
        # R.from_rotvec(obj_rot).as_matrix()
        ##### lhand parameters #####
        
    
        
        rhand_verts, rhand_joints = self.rgt_mano_layer(
            torch.cat([rhand_global_orient_var, rhand_pose_var], dim=-1),
            rhand_beta_var.unsqueeze(0).repeat(self.window_size, 1).view(-1, 10), rhand_transl_var
        )
        ### rhand_joints: for joints ###
        rhand_verts = rhand_verts * 0.001
        rhand_joints = rhand_joints * 0.001
        
        
        lhand_verts, lhand_joints = self.lft_mano_layer(
            torch.cat([lhand_global_orient_var, lhand_pose_var], dim=-1),
            lhand_beta_var.unsqueeze(0).repeat(self.window_size, 1).view(-1, 10), lhand_transl_var
        )
        ### rhand_joints: for joints ###
        lhand_verts = lhand_verts * 0.001
        lhand_joints = lhand_joints * 0.001
        
        
        ### lhand and the rhand ###
        # rhand_verts, lhand_verts #
        self.rhand_verts = rhand_verts
        self.lhand_verts = lhand_verts 
        
        self.hand_faces = self.rgt_mano_layer.th_faces
        
        
        
        if '30_sv_dict' in sv_fn:
            bbox_selected_verts_idxes = torch.tensor([1511, 1847, 2190, 2097, 2006, 2108, 1604], dtype=torch.long).cuda()
            obj_selected_verts = self.obj_verts[bbox_selected_verts_idxes]
        else:
            obj_selected_verts = self.obj_verts.clone()
        
        maxx_init_passive_mesh, _ = torch.max(obj_selected_verts, dim=0)
        minn_init_passive_mesh, _ = torch.min(obj_selected_verts, dim=0)
        self.maxx_init_passive_mesh = maxx_init_passive_mesh
        self.minn_init_passive_mesh = minn_init_passive_mesh
        
        
        init_obj_verts = obj_verts # [0] # cannnot rotate it at all # frictional forces in the pybullet? # 
        
        mesh_scale = 0.8
        bbmin, _ = init_obj_verts.min(0) #
        bbmax, _ = init_obj_verts.max(0) #
        print(f"bbmin: {bbmin}, bbmax: {bbmax}")
        center = (bbmin + bbmax) * 0.5
        
        self.obj_normals = torch.zeros_like(obj_verts)
        
        scale = 2.0 * mesh_scale / (bbmax - bbmin).max() # bounding box's max #
        # vertices = (vertices - center) * scale # (vertices - center) * scale # #
        
        self.sdf_space_center = center.detach().cpu().numpy()
        self.sdf_space_scale = scale.detach().cpu().numpy()
        # sdf_sv_fn = "/data/xueyi/diffsim/NeuS/init_box_mesh.npy"
        # if not os.path.exists(sdf_sv_fn):
        #     sdf_sv_fn = "/home/xueyi/diffsim/NeuS/init_box_mesh.npy"
        # self.obj_sdf = np.load(sdf_sv_fn, allow_pickle=True)
        # self.sdf_res = self.obj_sdf.shape[0]
        # print(f"obj_sdf loaded from {sdf_sv_fn} with shape {self.obj_sdf.shape}")
        
        re_transformed_obj_verts = torch.stack(re_transformed_obj_verts, dim=0)
        self.re_transformed_obj_verts = re_transformed_obj_verts
        
        # tot_obj_quat, tot_reversed_obj_rot_mtx #
        tot_obj_quat = torch.stack(tot_obj_quat, dim=0) ## tot obj quat ##
        tot_reversed_obj_rot_mtx = torch.stack(tot_reversed_obj_rot_mtx, dim=0)
        self.tot_obj_quat = tot_obj_quat # obj quat #
        
        # self.tot_obj_quat[0, 0] = 1.
        # self.tot_obj_quat[0, 1] = 0.
        # self.tot_obj_quat[0, 2] = 0.
        # self.tot_obj_quat[0, 3] = 0.
        
        self.tot_reversed_obj_rot_mtx = tot_reversed_obj_rot_mtx
        
        # self.tot_reversed_obj_rot_mtx[0] = torch.eye(3, dtype=torch.float32).cuda()
        
        ## should save self.object_global_orient and self.object_transl ##
        # object_global_orient, object_transl #
        self.object_global_orient = torch.from_numpy(object_global_orient).float().cuda()
        self.object_transl = torch.from_numpy(object_transl).float().cuda()
        
        # self.object_transl[0, :] = self.object_transl[0, :] * 0.0
        return transformed_obj_verts, rhand_verts, obj_tot_normals
    
    
    
    
    
    # self.center_verts, self.ball_r = self.get_ball_primitives()
    def get_ball_primitives(self, ):
        # obj_verts # 
        ## get the maximum outer ball #3
        maxx_verts, _ = torch.max(self.obj_verts, dim=0)
        minn_verts, _ = torch.min(self.obj_verts, dim=0) # 
        center_verts = (maxx_verts + minn_verts) / 2.
        extent_verts = (maxx_verts - minn_verts)
        ball_d = max(extent_verts[0].item(), max(extent_verts[1].item(), extent_verts[2].item()))
        ball_r = ball_d / 2.
        return center_verts, ball_r
    
    def load_active_passive_timestep_to_mesh(self,): # timestep to mesh #
        # sv_fn = ".../sv_dict.npy" ## box grab extracted dict ### and should not be the simple and single object #
        # multiple object and multiple data #
        sv_fn = "/data2/xueyi/arctic_processed_data/processed_sv_dicts/s01/box_grab_01_extracted_dict.npy"
        if not os.path.exists(sv_fn):
            sv_fn = "/data/xueyi/arctic/processed_sv_dicts/box_grab_01_extracted_dict.npy"
        active_passive_sv_dict = np.load(sv_fn, allow_pickle=True).item() # 

        obj_verts = active_passive_sv_dict['obj_verts'] # object orientation #
        obj_faces = active_passive_sv_dict['obj_faces']
        
        
        init_obj_verts = obj_verts[0] # load obj vert
        
        # the solution of ## obj
        ### get the boundary information of the SDF grid ###
        # init_obj_verts # 
        mesh_scale = 0.8
        bbmin = init_obj_verts.min(0) #
        bbmax = init_obj_verts.max(0) #
        center = (bbmin + bbmax) * 0.5
        scale = 2.0 * mesh_scale / (bbmax - bbmin).max() # bounding box's max #
        # vertices = (vertices - center) * scale # (vertices - center) * scale # #
        
        # for a posed vertex in frame k #
        # back transform the vertex to the original initial frame #
        # scale the transformed vertex to the space on which the sdf is compute #
        # compute the sdf index via [(x + 1.) / 2 * res, ...] #
        # self.sdf_space_center, self.sdf_space_scale, self.obj_sdf #
        
        self.sdf_space_center = center
        self.sdf_space_scale = scale
        sdf_sv_fn = "/data/xueyi/diffsim/NeuS/init_box_mesh.npy"
        if not os.path.exists(sdf_sv_fn):
            sdf_sv_fn = "/home/xueyi/diffsim/NeuS/init_box_mesh.npy"
        self.obj_sdf = np.load(sdf_sv_fn, allow_pickle=True)
        self.sdf_res = self.obj_sdf.shape[0]
        print(f"obj_sdf loaded from {sdf_sv_fn} with shape {self.obj_sdf.shape}")
        
        sdf_grad_sv_fn = "/home/xueyi/diffsim/NeuS/init_box_mesh_sdf_grad.npy"
        # obj_sdf_grad
        if os.path.exists(sdf_grad_sv_fn):
            self.obj_sdf_grad = np.load(sdf_grad_sv_fn, allow_pickle=True)
            print(f"obj_sdf_grad loaded from {sdf_grad_sv_fn} with shape {self.obj_sdf_grad.shape}")
        else:
            self.obj_sdf_grad = None
                
        init_trimesh = trimesh.Trimesh(vertices=init_obj_verts, faces=obj_faces)
        mesh_exported_path = 'init_box_mesh.ply'
        init_trimesh.export('init_box_mesh.ply')
        
        o3d_mesh = o3d.io.read_triangle_mesh(mesh_exported_path)
        o3d_mesh.compute_vertex_normals()
        init_verts_normals = o3d_mesh.vertex_normals
        init_verts_normals = np.array(init_verts_normals, dtype=np.float32)
        init_verts_normals = torch.from_numpy(init_verts_normals).float().cuda()
        
        rhand_verts = active_passive_sv_dict['rhand_verts']
        lhand_verts = active_passive_sv_dict['lhand_verts']
        hand_verts = np.concatenate([rhand_verts, lhand_verts], axis=1)
        obj_verts = torch.from_numpy(obj_verts).float().cuda()
        
        hand_verts = torch.from_numpy(hand_verts).float().cuda()
        # rhand_verts = ## 
        # self.hand_faces, self.obj_faces # # rhand verts and lhand verts #
        hand_faces = active_passive_sv_dict['hand_faces']
        
        self.rhand_verts = torch.from_numpy(rhand_verts).float().cuda()
        self.hand_faces = torch.from_numpy(hand_faces).long().cuda() # hand faces ##
        self.obj_faces = torch.tensor(obj_faces).long().cuda()
        self.obj_normals = {0: init_verts_normals}
        
        # rhand verts and hand faces and obj vertices # 
        dist_rhand_verts_to_obj_vertts = torch.sum(
            (self.rhand_verts.unsqueeze(2) - obj_verts.unsqueeze(1)) ** 2, dim=-1
        ) ## nn_hand_verts x nn_obj_verts ## 
        dist_rhand_verts_to_obj_vertts, _ = torch.min(dist_rhand_verts_to_obj_vertts, dim=-1) ### nn_hand_verts ##
        self.ts_to_contact_pts = {}
        thresold_dist = 0.001
        for ts in range(self.rhand_verts.size(0)):
            cur_contact_pts_threshold_indicator = dist_rhand_verts_to_obj_vertts[ts] <= thresold_dist
            if torch.sum(cur_contact_pts_threshold_indicator.float()).item() > 0.1:
                self.ts_to_contact_pts[ts] = self.rhand_verts[ts][cur_contact_pts_threshold_indicator]
        
        print(f"obj_verts: {obj_verts.size()}, hand_verts: {hand_verts.size()}, rhand_verts: {rhand_verts.shape}, lhand_verts: {lhand_verts.shape}, hand_faces: {self.hand_faces.size()}, obj_faces: {self.obj_faces.size()}")
        return obj_verts, hand_verts, self.obj_normals
    
    
    
    def load_active_passive_timestep_to_mesh_v2(self,): # timestep to mesh #
        # sv_fn = ".../sv_dict.npy" ## box grab extracted dict ### and should not be the simple and single object #
        # multiple object and multiple data #
        sv_fn = "/data2/datasets/sim/arctic_processed_data/processed_sv_dicts/s01/box_grab_01_extracted_dict.npy"
        if not os.path.exists(sv_fn):
            sv_fn = "/data/xueyi/arctic/processed_sv_dicts/box_grab_01_extracted_dict.npy"
        active_passive_sv_dict = np.load(sv_fn, allow_pickle=True).item() # 

        ##### get object vertices and faces #####
        obj_verts = active_passive_sv_dict['obj_verts']
        obj_faces = active_passive_sv_dict['obj_faces']
        init_obj_verts = obj_verts[0]

        ### get the boundary information of the SDF grid ### ## # SDF 
        mesh_scale = 0.8
        bbmin = init_obj_verts.min(0) #
        bbmax = init_obj_verts.max(0) #
        center = (bbmin + bbmax) * 0.5
        scale = 2.0 * mesh_scale / (bbmax - bbmin).max() # bounding box's max #
        # vertices = (vertices - center) * scale # (vertices - center) * scale # #
        
        # 
        
        # for a posed vertex in frame k #
        # back transform the vertex to the original initial frame #
        # scale the transformed vertex to the space on which the sdf is compute #
        # compute the sdf index via [(x + 1.) / 2 * res, ...] #
        # self.sdf_space_center, self.sdf_space_scale, self.obj_sdf #
        
        ### load the sdf for the box ###
        self.sdf_space_center = center
        self.sdf_space_scale = scale
        sdf_sv_fn = "/data/xueyi/diffsim/NeuS/init_box_mesh.npy"
        if not os.path.exists(sdf_sv_fn):
            sdf_sv_fn = "/home/xueyi/diffsim/NeuS/init_box_mesh.npy"
        self.obj_sdf = np.load(sdf_sv_fn, allow_pickle=True)
        self.sdf_res = self.obj_sdf.shape[0] # 
        print(f"obj_sdf loaded from {sdf_sv_fn} with shape {self.obj_sdf.shape}")
        ### load the sdf for the box ###
                
        init_trimesh = trimesh.Trimesh(vertices=init_obj_verts, faces=obj_faces)
        mesh_exported_path = 'init_box_mesh.ply'
        init_trimesh.export('init_box_mesh.ply')
        
        o3d_mesh = o3d.io.read_triangle_mesh(mesh_exported_path)
        o3d_mesh.compute_vertex_normals()
        init_verts_normals = o3d_mesh.vertex_normals
        init_verts_normals = np.array(init_verts_normals, dtype=np.float32)
        init_verts_normals = torch.from_numpy(init_verts_normals).float().cuda()
        
        rhand_verts = active_passive_sv_dict['rhand_verts']
        lhand_verts = active_passive_sv_dict['lhand_verts']
        hand_verts = np.concatenate([rhand_verts, lhand_verts], axis=1)
        
        obj_verts = torch.from_numpy(obj_verts).float().cuda()
        hand_verts = torch.from_numpy(hand_verts).float().cuda()
        # rhand_verts = ## 
        # self.hand_faces, self.obj_faces # # rhand verts and lhand verts #
        hand_faces = active_passive_sv_dict['hand_faces']
        
        self.rhand_verts = torch.from_numpy(rhand_verts).float().cuda()
        self.hand_faces = torch.from_numpy(hand_faces).long().cuda()
        self.obj_faces = torch.tensor(obj_faces).long().cuda()
        self.obj_normals = {0: init_verts_normals}
        
        # # rhand verts and hand faces and obj vertices # 
        # dist_rhand_verts_to_obj_vertts = torch.sum(
        #     (self.rhand_verts.unsqueeze(2) - obj_verts.unsqueeze(1)) ** 2, dim=-1
        # ) ## nn_hand_verts x nn_obj_verts ## 
        # dist_rhand_verts_to_obj_vertts, _ = torch.min(dist_rhand_verts_to_obj_vertts, dim=-1) ### nn_hand_verts ##
        # self.ts_to_contact_pts = {}
        # thresold_dist = 0.001
        # for ts in range(self.rhand_verts.size(0)):
        #     cur_contact_pts_threshold_indicator = dist_rhand_verts_to_obj_vertts[ts] <= thresold_dist
        #     if torch.sum(cur_contact_pts_threshold_indicator.float()).item() > 0.1:
        #         self.ts_to_contact_pts[ts] = self.rhand_verts[ts][cur_contact_pts_threshold_indicator]
        
        # print(f"obj_verts: {obj_verts.size()}, hand_verts: {hand_verts.size()}, rhand_verts: {rhand_verts.shape}, lhand_verts: {lhand_verts.shape}, hand_faces: {self.hand_faces.size()}, obj_faces: {self.obj_faces.size()}")
        return obj_verts, self.rhand_verts, self.obj_normals
        
    
    # from the model and the optimized rules #
    def optimize_states_from_meshes_seq(self, ):
        # /data2/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime/wmask_reverse_value_totviews_tag_act_xs_def_n5_bending_net/meshes/00028500_tets_ts_1.ply
        sv_mesh_root_fn = self.conf['model.sv_mesh_root']
        target_meshes_verts = []
        for i_ts in range(self.n_timesteps):
            # if i_ts == 0:
            #     cur_ts_mesh_fn = os.path.join(sv_mesh_root_fn, f"00028500_tets.ply")
            # else:
            cur_ts_mesh_fn = os.path.join(sv_mesh_root_fn, f"00258500_tets_ts_{i_ts}.ply")
            obj_mesh = trimesh.load(cur_ts_mesh_fn, process=False)
            cur_mesh_verts = np.array(obj_mesh.vertices)
            target_meshes_verts.append(torch.from_numpy(cur_mesh_verts).float().cuda())
        # target_meshes_verts = torch.stack(target_meshes_verts, dim=0) ##
        tot_optimized_pts = []
        for i_ts in range(self.n_timesteps):
            n_sampling = 5000 ## GT_mesh_verts ## # point sampling #
            # pts_fps_idx = data_utils.farthest_point_sampling(self.GT_mesh_verts.cuda().unsqueeze(0), n_sampling=n_sampling) # farthes points sampling ##
            # # pts = pts[pts_fps_idx].cpu() # GT_mesh_verts #
            # # mesh vertices #
            # self.GT_mesh_verts = self.GT_mesh_verts[pts_fps_idx].cpu()
            for i_iter in range(1000):
                cur_target_mesh_verts = target_meshes_verts[i_ts]
                
                cur_state = self.state_vals[i_ts]
                cur_active_robot_pts = []
                cur_active_robot_pts = self.active_robot.compute_transformation_via_state_vecs(cur_state, cur_active_robot_pts)
                cur_active_robot_pts = torch.cat(cur_active_robot_pts, dim=0) ### cat pts ###
                
                cur_active_robot_pts =  (cur_active_robot_pts - self.minn_pts) / self.extent
                cur_active_robot_pts = cur_active_robot_pts * 2. - 1.
                ori_cur_active_robot_pts = cur_active_robot_pts.clone()
                cur_active_robot_pts = cur_active_robot_pts[self.act_pts_fps_idx]
                minn_act_pts, _ = torch.min(cur_active_robot_pts, dim=0)
                maxx_act_pts, _ = torch.max(cur_active_robot_pts, dim=0)
                
                minn_tar_pts, _ = torch.min(cur_target_mesh_verts, dim=0)
                maxx_tar_pts, _ = torch.max(cur_target_mesh_verts, dim=0)
                
                # print(f"cur_target_mesh_verts: {cur_target_mesh_verts.size()}")
                cd_cur_active_to_target = torch.sum(
                    (cur_active_robot_pts.unsqueeze(1) - cur_target_mesh_verts.unsqueeze(0)) ** 2, dim=-1
                )
                cd_cur_active_to_target = torch.sqrt(cd_cur_active_to_target)
                dist_cur_to_tar, _ = torch.min(cd_cur_active_to_target, dim=-1)
                dist_tar_to_cur, _ = torch.min(cd_cur_active_to_target, dim=0)
                cd_cur = 0.5 * (dist_cur_to_tar.mean() + dist_tar_to_cur.mean())
                cd_cur.backward()
                
                lr = 0.1
                self.state_vals.data = self.state_vals.data - lr * self.state_vals.grad.data
                self.state_vals.grad.data = self.state_vals.grad.data * 0.
                self.active_robot.clear_grads()
                print(f"i_ts: {i_ts}, i_iter: {i_iter}, state_vals: {self.state_vals.data}, cd_cur: {cd_cur.item()}, minn_act_pts: {minn_act_pts}, maxx_act_pts: {maxx_act_pts}, minn_tar_pts: {minn_tar_pts}, maxx_tar_pts: {maxx_tar_pts}")
        
            tot_optimized_pts.append(ori_cur_active_robot_pts.detach().cpu().numpy())
        tot_optimized_pts = np.stack(tot_optimized_pts, axis=0)
        np.save(f"optimized_act_pts.npy", tot_optimized_pts)
        tot_state_vals = self.state_vals.data.detach().cpu().numpy()
        state_vals_sv_fn = "optimized_states.npy"
        np.save(state_vals_sv_fn, tot_state_vals)
        print(f"optimized states saved to {state_vals_sv_fn}")
    
    
    
    def optimize_states_from_model_rules(self, ):
        # get the mehs seq #
        # /data2/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime/wmask_reverse_value_totviews_tag_act_xs_def_n5_bending_net/meshes/00028500_tets_ts_1.ply
        sv_mesh_root_fn = self.conf['model.sv_mesh_root']
        target_meshes_verts = []
        for i_ts in range(self.n_timesteps):
            # if i_ts == 0:
            #     cur_ts_mesh_fn = os.path.join(sv_mesh_root_fn, f"00028500_tets.ply")
            # else:
            cur_ts_mesh_fn = os.path.join(sv_mesh_root_fn, f"00258500_tets_ts_{i_ts}.ply")
            obj_mesh = trimesh.load(cur_ts_mesh_fn, process=False)
            cur_mesh_verts = np.array(obj_mesh.vertices)
            target_meshes_verts.append(torch.from_numpy(cur_mesh_verts).float().cuda())
        # target_meshes_verts = torch.stack(target_meshes_verts, dim=0) ##
        tot_optimized_pts = []
        
        for i_iter in range(1000):
            n_sampling = 5000 ## GT_mesh_verts ##
            # pts_fps_idx = data_utils.farthest_point_sampling(self.GT_mesh_verts.cuda().unsqueeze(0), n_sampling=n_sampling) # farthes points sampling ##
            # # pts = pts[pts_fps_idx].cpu() # GT_mesh_verts #
            # # mesh vertices #
            # self.GT_mesh_verts = self.GT_mesh_verts[pts_fps_idx].cpu() #
            cur_timestep_to_active_robot_pts = {}
            for i_ts in range(self.n_timesteps):
                cur_target_mesh_verts = target_meshes_verts[i_ts]
                
                cur_state = self.state_vals[i_ts]
                cur_active_robot_pts = []
                cur_active_robot_pts = self.active_robot.compute_transformation_via_state_vecs(cur_state, cur_active_robot_pts)
                cur_active_robot_pts = torch.cat(cur_active_robot_pts, dim=0) ### cat pts ###
                
                cur_active_robot_pts =  (cur_active_robot_pts - self.minn_pts) / self.extent
                cur_active_robot_pts = cur_active_robot_pts * 2. - 1.
                
                ori_cur_active_robot_pts = cur_active_robot_pts.clone()
                cur_active_robot_pts = cur_active_robot_pts[self.act_pts_fps_idx]
                minn_act_pts, _ = torch.min(cur_active_robot_pts, dim=0) # minn_act_pts #
                maxx_act_pts, _ = torch.max(cur_active_robot_pts, dim=0) # maxx_act_pts #
                
                
                # minn_act_pts; maxx_act_pts #
                # cur_active_robot_pts # # cur_active_robot
                cur_timestep_to_active_robot_pts[i_ts] = cur_active_robot_pts #
            # cur_timestep_to_active_robot_pts ###
                # minn_tar_pts, _ = # 
                # minn_tar_pts, _ = torch.min(cur_target_mesh_verts, dim=0) # 
                # maxx_tar_pts, _ = torch.max(cur_target_mesh_verts, dim=0) # 
                
                # # print(f"cur_target_mesh_verts: {cur_target_mesh_verts.size()}")
                # cd_cur_active_to_target = torch.sum(
                #     (cur_active_robot_pts.unsqueeze(1) - cur_target_mesh_verts.unsqueeze(0)) ** 2, dim=-1
                # )
                # cur_active_robot_pts -> nn_act_pts x 3 #
        pass
    
      
        
    def get_canonicalized_obj_act_pts(self, ):
        zero_active_states = torch.zeros_like(self.state_vals[0, :self.n_act_states])
        zero_state_active_robot_pts = []
        zero_state_active_robot_pts = self.active_robot.compute_transformation_via_state_vecs(zero_active_states, zero_state_active_robot_pts)
        zero_state_active_robot_pts = torch.cat(zero_state_active_robot_pts, dim=0) ## get the active robot pts #
        self.zero_state_active_robot_pts = zero_state_active_robot_pts
        self.zero_state_active_robot_pts = (self.zero_state_active_robot_pts - self.minn_pts) / self.extent ## 
        self.zero_state_active_robot_pts = self.zero_state_active_robot_pts * 2 - 1.
        
        
    def get_deformed_act_pts(self, ): # act_pts #
        tot_frame_act_xs = []
        for i_fr in range(self.n_timesteps): # act_pts #
            cur_fr_active_states = self.state_vals[i_fr, :self.n_act_states]
            
            cur_active_robot_pts = []
            cur_active_robot_pts = self.active_robot.compute_transformation_via_state_vecs(cur_fr_active_states, cur_active_robot_pts) # 
            cur_active_robot_pts = torch.cat(cur_active_robot_pts, dim=0) ### cat pts
            
            tot_frame_act_xs.append(cur_active_robot_pts)
        tot_frame_act_xs = torch.stack(tot_frame_act_xs, dim=0)
        # # get the act data # 
        
        if self.gt_act_xs_def:
            tot_frame_act_xs = self.gt_act_xs.clone()
        
        tot_frame_act_xs = (tot_frame_act_xs - self.minn_pts) / self.extent
        tot_frame_act_xs = tot_frame_act_xs * 2. - 1. # 
        
        return tot_frame_act_xs
    
    # get bkg pts deformation #
    def get_bkg_pts_deformation(self, act_xs): # get the delta xs and the delta xs #
        # get bkg pts deformation # # 
        act_xs = act_xs[:, self.act_pts_fps_idx, :] # 
        act_xs_deformation = act_xs - self.zero_state_active_robot_pts.detach().unsqueeze(0) ## get the deformation field #
        
        # act_xs_deformation: nn_frames x nn_act_pts x 3 -> as the
        # bkg_pts_deformation: nn_frames x nn_bkg_pts x j ##
        # nn_bkg_pts #
        # act_xs_deformation #
        # interpolate over the grids #
        # identify a circel -> #
        # the distnace to the zero_steat_act_robot_pts #
        # zero_state_active_robot_pts: nn_active_pts x 3 #
        # zero state active robot pts #
        # print(f"zero_state_active_robot_pts: {self.zero_state_active_robot_pts.size()}")
        # fps_idx = 
        rel_between_zero_state_active_pts = torch.sum(
            (self.zero_state_active_robot_pts.unsqueeze(1) - self.zero_state_active_robot_pts.unsqueeze(0)) ** 2, dim=-1
        )
        # rel_between_zero_state_active_pts[]
        pts_arng = torch.arange(start=0, end=self.zero_state_active_robot_pts.size(0)).cuda()
        rel_between_zero_state_active_pts[pts_arng, pts_arng] = 1e9
        minn_dists, _ = torch.min(rel_between_zero_state_active_pts, dim=-1)
        minn_dists = minn_dists.mean()
        
        dists_thres = minn_dists.item() * 5
        
        self.renderer.dist_interp_thres = dists_thres
        
        # dist_bkg_pts_to_zero_act_pts = torch.sum(
        #     (self.bkg_pts.unsqueeze(1) - self.zero_state_active_robot_pts.unsqueeze(0)) ** 2, dim=-1 ## nn_bkg_pts x nn_act_pts     # bkg_pts and the bkg_pts #
        # )
        # bkg_pts_w_act_pts_mask = dist_bkg_pts_to_zero_act_pts <= dists_thres # nn_bkg_pts x nn_act_pts # 
        # bkg_pts_w_act_pts_mask_float = bkg_pts_w_act_pts_mask.float() # nn_bkg_pts x nn_act_pts #
        
        tot_frames_aggre_deforms = []
        for i_fr in range(self.n_timesteps):
            cur_fr_expand_deform = act_xs_deformation[i_fr].unsqueeze(0).repeat(self.bkg_pts.size(0), 1, 1)
            
            cur_fr_act = act_xs[i_fr]
            dist_bkg_pts_to_act_pts = torch.sum(  #### dist threshold; -> the act pts and the act_pts_mask #
                (self.bkg_pts.unsqueeze(1) - cur_fr_act.unsqueeze(0)) ** 2, dim=-1 ## nn_bkg_pts x nn_act_pts
            )
            cur_bkg_pts_w_act_pts_mask = dist_bkg_pts_to_act_pts <= dists_thres
            # cur_bkg_pts_w_act_pts_mask =
            cur_bkg_pts_w_act_pts_mask_float = cur_bkg_pts_w_act_pts_mask.float().detach()
            
            
            ### get the aggregated expanded deformation ### # deform, pts_mask # 
            cur_fr_expand_deform = torch.sum(cur_fr_expand_deform * cur_bkg_pts_w_act_pts_mask_float.unsqueeze(-1), dim=1) # nn_bkg_pts x 3 #
            cur_fr_mask_nns = torch.sum(cur_bkg_pts_w_act_pts_mask_float, dim=-1) # nn_bkg_pts mask nns; min = 1 - deformations 
            cur_fr_mask_nns = torch.clamp(cur_fr_mask_nns, min=1)
            cur_fr_expand_deform = cur_fr_expand_deform / cur_fr_mask_nns.unsqueeze(-1)
            tot_frames_aggre_deforms.append(cur_fr_expand_deform)
        tot_frames_aggre_deforms = torch.stack(tot_frames_aggre_deforms, dim=0) ## n_timesteps x nn_bkg_pts x 
        
        if self.iter_step % self.val_freq == 0:
            sv_deform_pts = {
                'tot_frames_aggre_deforms': tot_frames_aggre_deforms.detach().cpu().numpy(),
                'bkg_pts': self.bkg_pts.detach().cpu().numpy(),
            }
            sv_deform_pts_sv_fn = os.path.join(self.base_exp_dir, 'meshes')
            os.makedirs(sv_deform_pts_sv_fn, exist_ok=True)
            sv_deform_pts_sv_fn = os.path.join(sv_deform_pts_sv_fn, 'sv_deform_pts_{:0>8d}'.format(self.iter_step))
            np.save(sv_deform_pts_sv_fn, sv_deform_pts)
            # mesh_tets.export(os.path.join(self.base_exp_dir, 'meshes', '{:0>8d}_tets_ts_1.ply'.format(self.iter_step)))
        
        
        return tot_frames_aggre_deforms # the use the pts to get the deformed pts 

        

    def query_delta_mesh_fn(self, pts):
        # for negative 
        # 1) inside the current mesh but outside the previous mesh ---> negative sdf for this field but positive for another field
        # 2) negative in thie field and also negative in the previous field ---> 
        # 2) for positive values of this current field ---> 
        cur_sdf = self.sdf_network.sdf(pts)
        prev_sdf = self.prev_sdf_network.sdf(pts)
        neg_neg = ((cur_sdf < 0.).float() + (prev_sdf < 0.).float()) > 1.5
        neg_pos = ((cur_sdf < 0.).float() + (prev_sdf >= 0.).float()) > 1.5
        pos_neg = ((cur_sdf >= 0.).float() + (prev_sdf < 0.).float()) > 1.5
        pos_pos = ((cur_sdf >= 0.).float() + (prev_sdf >= 0.).float()) > 1.5
        res_sdf = torch.zeros_like(cur_sdf)
        res_sdf[neg_neg] = 1.
        res_sdf[neg_pos] = cur_sdf[neg_pos]
        res_sdf[pos_neg] = cur_sdf[pos_neg]
        
        cat_cur_prev_sdf = torch.stack(
            [cur_sdf, prev_sdf], dim=-1
        )
        minn_cur_prev_sdf, _ = torch.min(cat_cur_prev_sdf, dim=-1)
        res_sdf[pos_pos] = minn_cur_prev_sdf[pos_pos]
        
        return res_sdf
    
    
    def extract_fields_from_tets(self, bound_min, bound_max, resolution, query_func, def_func=None):
        # extract the geometry #
        # /home/xueyi/gen/DeepMetaHandles/data/tets/100_compress.npz # strange #
        device = bound_min.device # 
        # if resolution in [64, 70, 80, 90, 100]: # resolution #
        #     tet_fn = f"/home/xueyi/gen/DeepMetaHandles/data/tets/{resolution}_compress.npz"
        # else: # query_func only -> 
        tet_fn = f"/home/xueyi/gen/DeepMetaHandles/data/tets/{100}_compress.npz"
        if not os.path.exists(tet_fn):
            tet_fn = f"/data/xueyi/NeuS/data/tets/{100}_compress.npz"
        tets = np.load(tet_fn)
        verts = torch.from_numpy(tets['vertices']).float().to(device) # verts positions 
        indices = torch.from_numpy(tets['tets']).long().to(device) # .to(self.device)
        # split #
        # verts; verts; #
        minn_verts, _ = torch.min(verts, dim=0)
        maxx_verts, _ = torch.max(verts, dim=0) # (3, ) # exporting the  
        # scale_verts = maxx_verts - minn_verts
        scale_bounds = bound_max - bound_min # scale bounds # 
        
        ### scale the vertices ###
        scaled_verts = (verts - minn_verts.unsqueeze(0)) / (maxx_verts - minn_verts).unsqueeze(0) ### the maxx and minn verts scales ###
        
        # scaled_verts = (verts - minn_verts.unsqueeze(0)) / (maxx_verts - minn_verts).unsqueeze(0) ### the maxx and minn verts scales ###

        scaled_verts = scaled_verts * 2. - 1. # init the sdf filed viathe tet mesh vertices and the sdf values ##
        # scaled_verts = (scaled_verts * scale_bounds.unsqueeze(0)) + bound_min.unsqueeze(0) ## the scaled verts ###
        
        # scaled_verts = scaled_verts - scale_bounds.unsqueeze(0) / 2. # 
        # scaled_verts = scaled_verts -  bound_min.unsqueeze(0) - scale_bounds.unsqueeze(0) / 2.
        
        sdf_values = []
        N = 64
        query_bundles = N ** 3 ### N^3
        query_NNs = scaled_verts.size(0) // query_bundles
        if query_NNs * query_bundles < scaled_verts.size(0):
            query_NNs += 1
        for i_query in range(query_NNs):
            cur_bundle_st = i_query * query_bundles
            cur_bundle_ed = (i_query + 1) * query_bundles
            cur_bundle_ed = min(cur_bundle_ed, scaled_verts.size(0))
            cur_query_pts = scaled_verts[cur_bundle_st: cur_bundle_ed]
            if def_func is not None:
                cur_query_pts = def_func(cur_query_pts)
            cur_query_vals = query_func(cur_query_pts)
            sdf_values.append(cur_query_vals)
        sdf_values = torch.cat(sdf_values, dim=0)
        # print(f"queryed sdf values: {sdf_values.size()}") #
        
        GT_sdf_values_fn = "/home/xueyi/diffsim/DiffHand/assets/hand/100_sdf_values.npy"
        if not os.path.exists(GT_sdf_values_fn):
            GT_sdf_values_fn = "/data/xueyi/NeuS/data/100_sdf_values.npy"
        GT_sdf_values = np.load(GT_sdf_values_fn, allow_pickle=True)
        # GT_sdf_values = np.load("/home/xueyi/diffsim/DiffHand/assets/hand/100_sdf_values.npy", allow_pickle=True)
        # GT_sdf_values = np.load("/data/xueyi/NeuS/data/100_sdf_values.npy", allow_pickle=True)
        
        GT_sdf_values = torch.from_numpy(GT_sdf_values).float().to(device)
        
        # intrinsic, tet values, pts values, sdf network #
        triangle_table, num_triangles_table, base_tet_edges, v_id = render_utils.create_mt_variable(device)
        tet_table, num_tets_table = render_utils.create_tetmesh_variables(device)
        
        sdf_values = sdf_values.squeeze(-1) # how the rendering # 
        
        # print(f"GT_sdf_values: {GT_sdf_values.size()}, sdf_values: {sdf_values.size()}, scaled_verts: {scaled_verts.size()}")
        # print(f"scaled_verts: {scaled_verts.size()}, ")
        # pos_nx3, sdf_n, tet_fx4, triangle_table, num_triangles_table, base_tet_edges, v_id,
        # return_tet_mesh=False, ori_v=None, num_tets_table=None, tet_table=None):
        # marching_tets_tetmesh ##
        verts, faces, tet_verts, tets = render_utils.marching_tets_tetmesh(scaled_verts, sdf_values, indices, triangle_table, num_triangles_table, base_tet_edges, v_id, return_tet_mesh=True, ori_v=scaled_verts, num_tets_table=num_tets_table, tet_table=tet_table)
        ### use the GT sdf values for the marching tets ###
        GT_verts,  GT_faces,  GT_tet_verts,  GT_tets = render_utils.marching_tets_tetmesh(scaled_verts, GT_sdf_values, indices, triangle_table, num_triangles_table, base_tet_edges, v_id, return_tet_mesh=True, ori_v=scaled_verts, num_tets_table=num_tets_table, tet_table=tet_table)
        
        # print(f"After tet marching with verts: {verts.size()}, faces: {faces.size()}")
        return verts, faces, sdf_values, GT_verts, GT_faces  # verts, faces #


    # tot_l2loss = self.compute_loss_optimized_offset_with_preopt_offset()
    def compute_loss_optimized_offset_with_preopt_offset(self, tot_time_idx):
        # timestep_to_optimizable_offset
        optimized_offset = self.renderer.bending_network[1].timestep_to_optimizable_offset
        preopt_offset = self.ts_to_mesh_offset_for_opt # 
        # tot_l2loss = 0.
        tot_l2losses = []
        # for ts in range(0, self.n_timesteps):
        for ts in range(1, tot_time_idx + 1):
            if ts in optimized_offset and ts in preopt_offset:
                cur_optimized_offset = optimized_offset[ts]
                cur_preopt_offset = preopt_offset[ts]
                diff_optimized_preopt_offset = torch.mean(torch.sum((cur_preopt_offset - cur_optimized_offset) ** 2))
                # if ts == 1:
                #     tot_l2loss = 
                tot_l2losses.append(diff_optimized_preopt_offset)
            # tot_l2loss += diff_optimized_preopt_offset
        tot_l2losses = torch.stack(tot_l2losses, dim=0)
        tot_l2loss = torch.mean(tot_l2losses)
        # tot_l2loss = tot_l2loss / float(self.n_timesteps - 1)
        return tot_l2loss
    
    # tracking_loss = self.compute_loss_optimized_transformations(cur_time_idx)
    def compute_loss_optimized_transformations(self, cur_time_idx):
        # # 
        cur_rot_mtx = self.other_bending_network.timestep_to_optimizable_rot_mtx[cur_time_idx]
        cur_translations = self.other_bending_network.timestep_to_optimizable_total_def[cur_time_idx]
        init_passive_mesh = self.timestep_to_passive_mesh[0]
        center_passive_mesh = torch.mean(init_passive_mesh, dim=0)
        pred_passive_mesh = torch.matmul(
            cur_rot_mtx, (init_passive_mesh - center_passive_mesh.unsqueeze(0)).transpose(1, 0)
        ).transpose(1, 0) + center_passive_mesh.unsqueeze(0) + cur_translations.unsqueeze(0)
        gt_passive_mesh = self.timestep_to_passive_mesh[cur_time_idx]
        tracking_loss = torch.sum(
            (pred_passive_mesh - gt_passive_mesh) ** 2, dim=-1
        ).mean()
        return tracking_loss

    def compute_loss_optimized_transformations_v2(self, cur_time_idx, cur_passive_time_idx):
        # # ## get the 
        
        # timestep_to_optimizable_rot_mtx, timestep_to_optimizable_total_def
        cur_rot_mtx = self.other_bending_network.timestep_to_optimizable_rot_mtx[cur_time_idx]
        cur_translations = self.other_bending_network.timestep_to_optimizable_total_def[cur_time_idx]
        
        if self.other_bending_network.canon_passive_obj_verts is None:
            init_passive_mesh = self.timestep_to_passive_mesh[0]
            center_passive_mesh = torch.mean(init_passive_mesh, dim=0)
            # center_passive_mesh = torch.zeros((3, )).cuda()
        else:
            init_passive_mesh = self.other_bending_network.canon_passive_obj_verts
            center_passive_mesh = torch.zeros((3, )).cuda()
        pred_passive_mesh = torch.matmul(
            cur_rot_mtx, (init_passive_mesh - center_passive_mesh.unsqueeze(0)).transpose(1, 0)
        ).transpose(1, 0) + center_passive_mesh.unsqueeze(0) + cur_translations.unsqueeze(0)
        gt_passive_mesh = self.timestep_to_passive_mesh[cur_passive_time_idx]
        tracking_loss = torch.sum( # gt mehses # 
            (pred_passive_mesh - gt_passive_mesh) ** 2, dim=-1
        ).mean()
        return tracking_loss

    # tot_forces_l2_losses = comput_loss_optimized_forces_with_cur_forces(self, tot_time_idx)
    def comput_loss_optimized_forces_with_cur_forces(self, tot_time_idx):
        reference_forces = self.renderer.bending_network[1].timestep_to_spring_forces_ori
        target_forces = self.renderer.bending_network[1].timestep_to_spring_forces
        
        tot_forces_l2_losses = []
        for ts in range(tot_time_idx):
            if ts in reference_forces and ts in target_forces:
                cur_reference_force = reference_forces[ts]
                cur_target_force = target_forces[ts]
                diff_reference_force_target_force = torch.mean(
                    torch.sum(
                        (cur_reference_force - cur_target_force) ** 2, dim=-1
                    )
                )
                tot_forces_l2_losses.append(diff_reference_force_target_force)
        tot_forces_l2_losses = torch.stack(tot_forces_l2_losses, dim=0)
        tot_forces_l2_losses = torch.mean(tot_forces_l2_losses)
        return tot_forces_l2_losses

    
    def train_actions_single_step_from_sim_rules(self): # get the bending network #
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        res_step = self.end_iter - self.iter_step
        image_perm = self.get_image_perm() # image permutation #
        
        # self.validate_mesh()
        

        for i in range(1):
            img_idx = image_perm[self.iter_step % len(image_perm)]
            # img_idx = 0
            if not self.use_bending_network:
            
                tot_frame_act_xs = self.get_deformed_act_pts()
                self.tot_frame_act_xs = tot_frame_act_xs
                tot_frame_aggre_deforms = self.get_bkg_pts_deformation(tot_frame_act_xs) # tot_frame_aggre_deforms
                self.tot_frame_aggre_deforms = tot_frame_aggre_deforms
            # tot_frame_aggre_deforms = torch.zeros_like(self.bkg_pts).unsqueeze(0).repeat(self.n_time)
            # rnd_time_idx = torch.randint(low=0, high=self.n_timesteps, size=(1,)).item()
            #### random rays at ####
            # for i_time_idx in range(self.n_timesteps): # n_timesteps # 
            # for i_time_idx in [rnd_time_idx]: # 
            # or it is related to the i_time_idx # # range(1, self.n_timesteps) # 
            timestep_to_ptarget_selected_act_mesh = {}
            for i_time_idx in range(1, self.n_timesteps):
                # if not self.use_bending_network:
                #     # self.tot_frame_aggre_deforms # # curretn bkg pts defs #
                #     self.renderer.cur_fr_bkg_pts_defs = tot_frame_aggre_deforms[i_time_idx] # ## set the bkg_pts_deffs ##
                # #
                # self.renderer.cur_fr_bkg_pts_defs = torch.zeros_like(tot_frame_aggre_deforms[i_time_idx])
                
                cur_timestep_to_active_robot_pts = {}
                for i_ts in range(self.n_timesteps):
                    cur_state = self.state_vals[i_ts] # cur_state #
                    # cur_state = torch.sum(self.state_vals[:i_ts + 1], dim=0)
                    cur_active_robot_pts = []
                    cur_active_robot_pts = self.active_robot.compute_transformation_via_state_vecs(cur_state, cur_active_robot_pts)
                    cur_active_robot_pts = torch.cat(cur_active_robot_pts, dim=0) ### cat pts ###
                    cur_active_robot_pts =  (cur_active_robot_pts - self.minn_pts) / self.extent
                    cur_active_robot_pts = cur_active_robot_pts * 2. - 1. # active robot pts #
                    # ori_cur_active_robot_pts = cur_active_robot_pts.clone() #
                    # print(f"cur_active_robot_pts: {cur_active_robot_pts.size()}") #
                    cur_active_robot_pts = cur_active_robot_pts[self.act_pts_fps_idx]
                    cur_timestep_to_active_robot_pts[i_ts] = cur_active_robot_pts # active robot pts #
                # self.renderer.timestep_to_active_mesh[i_time_idx - 1] = cur_active_robot_pts ### get the robot pts ###
                self.renderer.timestep_to_active_mesh = cur_timestep_to_active_robot_pts
                # data = self.dataset.gen_random_rays_at(img_idx, self.batch_size) # sample rays 
                data = self.time_idx_to_dataset[i_time_idx].gen_random_rays_at(img_idx, self.batch_size) # sample rays #

                ### dataset near far from sphere ###
                rays_o, rays_d, true_rgb, mask = data[:, :3], data[:, 3: 6], data[:, 6: 9], data[:, 9: 10]
                near, far = self.time_idx_to_dataset[i_time_idx].near_far_from_sphere(rays_o, rays_d) # sample rays #

                # print(f"near: {near}, far: {far}, weight: {self.mask_weight}")
                background_rgb = None
                if self.use_white_bkgd:
                    background_rgb = torch.ones([1, 3])

                if self.mask_weight > 0.0:
                    mask = (mask > 0.5).float()
                else:
                    mask = torch.ones_like(mask)

                # mask_sum = mask.sum() + 1e-5 # get mask #
                ## ambient contact forces; amibinet ##

                render_out = self.renderer.render_def(rays_o, rays_d, near, far, pts_ts=i_time_idx, perturb_overwrite=-1, background_rgb=background_rgb, cos_anneal_ratio=self.get_cos_anneal_ratio(), use_gt_sdf=False, update_tot_def=False)
                
                point_loss = self.compute_loss_optimized_offset_with_preopt_offset(tot_time_idx=i_time_idx)
                
                
                # print(f"cur_target_mesh_verts: {cur_target_mesh_verts.size()}")
                cur_target_mesh_verts = self.target_meshes_verts[i_time_idx]
                cur_active_robot_pts = self.renderer.timestep_to_active_mesh[i_time_idx]
                cd_cur_active_to_target = torch.sum(
                    (cur_active_robot_pts.unsqueeze(1) - cur_target_mesh_verts.unsqueeze(0)) ** 2, dim=-1
                )
                cd_cur_active_to_target = torch.sqrt(cd_cur_active_to_target)
                dist_cur_to_tar, _ = torch.min(cd_cur_active_to_target, dim=-1)
                dist_tar_to_cur, _ = torch.min(cd_cur_active_to_target, dim=0)
                cd_cur = 0.5 * (dist_cur_to_tar.mean() + dist_tar_to_cur.mean())
                # cd_cur.backward()
                
                # loss = point_loss * 100 + cd_cur # 
                
                # timestep_to_prev_selected_active_mesh_ori # bending_network[1] # 
                cur_target_prev_selected_act_mesh_ori = self.renderer.bending_network[1].timestep_to_prev_selected_active_mesh_ori[i_time_idx - 1]
                cur_target_prev_selected_act_mesh_ori = cur_target_prev_selected_act_mesh_ori.view(cur_target_prev_selected_act_mesh_ori.size(0) * cur_target_prev_selected_act_mesh_ori.size(1), 3).contiguous()
                # cur_target_prev_selected_act_mesh_ori_fps_idx = np.
                
                # 
                # rng = np.random.default_rng()
                cat_prob = torch.ones_like(cur_target_prev_selected_act_mesh_ori[:, 0])
                cat_prob = cat_prob / torch.sum(cat_prob)
                cat_dist = Categorical(cat_prob) # 
                cur_target_prev_selected_act_mesh_ori_fps_idx = cat_dist.sample(sample_shape=(5000,))
                
                # cur_target_prev_selected_act_mesh_ori_fps_idx = np.random.permutation(cur_target_prev_selected_act_mesh_ori.size(0))[:5000]
                # cur_target_prev_selected_act_mesh_ori_fps_idx = torch.from_numpy(cur_target_prev_selected_act_mesh_ori_fps_idx).long().cuda()
                
                
                # cur_target_prev_selected_act_mesh_ori_fps_idx = data_utils.farthest_point_sampling(cur_target_prev_selected_act_mesh_ori.unsqueeze(0), n_sampling=5000)
                cur_target_prev_selected_act_mesh_ori = cur_target_prev_selected_act_mesh_ori[cur_target_prev_selected_act_mesh_ori_fps_idx]
                
                #### timestep_to_ptarget_selected_act_mesh -> the timestep to the selected active mesh pts ####
                timestep_to_ptarget_selected_act_mesh[i_time_idx - 1] = cur_target_prev_selected_act_mesh_ori.detach().cpu().numpy()
                
                dist_cur_active_to_selected_act = torch.sum(
                    (cur_active_robot_pts.unsqueeze(1) - cur_target_prev_selected_act_mesh_ori.unsqueeze(0)) ** 2, dim=-1
                )
                dist_active_to_selected_act, _ = torch.min(dist_cur_active_to_selected_act, dim=-1) ### nn_act_pts
                dist_selected_act_to_active, _ = torch.min(dist_cur_active_to_selected_act, dim=0)
                #### dist_cur_active_to_selected_act, dist_cur_selected_act_to_active ####
                dist_active_to_selected_act = dist_active_to_selected_act.mean()
                dist_selected_act_to_active = dist_selected_act_to_active.mean()
                
                # loss = cd_cur + dist_cur_active_to_selected_act * 10
                
                loss = (dist_active_to_selected_act + dist_selected_act_to_active) * 10
                
                ### backpropagate the loss ###
                self.optimizer.zero_grad()
                loss.backward()
                # self.optimizer.step()
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                if self.iter_step % self.report_freq == 0:
                    print(self.base_exp_dir)
                    # print('iter:{:8>d} loss = {} sdf_value_loss = {} lr={}'.format(self.iter_step, loss, sdf_value_err.item(), self.optimizer.param_groups[0]['lr']))
                    print('iter:{:8>d} loss = {} point_loss = {} cd_loss = {} cd_act_to_selected_pts = {} cd_selected_pts_to_act = {} lr={}'.format(self.iter_step, loss, point_loss.detach().item(), cd_cur.detach().item(), dist_active_to_selected_act.detach().item(), dist_selected_act_to_active.detach().item(), self.optimizer.param_groups[0]['lr']))

                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint()

                # if self.iter_step % self.val_freq == 0:
                #     print(f"state_vals: {self.state_vals.data}")
                #     self.validate_image(0)
                #     # self.validate_image(0, use_gt_sdf=True)
                #     self.validate_image()
                #     # self.validate_image( use_gt_sdf=True)

                # if self.iter_step % self.val_mesh_freq == 0:
                #     self.validate_mesh()

                self.update_learning_rate()

                if self.iter_step % len(image_perm) == 0:
                    image_perm = self.get_image_perm()
            
                # state_vals_lr = 1.
                # state_vals_lr = 0.1
                # state_vals_lr = 0.01
                if self.state_vals.grad is not None: # 
                    state_vals_grad = self.state_vals.grad.data.detach().cpu().numpy() # 
                    
                    tot_df_dq = []
                    for i_step in range(self.num_steps):
                        if (i_step + 1) % self.nn_mod_steps == 0:
                            cur_df_dq = state_vals_grad[(i_step + 1) // self.nn_mod_steps  - 1, :]
                            cur_df_dq = np.concatenate([cur_df_dq, np.zeros((self.ndof_r - cur_df_dq.shape[0], ))], axis=0)
                        else:
                            cur_df_dq = np.zeros((self.ndof_r,), dtype=np.float32)
                        tot_df_dq.append(cur_df_dq)
                    tot_df_dq = np.concatenate(tot_df_dq, axis=0) ### tot_df_dq ### 
                    # print(f"tot_df_dq: {tot_df_dq.shape}")
                    self.sim.backward_info.df_dq = tot_df_dq # get to q # 
                    self.sim.backward_info.df_du =  np.zeros(self.ndof_u * self.num_steps) 
                    self.sim.backward()
                    grad = np.copy(self.sim.backward_results.df_du)
                    # 
                    u_lr = 0.01
                    # u_lr = 0.001
                    self.u = self.u - u_lr * grad # update the action variables; # #### update the action variables ###
                    
                    self.sim.reset(backward_flag = True)
                    ### get total states ###
                    tot_qs = []
                    for i in range(self.num_steps):
                        cur_u = self.u[i * self.ndof_u: (i + 1) * self.ndof_u]
                        self.sim.set_u(cur_u)
                        self.sim.forward(1, verbose=False)
                        q = self.sim.get_q()
                        if (i + 1) % self.nn_mod_steps == 0:
                            tot_qs.append(q) # nn_mod_steps # 
                    tot_qs = np.stack(tot_qs, axis=0) ### nn_steps x ndof_q ###
                    # self.state_vals = 
                    self.state_vals.data[:, :] = torch.from_numpy(tot_qs).float().cuda()[:, :self.ndof_u] ### state_vals.data = torch_tensor # 
                    # self.sim = sim
                    self.state_vals.grad.data = self.state_vals.grad.data * 0.
                    
                    
                    # self.passive_state_vals.data[:, :] = torch.from_numpy(tot_qs).float().cuda()[:, self.ndof_u:]
                    # # self.passive_state_vals = self.passive_state_vals + self.passive_pos.unsqueeze(0)
                    # self.passive_state_vals.data = self.passive_pos[:].unsqueeze(0).repeat(self.passive_state_vals.data.size(0), 1)
                    
                    for i_ts in range(self.passive_state_vals.data.size(0)):
                        cur_state = torch.from_numpy(tot_qs[i_ts, self.ndof_u: ]).float().cuda()
                        # rot_mtx = plane_rotation_matrix_from_angle_xz(angle=cur_state[2])
                        cur_trans_vec = torch.stack(
                            [cur_state[0], torch.zeros_like(cur_state[0]), cur_state[1]], dim=0
                        )
                        cur_state = torch.matmul(
                            self.axis_rot_mtx.transpose(1, 0), cur_trans_vec.unsqueeze(-1).contiguous()
                        ).squeeze(-1).contiguous() # cur_state # 
                        cur_state = cur_state / self.extent
                        cur_state = cur_state * 2.
                        self.passive_state_vals.data[i_ts, :] = cur_state[:]
                    
                    # self.passive_state_vals = (self.passive_state_vals - self.minn_pts) / self.extent
                    # self.passive_state_vals = self.passive_state_vals * 2. - 1.
                    for i_ts in range(self.passive_state_vals.size(0)):
                        cur_passive_def = self.passive_state_vals.data[i_ts]
                        self.renderer.bending_network[1].timestep_to_total_def[i_ts] = cur_passive_def
                    # df_dq[-ndof_r: ] = 2 * (q - init_q_goal)
                    # df_dq[-ndof_r: -(ndof_r - ndof_u)] = 2 * (q - init_q_goal)[: -(ndof_r - ndof_u)]
                    # ## dense states supervision #### ## supervision
                    # tot_qs = np.concatenate(tot_qs, axis=0) # axis
                    # df_dq = 2 * tot_qs
                    # f = np.sum(tot_qs ** 2)
                    
                
                self.active_robot.clear_grads()
            
            self.timestep_to_ptarget_selected_act_mesh = timestep_to_ptarget_selected_act_mesh
            
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache() # # # empty_cache() ### cache_
    
    # train 
    # train from model rules -> use the model rules to optimize the underlying dynamical model #
    def train_from_model_rules(self, ): # only the active mdoel optimization #
        # get the mehs seq #
        # /data2/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime/wmask_reverse_value_totviews_tag_act_xs_def_n5_bending_net/meshes/00028500_tets_ts_1.ply
        # sv_mesh_root_fn = self.conf['model.sv_mesh_root']
        # target_meshes_verts = []
        # for i_ts in range(self.n_timesteps):
        #     # if i_ts == 0:
        #     #     cur_ts_mesh_fn = os.path.join(sv_mesh_root_fn, f"00028500_tets.ply")
        #     # else:
        #     cur_ts_mesh_fn = os.path.join(sv_mesh_root_fn, f"00258500_tets_ts_{i_ts}.ply")
        #     obj_mesh = trimesh.load(cur_ts_mesh_fn, process=False)
        #     cur_mesh_verts = np.array(obj_mesh.vertices)
        #     target_meshes_verts.append(torch.from_numpy(cur_mesh_verts).float().cuda())
        # # target_meshes_verts = torch.stack(target_meshes_verts, dim=0)
        # tot_optimized_pts = []
        
        # get the mehs seq #
        # /data/xueyi/NeuS/exp/hand_test_routine_2_light_color_wtime/wmask_reverse_value_totviews_tag_act_xs_def_n5_bending_net_val_more_ts_7_delta_bending_sv_resume_v2_split_bending_/meshes/00300000_tets_ts_6.ply
        sv_mesh_root_fn = self.conf['model.sv_mesh_root']
        target_meshes_verts = []
        for i_ts in range(self.n_timesteps):
            # if i_ts == 0:
            #     cur_ts_mesh_fn = os.path.join(sv_mesh_root_fn, f"00028500_tets.ply")
            # else:
            cur_ts_mesh_fn = os.path.join(sv_mesh_root_fn, f"00300000_tets_ts_{i_ts}.ply")
            obj_mesh = trimesh.load(cur_ts_mesh_fn, process=False)
            cur_mesh_verts = np.array(obj_mesh.vertices)
            target_meshes_verts.append(torch.from_numpy(cur_mesh_verts).float().cuda())
        # target_meshes_verts = torch.stack(target_meshes_verts, dim=0) ##
        self.target_meshes_verts = target_meshes_verts # get the target meshes verts ##
        
        
        for i_iter in tqdm(range(100000)):
            
            self.train_def_single_step()
            
            if self.iter_step % self.report_freq == 0:
                print("state values")
                print(self.state_vals)
                
            self.active_robot.clear_grads()
            
            cur_timestep_to_active_robot_pts_np = {
                cur_ts: self.renderer.timestep_to_active_mesh[cur_ts].detach().cpu().numpy() for cur_ts in self.renderer.timestep_to_active_mesh
            }
            # 
            # optimized_offset = self.renderer.bending_network[1].timestep_to_optimizable_offset
            optimized_offset = self.renderer.bending_network[1].timestep_to_total_def
            # from the timestep_to_optimizable_offset to others # 
            cur_timestep_to_passive_robot_pts = {}
            cur_timestep_to_passive_robot_pts[0] = self.renderer.timestep_to_passive_mesh[0].detach().cpu().numpy() ## passive mesh at the zero timestep ##
            for cur_ts in optimized_offset: # get the offset for each offset # 
                cur_ts_offset = optimized_offset[cur_ts].detach().cpu().numpy() ##
                cur_offset_passive_mesh_pts = self.renderer.timestep_to_passive_mesh[0].detach().cpu().numpy() + np.reshape(cur_ts_offset, (1, 3))
                cur_timestep_to_passive_robot_pts[cur_ts] = cur_offset_passive_mesh_pts
            
            target_offset = self.renderer.bending_network[1].timestep_to_total_def_copy
            cur_timestep_to_passive_robot_pts_target = {}
            cur_timestep_to_passive_robot_pts_target[0] = self.renderer.timestep_to_passive_mesh[0].detach().cpu().numpy() ## passi
            for cur_ts in target_offset:
                cur_ts_offset = target_offset[cur_ts].detach().cpu().numpy() ##
                cur_offset_passive_mesh_pts = self.renderer.timestep_to_passive_mesh[0].detach().cpu().numpy() + np.reshape(cur_ts_offset, (1, 3))
                cur_timestep_to_passive_robot_pts_target[cur_ts] = cur_offset_passive_mesh_pts
            
            
            
            
            cur_timestep_to_active_passive_robot_pts = {
                'active': cur_timestep_to_active_robot_pts_np, 
                'passive': cur_timestep_to_passive_robot_pts,
                'passive_target': cur_timestep_to_passive_robot_pts_target,
                #### prevtarget selected act mesh ####
                'timestep_to_ptarget_selected_act_mesh': self.timestep_to_ptarget_selected_act_mesh
            } #
            
            if i_iter % 100 == 0:
                # self.validate_mesh()
                os.makedirs(os.path.join(self.base_exp_dir, 'act_pts'), exist_ok=True)
                act_robot_pts_np_sv_fn = os.path.join(
                    self.base_exp_dir, 'act_pts', '{:0>8d}_act_pts.npy'.format(self.iter_step) ### 
                )
                # np.save(act_robot_pts_np_sv_fn, cur_timestep_to_active_robot_pts_np) #### active robot pts numpy ###
                np.save(act_robot_pts_np_sv_fn, cur_timestep_to_active_passive_robot_pts) # act_passive_robot_pts #
                print(f"optimized pts saved to {act_robot_pts_np_sv_fn}")
            
        pass
    
    
    # self.free_bending_network = construct_field_network(self, input_dim, hidden_dimensions,  output_dim):
    def construct_field_network(self, input_dim, hidden_dimensions,  output_dim):
        cur_field_network = nn.Sequential(
            *[
                nn.Linear(input_dim, hidden_dimensions), nn.ReLU(),
                nn.Linear(hidden_dimensions, hidden_dimensions * 2), # with maxpoll layers # 
                nn.Linear(hidden_dimensions * 2, hidden_dimensions), nn.ReLU(), # 
                nn.Linear(hidden_dimensions, output_dim), # hidden
            ]
        )
        # nn.ModuleList(
        #     [
        #         nn.Linear(input_dim, hidden_dimensions), nn.ReLU()),
        #         nn.Sequential(nn.Linear(hidden_dimensions, hidden_dimensions)), # with maxpoll layers # 
        #         nn.Sequential(nn.Linear(hidden_dimensions * 2, hidden_dimensions), nn.ReLU()), # 
        #         nn.Sequential(nn.Linear(hidden_dimensions, output_dim, )), # hidden
        #     ]
        # )
        
        with torch.no_grad():
            for i, cc in enumerate(cur_field_network[:]):
                # for cc in layer:
                if isinstance(cc, nn.Linear):
                    torch.nn.init.kaiming_uniform_(
                        cc.weight, a=0, mode="fan_in", nonlinearity="relu"
                    )
                    if i < len(cur_field_network) - 1:
                        torch.nn.init.zeros_(cc.bias)
            torch.nn.init.zeros_(cur_field_network[-1].weight) 
            torch.nn.init.zeros_(cur_field_network[-1].bias) # initialize the field network for bendign and deofrmation
            
        return cur_field_network
    
    
    ### mano model rules ###
    def train_robot_actions_from_mano_model_rules(self, ): # only the active mdoel optimization #
        ### 
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ## closing the gap between two sims ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps # nn_timesteps # # 
        
        
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path']
        robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent # robot agent #
        # morph robot agent # # morph from the mano hand model to the robot hand model #
        robo_init_verts = self.robot_agent.robot_pts # the robot initial points # 
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy" #the sampled points from the robots # 
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        
        self.robo_hand_faces = self.robot_agent.robot_faces
        ''' Load the robot hand '''
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano)
        self.mano_agent = mano_agent # mano agent #
        ''' Load the mano hand '''
        # ### # load the mano model ## # 
        
        
        print(f"Start expanding the current visual pts...")
        expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        
        self.expanded_visual_pts_nn = expanded_visual_pts.size(0)



        ## expanded_visual_pts of the expanded visual pts #
        expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy) # 
        
        # expaned visual pts 
        ## train robot ## 
        # for each mano action # 
        # should laod the pre-optimized mano action model file #
        # for each mano action, articulate the mano model and get the transformed visual pts # 
        # add an additional visual pts free motion field #
        # use the weighted motion to determine the real motion # 
        # sue the visual pts as the real manipulating pts which will further influence the pasiveobject states # 
        # use the visual pts to determine the passive object states ## object states ## 
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0. # 
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        # expanded_visual_pts_nn
        
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=778 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        self.expanded_actuator_friction_forces = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.expanded_actuator_friction_forces.weight)
        params_to_train += list(self.expanded_actuator_friction_forces.parameters())
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        # params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        ## with the timestep ##  
        ## with the timestep ##
        # with the timestep #
        # self.
        # time latent vectors #
        # deformation field network #
        # from the deformation field and the time  latents to get the free motion field #
        # get the time latent # 
        # get the deformaton result #
        # free_deformation_time_latent, free_deformation_network
        self.free_deformation_time_latent = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        )
        params_to_train += list(self.free_deformation_time_latent.parameters())
        
        # free deformations; time latnets #
        self.deformation_input_dim = 3 + self.bending_latent_size
        self.deformation_output_dim = 3
        ### free deformation network ### # free deformation # 
        self.free_deformation_network = self.construct_field_network(self.deformation_input_dim, self.bending_latent_size,  self.deformation_output_dim)
        params_to_train += list(self.free_deformation_network.parameters())
        
        
        # 40 # 
        # nn_expanded_pts = 40
        # expanded_pts #
        
        
        # ### laod optimized init actions #### # still articulated by the articulation network # ##the articulated states # freee df and the 
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # cur_optimized_init_actions = 
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            self.robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
            if 'expanded_actuator_friction_forces' in optimized_init_actions_ckpt:
                try:
                    self.expanded_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_friction_forces'])
                except: # load friction force # 
                    pass
            ### load init transformations ckpts ###




        # ### robot agent ###
        # params_to_train = []
        # ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        # self.robot_actions = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=22,
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actions.weight)
        # if not self.optimize_glb_transformations:
        #     params_to_train += list(self.robot_actions.parameters())
        
        # self.robot_init_states = nn.Embedding(
        #     num_embeddings=1, embedding_dim=22,
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_init_states.weight)
        # if not self.optimize_glb_transformations: # not to optimize the glb transformations #        
        #     params_to_train += list(self.robot_init_states.parameters())
        
        # self.robot_glb_rotation = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=4
        # ).cuda()
        # self.robot_glb_rotation.weight.data[:, 0] = 1.
        # self.robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        # self.robot_delta_states = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=60,
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        
        # self.robot_glb_rotation = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=6
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_glb_rotation.weight)
        # self.robot_glb_rotation[:, 0] = 1.
        # self.robot_glb_rotation[:, 4] = 1.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=365428 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        # params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        # self.robot_glb_trans = nn.Embedding( # 
        #     num_embeddings=num_steps, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())
        
        
        # # sketche out the actions #
        # ### laod optimized init actions ####
        # if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
        #     cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
        #     # cur_optimized_init_actions = 
        #     optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
        #     self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
        #     self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
        #     if 'robot_delta_states' in optimized_init_actions_ckpt:
        #         self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
        #     self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions']) # 
        #     self.robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
        #     self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
        #     ### load init transformations ckpts ###

        ## self.maxx_robo_pts = 25. ##
        # self.maxx_robo_pts = 25. ## 
        # self.minn_robo_pts = -15. ## 
        # self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        # self.mult_const_after_cent = 0.5437551664260203
        self.mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        # self.mano_fingers = [745, 279, 320, 444, 555, 672, 234, 121]
        # self.robot_fingers = [521, 624, 846, 973, 606, 459, 383, 265]
        
        # self.mano_fingers = torch.tensor(self.mano_fingers, dtype=torch.long).cuda()
        # self.robot_fingers = torch.tensor(self.robot_fingers, dtype=torch.long).cuda()
        
        # 
        init_trans = self.timestep_to_active_mesh[0][:778].mean(dim=0)
        self.robot_glb_trans.weight.data[0, :] = init_trans[:]
        
        # self.faces = 
        
        self.nn_ts = 1
        self.nn_ts = self.nn_timesteps - 1 # 
        # self.optimize_with_intermediates = False
        
        # self.optimize_with_intermediates # propagate -> #
        # 
        # search for the nearest point so that it can manipulate the object #
        ## TODO: co-optimize the bending network weights? ## # optimize active object is true ##
        if self.optimize_with_intermediates and (not self.optimize_active_object): # optimize with intermediates #  # search
            # if not self.optimize_active_object:
            # free_deformation_time_latent, free_deformation_network
            params_to_train = []
            params_to_train += list(self.other_bending_network.parameters())
            # params_to_train += list(self.robot_actuator_friction_forces.parameters())
            params_to_train += list(self.expanded_actuator_friction_forces.parameters()) # expanded actuator 
            params_to_train += list(self.free_deformation_time_latent.parameters())
            params_to_train += list(self.free_deformation_network.parameters())

        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        # init_rot = R.random().as_quat()
        # init_rot = torch.from_numpy(init_rot).float().cuda()
        # self.robot_glb_rotation.weight.data[0, :] = init_rot[:]
        
        
        # also optimize them only to get the expanded vvsisu # get the expadned visual pts expande the visual pts and g
        # 
        
        # self #
        #### TODO: rotation optimization and the translation optimization ####
        for i_iter in tqdm(range(100000)): ## actions from model rules ##
            tot_losses = []
            tot_penalty_dot_forces_normals = []
            tot_penalty_friction_constraint = []
            tot_dist_act_to_rhand = []
            tot_dist_to_contact = []
            ## tot_diff_grid_pts_forces, tot_diff_grid_pts_weight ## # grid_pts_weight #
            tot_diff_grid_pts_forces = []
            tot_diff_grid_pts_weight = []
            # tot_loss_figner_tracking = []
            tot_loss_corr_tracking = []
            tot_tracking_loss = []
            
            self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            # self.mano_agent #
            # for cur_ts in range(nn_timesteps - 1): # # nn_timesteps - 1 #
            # for cur_ts in range(nn_timesteps - 1): # # nn_timesteps - 1 #
            # for cur_ts in range(nn_timesteps - 1): # # cur_ts and the nn_timesteps - 1 ##
            for cur_ts in range(self.nn_ts):
                actions = {}
                
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot)
                
                # cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                # cur_glb_rot_a, cur_glb_rot_b = cur_glb_rot[:3], cur_glb_rot[3:]
                # cur_glb_rot_a = cur_glb_rot_a / torch.clamp(torch.norm(cur_glb_rot_a, dim=0, p=2), min=1e-7)
                # cur_glb_rot_b = cur_glb_rot_b / torch.clamp(torch.norm(cur_glb_rot_b, dim=0, p=2), min=1e-7)
                # cur_glb_rot_c = torch.cross(cur_glb_rot_a, cur_glb_rot_b) ### 3-dim rotation vector # 
                # cur_glb_rot = torch.stack(
                #     [cur_glb_rot_a, cur_glb_rot_b, cur_glb_rot_c], dim=-1 ### rotation vector ###
                # )
                
                
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                # roobt glb trans # # robot glb trans # # robot glb trans # robot glb trans #
                # split the optimization for the global transformations and the articulated actions #
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.mano_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.mano_agent.get_init_state_visual_pts()
                else:
                    if self.conf['model.train_states']:
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        self.mano_agent.active_robot.set_delta_state_and_update(cur_state, cur_ts)
                        cur_visual_pts = self.mano_agent.get_init_state_visual_pts()
                    else:
                        # actions['delta_glb_rot'] = cur_glb_rot #
                        # actions['delta_glb_trans'] = cur_glb_trans #
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        
                        actions['link_actions'] = actions_link_actions
                        self.mano_agent.active_robot.calculate_inertia()
                        self.mano_agent.active_robot.set_actions_and_update_states(actions_link_actions, cur_ts - 1, 0.2)
                        cur_visual_pts = self.mano_agent.get_init_state_visual_pts()

                # cur_visual_pts # # #
                cur_visual_pts = self.mano_agent.active_robot.get_transformed_visual_pts() # transformed visual pts # 
                
                ### transform the visual pts ###
                # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                # cur_visual_pts = cur_visual_pts * 2. -1.
                # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult const #
                
                # cur_visual_pts_mano_pts = cur_visual_pts[self.mano_to_dyn_corr_pts_idxes]
                
                if cur_ts > 0:
                    prev_rot = timestep_to_tot_rot[cur_ts - 1]
                    cur_rot = torch.matmul(cur_glb_rot, prev_rot)
                    prev_trans = timestep_to_tot_trans[cur_ts - 1]
                    cur_trans = cur_glb_trans + prev_trans
                else:
                    cur_rot = cur_glb_rot
                    cur_trans = cur_glb_trans
                
                if not self.conf['model.using_delta_glb_trans']:
                    cur_rot = cur_glb_rot
                    cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult const #
                
                
                # free_deformation_bending_weigh
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5 # optimize the actions to use the free deformation bending weight #
                ### rigidly deformed points ###
                # print(f"cur_visual_pts: {cur_visual_pts.size()}, cur_glb_rot: {cur_glb_rot.size()}")
                ### transform by the glboal transformation and the translation ###
                
                
                # cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0)
                
                if cur_ts > 0:
                    # need to combine the rigid deformation and the free deformation #
                    # rigid_deformation = cur_visual_pts - self.timestep_to_active_mesh[cur_ts - 1].detach()
                    rigid_deformation = cur_visual_pts - self.timestep_to_posed_active_mesh[cur_ts - 1].detach()
                    
                    cur_visual_pts = self.timestep_to_posed_active_mesh[cur_ts - 1].detach() + rigid_deformation * (1. - self.free_def_bending_weight)
                    
                    
                    input_ts_latents = self.free_deformation_time_latent(torch.zeros((rigid_deformation.size(0),), dtype=torch.long).cuda() + cur_ts)
                    
                    # print(f"input_ts_latents: {input_ts_latents.size()}, timestep_to_active_mesh: {self.timestep_to_active_mesh[cur_ts - 1].size()}")
                    input_free_def_latents = torch.cat(
                        [input_ts_latents, self.timestep_to_posed_active_mesh[cur_ts - 1].detach()], dim=-1
                    )
                    
                    free_deformation = self.free_deformation_network(input_free_def_latents) 
                    
                    cur_visual_pts = cur_visual_pts + free_deformation * self.free_def_bending_weight
                    
                    self.timestep_to_posed_active_mesh[cur_ts] = cur_visual_pts.detach()
                    
                    
                    cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0)
                    
                    
                    self.timestep_to_active_mesh[cur_ts] = cur_visual_pts.detach()
                    # input_free_def_latents = torch.cat(
                    #     [input_ts_latents, self.timestep_to_active_mesh[cur_ts - 1].detach()], dim=-1
                    # )
                    # # input_free_def_latents #
                    # free_deformation = self.free_deformation_network(input_free_def_latents) # input_free_def_latnets #
                    # tot_def = rigid_deformation * (1. - self.free_def_bending_weight) + self.free_def_bending_weight * free_deformation
                    # cur_visual_pts = self.timestep_to_active_mesh[cur_ts - 1].detach() + tot_def # 
                else:
                    self.timestep_to_posed_active_mesh[cur_ts] = cur_visual_pts.detach()
                    cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0)
                    self.timestep_to_active_mesh[cur_ts] = cur_visual_pts.detach()
                    
                
                # and also the free motion network #
                ##### transform to the active mesh visual pts #####
                # the cur_ts + 1 state in the robot agent is corresponding to the cur_ts state in the simulation with manipulation #
                # cur_visual_pts_mano_pts = cur_visual_pts[self.mano_to_dyn_corr_pts_idxes]
                
                # self.timestep_to_active_mesh[cur_ts] = cur_visual_pts #
                
                
                if self.optimize_with_intermediates:
                    # timestep_to_passive_mesh_normals; if self.bending_net_type == 'active_force_field_v11'
                    if self.bending_net_type in[ 'active_force_field_v11', 'active_force_field_v12', 'active_force_field_v13', 'active_force_field_v14']:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.expanded_actuator_friction_forces)
                    elif self.bending_net_type in ['active_force_field_v13_lageu']:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.expanded_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes)
                    else:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, friction_forces=self.expanded_actuator_friction_forces)
                # 
                cur_rhand_verts = self.rhand_verts[cur_ts]
                dist_active_pts_rhand_verts = torch.sum( # use the visual and the rhand verts ##
                    (cur_visual_pts.unsqueeze(1) - cur_rhand_verts.unsqueeze(0)) ** 2, dim=-1 ### nn_sampled_pts x nn_ref_pts ###
                ) # each oen should amtch one single point
                dist_rhand_verts_to_active_pts, _ = torch.min(dist_active_pts_rhand_verts, dim=0)
                dist_rhand_verts_to_active_pts = torch.mean(dist_rhand_verts_to_active_pts)
                dist_active_pts_rhand_verts, _ = torch.min(dist_active_pts_rhand_verts, dim=-1)
                dist_active_pts_rhand_verts = torch.mean(dist_active_pts_rhand_verts)
                
                # dist active pts rhand verts #
                dist_active_pts_rhand_verts = (dist_active_pts_rhand_verts + dist_rhand_verts_to_active_pts) / 2.
                
                
                if self.optimize_with_intermediates:
                    tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1)
                    # ### tracking loss with the cd loss -> to the reference motion ###
                    # loss = tracking_loss + dist_active_pts_rhand_verts
                    # ### cd loss ###
                    # loss = dist_active_pts_rhand_verts
                    # ### tracking loss with the cd loss -> to the reference motion ###
                    
                    # cur_ts_grid_forces = self.other_bending_network.timestep_to_grid_pts_forces[cur_ts]
                    # cur_ts_ref_grid_forces = self.ref_grid_forces[cur_ts]
                    # diff_cur_ref_contact_d_grid_forces = torch.sum(
                    #     (cur_ts_grid_forces[:, :3] - cur_ts_ref_grid_forces[:, :3]) ** 2, dim=-1 ## get the differences 
                    # )
                    # diff_contact_d = torch.sum(diff_cur_ref_contact_d_grid_forces)
                    # diff_contact_d = diff_contact_d * 1e4

                    # cur_ts_grid_weight ## cur_ts_grid_weight ####
                    # cur_ts_grid_weight = self.other_bending_network.timestep_to_grid_pts_weight[cur_ts]
                    # cur_ts_ref_grid_weight = self.ref_grid_weight[cur_ts]
                    # diff_grid_pts_weight = torch.sum(
                    #     (cur_ts_grid_weight - cur_ts_ref_grid_weight) ** 2, # dim=-1 ## get the differences
                    # )
                    # diff_grid_pts_weight = diff_grid_pts_weight * 1e4 #
                    
                    diff_contact_d = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    diff_grid_pts_weight = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    # tot_diff_grid_pts_forces, tot_diff_grid_pts_weight # 
                else:
                    tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    diff_contact_d = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    diff_grid_pts_weight = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                tot_diff_grid_pts_forces.append(diff_contact_d.detach().item())
                tot_diff_grid_pts_weight.append(diff_grid_pts_weight.detach().item())
                
                loss = diff_contact_d + diff_grid_pts_weight
                
                # self.ts_to_contact_pts[ts]
                if cur_ts in self.ts_to_contact_pts:
                    cur_contact_pts = self.ts_to_contact_pts[cur_ts] ### nn_contact_pts x 3 
                    dist_cur_active_pts_to_contact_pts = torch.sum(
                        (cur_visual_pts.unsqueeze(1) - cur_contact_pts.unsqueeze(0)) ** 2, dim=-1 
                    )
                    dist_to_contact, _ = torch.min(dist_cur_active_pts_to_contact_pts, dim=0)
                    dist_to_contact = torch.mean(dist_to_contact)
                    # loss = loss + dist_to_contact
                else:
                    dist_to_contact = torch.zeros((1,), dtype=torch.float32).cuda().mean()    
                    
                
                tot_dist_to_contact.append(dist_to_contact.item()) ### tot_dist_to_contact ## 
                
                tot_dist_act_to_rhand.append(dist_active_pts_rhand_verts.item())
                
                # # add finger tracking loss ##
                # cur_robot_finger_pts = cur_visual_pts[sampled_verts_idxes][self.robot_fingers[1:], :]
                # cur_mano_finger_pts = cur_rhand_verts[self.mano_fingers[1:], :]
                # dist_robot_fingers_to_mano_fingers = torch.sum(
                #     (cur_robot_finger_pts - cur_mano_finger_pts) ** 2, dim=-1
                # )
                # loss_finger_tracking = torch.mean(dist_robot_fingers_to_mano_fingers)
                
                # loss_corr_tracking = torch.sum(
                #     (cur_visual_pts_mano_pts - cur_rhand_verts) ** 2, dim=-1
                # )
                # loss_corr_tracking = loss_corr_tracking.mean()
                
                # tot_loss_corr_tracking.append(loss_corr_tracking)
                
                # tot_loss_figner_tracking.append(loss_finger_tracking.detach().item())
                
                loss = loss # + loss_corr_tracking
                
                # loss = loss_corr_tracking
                # if self.optimize_with_intermediates:
                #     loss = loss_corr_tracking  + tracking_loss * 0.5
                # loss_corr_tracking  # 
                
                loss = tracking_loss # 
                # constraints # 
                # constraints forces #
                # triangle constraints #
                
                if self.optimize_with_intermediates: # diffuse # diffuse # diffuse # how # with the correspondences and we change it to the grid pts forces #
                    # diffuse # 
                    # how to diffuse # four fingers # consolidaconsolidate via the distance via distances #
                    # from A to B #
                    if self.bending_net_type == 'active_force_field_v11':
                        cur_penalty_dot_forces_normals = self.other_bending_network.penalty_dot_forces_normals
                        cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                    elif self.bending_net_type == 'active_force_field_v13':
                        if self.no_friction_constraint:
                            cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                        else:
                            cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                    else:
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                        cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                else:
                    cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean().cuda()
                    cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean().cuda() ##
                
                loss = loss + cur_penalty_dot_forces_normals + cur_penalty_friction_constraint
                
                self.optimizer.zero_grad()
                # loss.backward()
                loss.backward(retain_graph=True)
                self.optimizer.step()
                
                tot_losses.append(loss.detach().item())
                tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                # if self.iter_step % self.report_freq == 0: #
                #     print(self.base_exp_dir)
                #     # print('iter:{:8>d} loss = {} sdf_value_loss = {} lr={}'.format(self.iter_step, loss, sdf_value_err.item(), self.optimizer.param_groups[0]['lr']))
                #     print('iter:{:8>d} loss = {} lr={}'.format(self.iter_step, loss, self.optimizer.param_groups[0]['lr']))
                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                # if self.iter_step % self.val_freq == 0:
                #     print(f"state_vals: {self.state_vals.data}")
                #     self.validate_image(0)
                #     # self.validate_image(0, use_gt_sdf=True) # 
                #     self.validate_image() # and the weights #
                #     # self.validate_image( use_gt_sdf=True)

                self.update_learning_rate() ## update learning rate ##
                
                
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            tot_penalty_dot_forces_normals = sum(tot_penalty_dot_forces_normals) / float(len(tot_penalty_dot_forces_normals))
            tot_penalty_friction_constraint = sum(tot_penalty_friction_constraint) / float(len(tot_penalty_friction_constraint))
            tot_dist_act_to_rhand = sum(tot_dist_act_to_rhand) / float(len(tot_dist_act_to_rhand))
            tot_dist_to_contact = sum(tot_dist_to_contact) / float(len(tot_dist_to_contact))
            tot_diff_grid_pts_forces = sum(tot_diff_grid_pts_forces) / float(len(tot_diff_grid_pts_forces))
            tot_diff_grid_pts_weight = sum(tot_diff_grid_pts_weight) / float(len(tot_diff_grid_pts_weight))
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            # tot_loss_figner_tracking = sum(tot_loss_figner_tracking) / float(len(tot_loss_figner_tracking))
            # tot_loss_corr_tracking = sum(tot_loss_corr_tracking) / float(len(tot_loss_corr_tracking))
            tot_loss_corr_tracking = 0.
            if i_iter % self.report_freq == 0:
                # print(self.base_exp_dir) ## get the grid pts forces and ###
                print('iter:{:8>d} loss = {} loss_corr_tracking = {} tracking_loss = {} diff_grid_pts_forces = {} diff_grid_pts_weight = {} act_to_rhand = {} dist_to_contact = {} penalty_dir = {} penalty_friction = {} lr={}'.format(self.iter_step, tot_losses, tot_loss_corr_tracking, tot_tracking_loss, tot_diff_grid_pts_forces, tot_diff_grid_pts_weight, tot_dist_act_to_rhand, tot_dist_to_contact, tot_penalty_dot_forces_normals, tot_penalty_friction_constraint, self.optimizer.param_groups[0]['lr']))

            if i_iter % self.val_mesh_freq == 0:
                # self.validate_mesh()
                # self.validate_mesh_mano()
                self.validate_mesh_expanded_pts()
            # self.active_robot.clear_grads()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
    
    # 
    def train_real_robot_actions_from_mano_model_rules(self, ):
        
        ### the real robot actions from mano model rules ###
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path']
        robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent # robot agent #
        # morph robot agent # # morph from the mano hand model to the robot hand model #
        robo_init_verts = self.robot_agent.robot_pts # the robot initial points # 
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy()) ## 
        
        self.robo_hand_faces = self.robot_agent.robot_faces
        ''' Load the robot hand '''
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano)
        self.mano_agent = mano_agent
        ''' Load the mano hand '''
        # ### # load the mano model ## # 
        
        
        print(f"Start expanding the current visual pts...")
        expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        
        self.expanded_visual_pts_nn = expanded_visual_pts.size(0)

        ## expanded_visual_pts of the expanded visual pts #
        expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        
        # expaned visual pts 
        ## train robot ## 
        # for each mano action # 
        # should laod the pre-optimized mano action model file #
        # for each mano action, articulate the mano model and get the transformed visual pts # 
        # add an additional visual pts free motion field #
        # use the weighted motion to determine the real motion # 
        # sue the visual pts as the real manipulating pts which will further influence the pasiveobject states # 
        # use the visual pts to determine the passive object states ## object states ## 
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0. # 
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        # expanded_visual_pts_nn
        
        self.mano_robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=778 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_robot_actuator_friction_forces.parameters())

        
        
        self.mano_expanded_actuator_friction_forces = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        # params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        ## with the timestep ##  
        # free_deformation_time_latent, free_deformation_network
        self.free_deformation_time_latent = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        )
        params_to_train += list(self.free_deformation_time_latent.parameters())
        
        # free deformations; time latnets #
        self.deformation_input_dim = 3 + self.bending_latent_size
        self.deformation_output_dim = 3
        ### free deformation network ### # free deformation # 
        self.free_deformation_network = self.construct_field_network(self.deformation_input_dim, self.bending_latent_size,  self.deformation_output_dim)
        params_to_train += list(self.free_deformation_network.parameters())
        
        
        
        # ### laod optimized init actions ####
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # cur_optimized_init_actions = 
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            if 'robot_init_states' in optimized_init_actions_ckpt:
                self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            if 'robot_glb_rotation' in optimized_init_actions_ckpt:
                self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                self.mano_robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            if 'robot_actions' in optimized_init_actions_ckpt:
                self.mano_robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
            if 'expanded_actuator_friction_forces' in optimized_init_actions_ckpt:
                try:
                    self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_friction_forces'])
                except: # load friction force # 
                    pass
            ### load init transformations ckpts ###
        
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        params_to_train = []
        params_to_train += list(self.robot_actions.parameters())
        params_to_train += list(self.robot_delta_states.parameters())
        params_to_train += list(self.robot_init_states.parameters())
        params_to_train += list(self.robot_glb_rotation.parameters())
        params_to_train += list(self.robot_glb_trans.parameters())
        
        params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        # robot actuator friction forces #
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        # self.robot_glb_trans = nn.Embedding( # 
        #     num_embeddings=num_steps, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())
        
        
        # # sketche out the actions #
        # ### laod optimized init actions ####
        # if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
        #     cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
        #     # cur_optimized_init_actions = 
        #     optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
        #     self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
        #     self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
        #     if 'robot_delta_states' in optimized_init_actions_ckpt:
        #         self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
        #     self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions']) # 
        #     self.robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
        #     self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
        #     ### load init transformations ckpts ###

        ## self.maxx_robo_pts = 25. ##
        # self.maxx_robo_pts = 25. ## 
        # self.minn_robo_pts = -15. ## 
        # self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        # self.mult_const_after_cent = 0.5437551664260203
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_transformations_fn = self.conf['model.load_optimized_init_transformations']
            ##### Load optimized initial transformations #####
            optimized_init_transformations_ckpt = torch.load(cur_optimized_init_transformations_fn, map_location=self.device, )
            self.robot_init_states.load_state_dict(optimized_init_transformations_ckpt['robot_init_states'])
            self.robot_glb_rotation.load_state_dict(optimized_init_transformations_ckpt['robot_glb_rotation'])
            # if 'robot_delta_states' in optimized_init_transformations_ckpt:
            #     self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            # self.robot_actions.load_state_dict(optimized_init_transformations_ckpt['robot_actions'])
            # self.robot_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['robot_actuator_friction_forces'])
            self.robot_glb_trans.load_state_dict(optimized_init_transformations_ckpt['robot_glb_trans'])
            # if 'expanded_actuator_friction_forces' in optimized_init_transformations_ckpt:
            #     try:
            #         self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['expanded_actuator_friction_forces'])
            #     except: # load friction force # 
            #         pass
        
        
        
        # self.mano_fingers = [745, 279, 320, 444, 555, 672, 234, 121]
        # self.robot_fingers = [521, 624, 846, 973, 606, 459, 383, 265]
        
        # self.mano_fingers = torch.tensor(self.mano_fingers, dtype=torch.long).cuda()
        # self.robot_fingers = torch.tensor(self.robot_fingers, dtype=torch.long).cuda()
        
        # 
        # init_trans = self.timestep_to_active_mesh[0][:778].mean(dim=0)
        # self.robot_glb_trans.weight.data[0, :] = init_trans[:]
        
        # self.faces = 
        
        # self.nn_ts = 1
        self.nn_ts = self.nn_timesteps - 1 # 
        # self.optimize_with_intermediates = False
        
        # self.optimize_with_intermediates # propagate -> #
        # 
        # search for the nearest point so that it can manipulate the object #
        ## TODO: co-optimize the bending network weights? ## # optimize active object is true ##
        # if self.optimize_with_intermediates and (not self.optimize_active_object): # optimize with intermediates #  # search
        #     # if not self.optimize_active_object:
        #     # free_deformation_time_latent, free_deformation_network
        #     params_to_train = []
        #     params_to_train += list(self.other_bending_network.parameters())
        #     # params_to_train += list(self.robot_actuator_friction_forces.parameters())
        #     params_to_train += list(self.expanded_actuator_friction_forces.parameters()) # expanded actuator 
        #     params_to_train += list(self.free_deformation_time_latent.parameters())
        #     params_to_train += list(self.free_deformation_network.parameters())

        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        # init_rot = R.random().as_quat()
        # init_rot = torch.from_numpy(init_rot).float().cuda()
        # self.robot_glb_rotation.weight.data[0, :] = init_rot[:]
        
        # self #
        #### TODO: rotation optimization and the translation optimization ####
        for i_iter in tqdm(range(100000)): ## actions from model rules ##
            tot_losses = []
            tot_penalty_dot_forces_normals = []
            tot_penalty_friction_constraint = []
            tot_dist_act_to_rhand = []
            tot_dist_to_contact = []
            ## tot_diff_grid_pts_forces, tot_diff_grid_pts_weight ## # grid_pts_weight #
            tot_diff_grid_pts_forces = []
            tot_diff_grid_pts_weight = []
            tot_loss_figner_tracking = []
            tot_loss_corr_tracking = []
            tot_tracking_loss = []
            
            self.timestep_to_active_mesh = {} # timestep to active mesh #
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            correspondence_pts_idxes = None
            
            # self.mano_agent #
            for cur_ts in range(self.nn_ts):
                actions = {}
                
                cur_mano_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_mano_glb_rot = cur_mano_glb_rot / torch.clamp(torch.norm(cur_mano_glb_rot, dim=-1, p=2), min=1e-7)
                cur_mano_glb_rot = dyn_model_act.quaternion_to_matrix(cur_mano_glb_rot) # mano glboal rotations #
                cur_mano_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    mano_links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.mano_agent.set_init_states_target_value(mano_links_init_states)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                else:
                    # if self.conf['model.train_states']:
                    #     cur_state = self.mano_robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    #     self.mano_agent.active_robot.set_delta_state_and_update(cur_state, cur_ts)
                    #     cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                    # else:
                    actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    mano_actions_link_actions = self.mano_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    actions['link_actions'] = mano_actions_link_actions
                    self.mano_agent.active_robot.calculate_inertia()
                    self.mano_agent.active_robot.set_actions_and_update_states(mano_actions_link_actions, cur_ts - 1, 0.2)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()

                # cur_mano_visual_pts = self.mano_agent.active_robot.get_transformed_visual_pts() # transformed visual pts # 
                
                if cur_ts > 0:
                    prev_rot = timestep_to_tot_rot[cur_ts - 1]
                    cur_mano_rot = torch.matmul(cur_mano_glb_rot, prev_rot)
                    prev_trans = timestep_to_tot_trans[cur_ts - 1]
                    cur_mano_trans = cur_mano_glb_trans + prev_trans
                else:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                if not self.conf['model.using_delta_glb_trans']:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_mano_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_mano_trans.detach()
                
                cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                if cur_ts > 0:
                    # need to combine the rigid deformation and the free deformation #
                    # rigid_deformation = cur_visual_pts - self.timestep_to_active_mesh[cur_ts - 1].detach()
                    # mano_rigid_deformation = cur_mano_visual_pts - self.timestep_to_posed_mano_active_mesh[cur_ts - 1].detach()
                    
                    # cur_mano_visual_pts = self.timestep_to_posed_mano_active_mesh[cur_ts - 1].detach() + mano_rigid_deformation * (1. - self.free_def_bending_weight)
                    
                    
                    # input_ts_latents = self.free_deformation_time_latent(torch.zeros((mano_rigid_deformation.size(0),), dtype=torch.long).cuda() + cur_ts)
                    
                    # # print(f"input_ts_latents: {input_ts_latents.size()}, timestep_to_active_mesh: {self.timestep_to_active_mesh[cur_ts - 1].size()}")
                    # input_free_def_latents = torch.cat(
                    #     [input_ts_latents, self.timestep_to_posed_mano_active_mesh[cur_ts - 1].detach()], dim=-1
                    # )
                    
                    # free_deformation = self.free_deformation_network(input_free_def_latents) 
                    
                    # cur_mano_visual_pts = cur_mano_visual_pts + free_deformation * self.free_def_bending_weight
                    
                    self.timestep_to_posed_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    
                    
                    cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                    
                    
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    # input_free_def_latents = torch.cat(
                    #     [input_ts_latents, self.timestep_to_active_mesh[cur_ts - 1].detach()], dim=-1
                    # )
                    # # input_free_def_latents #
                    # free_deformation = self.free_deformation_network(input_free_def_latents) # input_free_def_latnets #
                    # tot_def = rigid_deformation * (1. - self.free_def_bending_weight) + self.free_def_bending_weight * free_deformation
                    # cur_visual_pts = self.timestep_to_active_mesh[cur_ts - 1].detach() + tot_def # 
                else:
                    self.timestep_to_posed_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    
                    
                    
                
                
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                else:
                    if self.conf['model.train_states']:
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    else:
                        # should be able to set the states only viat he state values #
                        # actions['delta_glb_rot'] = cur_glb_rot #
                        # actions['delta_glb_trans'] = cur_glb_trans #
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        
                        actions['link_actions'] = actions_link_actions
                        self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()

                # cur_mano_visual_pts = self.mano_agent.active_robot.get_transformed_visual_pts() # transformed visual pts # 
                
                
                ### transform the visual pts ###
                cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_visual_pts = cur_visual_pts * 2. -1.
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                self.timestep_to_active_mesh[cur_ts] = cur_visual_pts #
                
                
                if cur_ts > 0:
                    prev_rot = timestep_to_tot_rot[cur_ts - 1]
                    cur_mano_rot = torch.matmul(cur_mano_glb_rot, prev_rot)
                    prev_trans = timestep_to_tot_trans[cur_ts - 1]
                    cur_mano_trans = cur_mano_glb_trans + prev_trans
                else:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                if not self.conf['model.using_delta_glb_trans']:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                # timestep_to_tot_rot[cur_ts] = cur_mano_rot.detach()
                # timestep_to_tot_trans[cur_ts] = cur_mano_trans.detach()
                
                # cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                if cur_ts == 0 and correspondence_pts_idxes is None:
                    dist_robot_pts_to_mano_pts = torch.sum(
                        (cur_visual_pts[sampled_verts_idxes].unsqueeze(1) - cur_mano_visual_pts.unsqueeze(0)) ** 2, dim=-1
                    )
                    minn_dist_robot_pts_to_mano_pts, correspondence_pts_idxes = torch.min(dist_robot_pts_to_mano_pts, dim=-1)
                    # dist_smaller_than_thres = minn_dist_robot_pts_to_mano_pts < 0.01
                    dist_smaller_than_thres = minn_dist_robot_pts_to_mano_pts < 0.001
                
                # timestep_to_corr_mano_pts # 
                # timestep_to_mano_active_mesh # 
                # cur_mano_visual_pts = cur_mano_visual_pts[correspondence_pts_idxes
                corr_correspondence_pts = cur_mano_visual_pts[correspondence_pts_idxes]
                
                self.timestep_to_corr_mano_pts[cur_ts] = corr_correspondence_pts.detach()
                
                
                diff_robo_to_corr_mano_pts = torch.sum(
                    (corr_correspondence_pts - cur_visual_pts[sampled_verts_idxes]) ** 2, dim=-1
                )
                diff_robo_to_corr_mano_pts = diff_robo_to_corr_mano_pts[dist_smaller_than_thres]
                diff_robo_to_corr_mano_pts = diff_robo_to_corr_mano_pts.mean()
                
                # and also the free motion network #
                ##### transform to the active mesh visual pts #####
                # the cur_ts + 1 state in the robot agent is corresponding to the cur_ts state in the simulation with manipulation #
                # cur_visual_pts_mano_pts = cur_visual_pts[self.mano_to_dyn_corr_pts_idxes]
                
                # self.timestep_to_active_mesh[cur_ts] = cur_visual_pts #
                
                
                if self.optimize_with_intermediates:
                    # timestep_to_passive_mesh_normals; if self.bending_net_type == 'active_force_field_v11'
                    if self.bending_net_type in[ 'active_force_field_v11', 'active_force_field_v12', 'active_force_field_v13', 'active_force_field_v14']:
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=cur_mano_visual_pts)
                        ### using mano pts to manipulate the object ###
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=correspondence_pts_idxes, reference_mano_pts=cur_mano_visual_pts)
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None)
                    elif self.bending_net_type in ['active_force_field_v13_lageu']:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes)
                    else:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, friction_forces=self.mano_expanded_actuator_friction_forces)
                # 
                cur_rhand_verts = self.rhand_verts[cur_ts]
                dist_active_pts_rhand_verts = torch.sum( # use the visual and the rhand verts ##
                    (cur_visual_pts.unsqueeze(1) - cur_rhand_verts.unsqueeze(0)) ** 2, dim=-1 ### nn_sampled_pts x nn_ref_pts ###
                ) # each oen should amtch one single point
                dist_rhand_verts_to_active_pts, _ = torch.min(dist_active_pts_rhand_verts, dim=0)
                dist_rhand_verts_to_active_pts = torch.mean(dist_rhand_verts_to_active_pts)
                dist_active_pts_rhand_verts, _ = torch.min(dist_active_pts_rhand_verts, dim=-1)
                dist_active_pts_rhand_verts = torch.mean(dist_active_pts_rhand_verts)
                
                # dist active pts rhand verts #
                dist_active_pts_rhand_verts = (dist_active_pts_rhand_verts + dist_rhand_verts_to_active_pts) / 2.
                
                
                if self.optimize_with_intermediates:
                    tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1)
                    # ### tracking loss with the cd loss -> to the reference motion ###
                    # loss = tracking_loss + dist_active_pts_rhand_verts
                    # ### cd loss ###
                    # loss = dist_active_pts_rhand_verts
                    # ### tracking loss with the cd loss -> to the reference motion ###
                    
                    # cur_ts_grid_forces = self.other_bending_network.timestep_to_grid_pts_forces[cur_ts]
                    # cur_ts_ref_grid_forces = self.ref_grid_forces[cur_ts]
                    # diff_cur_ref_contact_d_grid_forces = torch.sum(
                    #     (cur_ts_grid_forces[:, :3] - cur_ts_ref_grid_forces[:, :3]) ** 2, dim=-1 ## get the differences 
                    # )
                    # diff_contact_d = torch.sum(diff_cur_ref_contact_d_grid_forces)
                    # diff_contact_d = diff_contact_d * 1e4

                    # cur_ts_grid_weight ## cur_ts_grid_weight ####
                    # cur_ts_grid_weight = self.other_bending_network.timestep_to_grid_pts_weight[cur_ts]
                    # cur_ts_ref_grid_weight = self.ref_grid_weight[cur_ts]
                    # diff_grid_pts_weight = torch.sum(
                    #     (cur_ts_grid_weight - cur_ts_ref_grid_weight) ** 2, # dim=-1 ## get the differences
                    # )
                    # diff_grid_pts_weight = diff_grid_pts_weight * 1e4 #
                    
                    diff_contact_d = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    diff_grid_pts_weight = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    # tot_diff_grid_pts_forces, tot_diff_grid_pts_weight # 
                else:
                    tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    diff_contact_d = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    diff_grid_pts_weight = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                tot_diff_grid_pts_forces.append(diff_contact_d.detach().item())
                tot_diff_grid_pts_weight.append(diff_grid_pts_weight.detach().item())
                
                loss = diff_contact_d + diff_grid_pts_weight
                
                # self.ts_to_contact_pts[ts]
                if cur_ts in self.ts_to_contact_pts:
                    cur_contact_pts = self.ts_to_contact_pts[cur_ts] ### nn_contact_pts x 3 
                    dist_cur_active_pts_to_contact_pts = torch.sum(
                        (cur_visual_pts.unsqueeze(1) - cur_contact_pts.unsqueeze(0)) ** 2, dim=-1 
                    )
                    dist_to_contact, _ = torch.min(dist_cur_active_pts_to_contact_pts, dim=0)
                    dist_to_contact = torch.mean(dist_to_contact)
                    # loss = loss + dist_to_contact
                else:
                    dist_to_contact = torch.zeros((1,), dtype=torch.float32).cuda().mean()    
                    
                
                tot_dist_to_contact.append(dist_to_contact.item()) ### tot_dist_to_contact ## 
                
                tot_dist_act_to_rhand.append(dist_active_pts_rhand_verts.item())
                
                # # add finger tracking loss ##
                # cur_robot_finger_pts = cur_visual_pts[sampled_verts_idxes][self.robot_fingers[1:], :]
                # cur_mano_finger_pts = cur_rhand_verts[self.mano_fingers[1:], :]
                # dist_robot_fingers_to_mano_fingers = torch.sum(
                #     (cur_robot_finger_pts - cur_mano_finger_pts) ** 2, dim=-1
                # )
                # loss_finger_tracking = torch.mean(dist_robot_fingers_to_mano_fingers)
                
                # loss_corr_tracking = torch.sum(
                #     (cur_visual_pts_mano_pts - cur_rhand_verts) ** 2, dim=-1
                # )
                # loss_corr_tracking = loss_corr_tracking.mean()
                
                # tot_loss_corr_tracking.append(loss_corr_tracking)
                
                # tot_loss_figner_tracking.append(loss_finger_tracking.detach().item())
                
                loss = loss # + loss_corr_tracking
                
                # loss = loss_corr_tracking
                # if self.optimize_with_intermediates:
                #     loss = loss_corr_tracking  + tracking_loss * 0.5
                # loss_corr_tracking  # 
                
                loss_finger_tracking = diff_robo_to_corr_mano_pts
                tot_loss_figner_tracking.append(loss_finger_tracking.detach().item())
                loss = tracking_loss + loss_finger_tracking
                # constraints # 
                # constraints forces #
                # triangle constraints #
                
                if self.optimize_with_intermediates: # diffuse # diffuse # diffuse # how # with the correspondences and we change it to the grid pts forces #
                    # diffuse # 
                    # how to diffuse # four fingers # consolidaconsolidate via the distance via distances #
                    # from A to B #
                    if self.bending_net_type == 'active_force_field_v11':
                        cur_penalty_dot_forces_normals = self.other_bending_network.penalty_dot_forces_normals
                        cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                    elif self.bending_net_type == 'active_force_field_v13':
                        if self.no_friction_constraint:
                            cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                        else:
                            cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                    else:
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                        cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                else:
                    cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean().cuda()
                    cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean().cuda() ##
                
                loss = loss + cur_penalty_dot_forces_normals + cur_penalty_friction_constraint
                
                self.optimizer.zero_grad()
                # loss.backward()
                loss.backward(retain_graph=True)
                self.optimizer.step()
                
                tot_losses.append(loss.detach().item())
                tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                # if self.iter_step % self.report_freq == 0: #
                #     print(self.base_exp_dir)
                #     # print('iter:{:8>d} loss = {} sdf_value_loss = {} lr={}'.format(self.iter_step, loss, sdf_value_err.item(), self.optimizer.param_groups[0]['lr']))
                #     print('iter:{:8>d} loss = {} lr={}'.format(self.iter_step, loss, self.optimizer.param_groups[0]['lr']))
                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                # if self.iter_step % self.val_freq == 0:
                #     print(f"state_vals: {self.state_vals.data}")
                #     self.validate_image(0)
                #     # self.validate_image(0, use_gt_sdf=True) # 
                #     self.validate_image() # and the weights #
                #     # self.validate_image( use_gt_sdf=True)

                self.update_learning_rate() ## update learning rate ##
                
                
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            tot_penalty_dot_forces_normals = sum(tot_penalty_dot_forces_normals) / float(len(tot_penalty_dot_forces_normals))
            tot_penalty_friction_constraint = sum(tot_penalty_friction_constraint) / float(len(tot_penalty_friction_constraint))
            tot_dist_act_to_rhand = sum(tot_dist_act_to_rhand) / float(len(tot_dist_act_to_rhand))
            tot_dist_to_contact = sum(tot_dist_to_contact) / float(len(tot_dist_to_contact))
            tot_diff_grid_pts_forces = sum(tot_diff_grid_pts_forces) / float(len(tot_diff_grid_pts_forces))
            tot_diff_grid_pts_weight = sum(tot_diff_grid_pts_weight) / float(len(tot_diff_grid_pts_weight))
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            if len(tot_loss_figner_tracking) > 0:
                tot_loss_figner_tracking = sum(tot_loss_figner_tracking) / float(len(tot_loss_figner_tracking))
            else:
                tot_loss_figner_tracking = 0.
            # tot_loss_corr_tracking = sum(tot_loss_corr_tracking) / float(len(tot_loss_corr_tracking))
            tot_loss_corr_tracking = 0.
            if i_iter % self.report_freq == 0:
                # print(self.base_exp_dir) #
                print('iter:{:8>d} loss = {} loss_corr_tracking = {} tracking_loss = {} tot_loss_figner_tracking = {} diff_grid_pts_forces = {} diff_grid_pts_weight = {} act_to_rhand = {} dist_to_contact = {} penalty_dir = {} penalty_friction = {} lr={}'.format(self.iter_step, tot_losses, tot_loss_corr_tracking,  tot_tracking_loss, tot_loss_figner_tracking, tot_diff_grid_pts_forces, tot_diff_grid_pts_weight, tot_dist_act_to_rhand, tot_dist_to_contact, tot_penalty_dot_forces_normals, tot_penalty_friction_constraint, self.optimizer.param_groups[0]['lr']))

            if i_iter % self.val_mesh_freq == 0:
                # self.validate_mesh()
                self.validate_mesh_robo()
                # self.validate_mesh_expanded_pts()
            # self.active_robot.clear_grads()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
    
    # actions from mano model rules #
    def train_real_robot_actions_from_mano_model_rules_v2(self, ):
        
        ### the real robot actions from mano model rules ###
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path']
        robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        # morph robot agent #
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        ''' Load the robot hand '''
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano)
        self.mano_agent = mano_agent
        ''' Load the mano hand '''
        
        print(f"Start expanding the current visual pts...")
        expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        
        self.expanded_visual_pts_nn = expanded_visual_pts.size(0)
        ## expanded_visual_pts of the expanded visual pts #
        expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        
        # expaned visual pts 
        ## train robot ## 
        # for each mano action # 
        # should laod the pre-optimized mano action model file #
        # for each mano action, articulate the mano model and get the transformed visual pts # 
        # add an additional visual pts free motion field #
        # use the weighted motion to determine the real motion # 
        # sue the visual pts as the real manipulating pts which will further influence the pasiveobject states # 
        # use the visual pts to determine the passive object states ## object states ## 
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0. # 
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=778 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_robot_actuator_friction_forces.parameters())
        
        
        self.mano_expanded_actuator_delta_offset = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_delta_offset.weight)
        # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
        
        
        # mano_expanded_actuator_friction_forces, mano_expanded_actuator_delta_offset # 
        self.mano_expanded_actuator_friction_forces = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        # params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        
        
        ## with the timestep ##  
        # free_deformation_time_latent, free_deformation_network
        self.free_deformation_time_latent = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        )
        params_to_train += list(self.free_deformation_time_latent.parameters())
        
        # free deformations; time latnets #
        self.deformation_input_dim = 3 + self.bending_latent_size
        self.deformation_output_dim = 3
        ### free deformation network ### # free deformation # 
        self.free_deformation_network = self.construct_field_network(self.deformation_input_dim, self.bending_latent_size,  self.deformation_output_dim)
        params_to_train += list(self.free_deformation_network.parameters())
        
        
        
        # ### laod optimized init actions ####
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # cur_optimized_init_actions = 
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                self.mano_robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            self.mano_robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
            if 'expanded_actuator_friction_forces' in optimized_init_actions_ckpt:
                try:
                    self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_friction_forces'])
                except:
                    pass
            self.mano_expanded_actuator_delta_offset.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_delta_offset'])
        # ### load init transformations ckpts ###
        
        
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        params_to_train = []
        
        ''' Only set the robot friction forces optimizable '''
        params_to_train += list(self.robot_actions.parameters())
        params_to_train += list(self.robot_delta_states.parameters())
        params_to_train += list(self.robot_init_states.parameters())
        params_to_train += list(self.robot_glb_rotation.parameters())
        params_to_train += list(self.robot_glb_trans.parameters())
        ''' Only set the robot friction forces optimizable '''
        
        ## ##
        params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        # robot actuator friction forces #
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        if 'model.optimize_expanded_pts' in self.conf and self.conf['model.optimize_expanded_pts']:
            print(f"Setting optimizing expanded pts...")
            params_to_train = []
            params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
            params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
            
            self.optimize_expanded_pts = True
        else:
            self.optimize_expanded_pts = False
        
        # self.robot_glb_trans = nn.Embedding( # 
        #     num_embeddings=num_steps, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())
        
        
        # # sketche out the actions #
        # ### laod optimized init actions ####
        # if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
        #     cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
        #     # cur_optimized_init_actions = 
        #     optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
        #     self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
        #     self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
        #     if 'robot_delta_states' in optimized_init_actions_ckpt:
        #         self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
        #     self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions']) # 
        #     self.robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
        #     self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
        #     ### load init transformations ckpts ###

        ## self.maxx_robo_pts = 25. ##
        # self.maxx_robo_pts = 25. ## 
        # self.minn_robo_pts = -15. ## 
        # self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        # self.mult_const_after_cent = 0.5437551664260203
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_transformations_fn = self.conf['model.load_optimized_init_transformations']
            ##### Load optimized initial transformations #####
            optimized_init_transformations_ckpt = torch.load(cur_optimized_init_transformations_fn, map_location=self.device, )
            self.robot_init_states.load_state_dict(optimized_init_transformations_ckpt['robot_init_states'])
            self.robot_glb_rotation.load_state_dict(optimized_init_transformations_ckpt['robot_glb_rotation'])
            # if 'robot_delta_states' in optimized_init_transformations_ckpt:
            #     self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            # self.robot_actions.load_state_dict(optimized_init_transformations_ckpt['robot_actions'])
            self.robot_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['robot_actuator_friction_forces'])
            self.robot_glb_trans.load_state_dict(optimized_init_transformations_ckpt['robot_glb_trans'])
            
            if not self.load_only_glb:
                self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            
            # laod friction forces # friction forces # friction forces # 
            if 'model.load_friction_forces' in self.conf and len(self.conf['model.load_friction_forces']) > 0:
                print(f"Loading friction forces checkpoint from {self.conf['model.load_friction_forces']}")
                friction_forces_ckpt = self.conf['model.load_friction_forces']
                self.robot_actuator_friction_forces.load_state_dict(torch.load(friction_forces_ckpt, map_location=self.device)['robot_actuator_friction_forces'])
            
            # if 'expanded_actuator_friction_forces' in optimized_init_transformations_ckpt:
            #     try:
            #         self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['expanded_actuator_friction_forces'])
            #     except: # load friction force # 
            #         pass
        
        if 'model.load_optimized_expanded' in self.conf and len(self.conf['model.load_optimized_expanded']) > 0:
            load_optimized_expanded_ckpt = self.conf['model.load_optimized_expanded']
            load_optimized_expanded_ckpt = torch.load(load_optimized_expanded_ckpt, map_location=self.device)
            self.mano_expanded_actuator_delta_offset.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset'])
            self.mano_expanded_actuator_friction_forces.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_friction_forces'])
        
        # train the robot or train the expanded points # # expanded points # #
        # self.mano_fingers = [745, 279, 320, 444, 555, 672, 234, 121]
        # self.robot_fingers = [521, 624, 846, 973, 606, 459, 383, 265]
        
        # self.mano_fingers = torch.tensor(self.mano_fingers, dtype=torch.long).cuda()
        # self.robot_fingers = torch.tensor(self.robot_fingers, dtype=torch.long).cuda()
        
        self.nn_ts = self.nn_timesteps - 1
        # self.optimize_with_intermediates = False
        
        # search for the nearest point so that it can manipulate the object #
        ## TODO: co-optimize the bending network weights? ## # optimize active object is true ##
        if self.optimize_with_intermediates and (not self.optimize_active_object): # optimize with intermediates #  # search
            # if not self.optimize_active_object:
            # free_deformation_time_latent, free_deformation_network
            params_to_train = []
            params_to_train += list(self.other_bending_network.parameters())
            params_to_train += list(self.robot_actuator_friction_forces.parameters())
            # params_to_train += list(self.expanded_actuator_friction_forces.parameters()) # expanded actuator 
            # params_to_train += list(self.free_deformation_time_latent.parameters())
            # params_to_train += list(self.free_deformation_network.parameters())

        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        # init_rot = R.random().as_quat()
        # init_rot = torch.from_numpy(init_rot).float().cuda()
        # self.robot_glb_rotation.weight.data[0, :] = init_rot[:]
        
        # self #
        #### TODO: rotation optimization and the translation optimization ####
        for i_iter in tqdm(range(100000)): ## actions from model rules ##
            tot_losses = []
            tot_penalty_dot_forces_normals = []
            tot_penalty_friction_constraint = []
            tot_dist_act_to_rhand = []
            tot_dist_to_contact = []
            ## tot_diff_grid_pts_forces, tot_diff_grid_pts_weight ## # grid_pts_weight #
            tot_diff_grid_pts_forces = []
            tot_diff_grid_pts_weight = []
            tot_loss_figner_tracking = []
            tot_loss_corr_tracking = []
            tot_tracking_loss = []
            
            self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # self.timestep_to_
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            correspondence_pts_idxes = None
            
            tot_penetrating_depth_penalty = []
            
            for cur_ts in range(self.nn_ts):
                actions = {}
                
                cur_mano_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_mano_glb_rot = cur_mano_glb_rot / torch.clamp(torch.norm(cur_mano_glb_rot, dim=-1, p=2), min=1e-7)
                cur_mano_glb_rot = dyn_model_act.quaternion_to_matrix(cur_mano_glb_rot) # mano glboal rotations #
                cur_mano_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    mano_links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.mano_agent.set_init_states_target_value(mano_links_init_states)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                else:
                    # if self.conf['model.train_states']:
                    #     cur_state = self.mano_robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    #     self.mano_agent.active_robot.set_delta_state_and_update(cur_state, cur_ts)
                    #     cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                    # else:
                    actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    mano_actions_link_actions = self.mano_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    actions['link_actions'] = mano_actions_link_actions
                    self.mano_agent.active_robot.calculate_inertia()
                    self.mano_agent.active_robot.set_actions_and_update_states(mano_actions_link_actions, cur_ts - 1, 0.2)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()

                cur_mano_visual_pts = self.mano_agent.active_robot.get_transformed_visual_pts() # transformed visual pts # 
                
                if cur_ts > 0:
                    prev_rot = timestep_to_tot_rot[cur_ts - 1]
                    cur_mano_rot = torch.matmul(cur_mano_glb_rot, prev_rot)
                    prev_trans = timestep_to_tot_trans[cur_ts - 1]
                    cur_mano_trans = cur_mano_glb_trans + prev_trans
                else:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                if not self.conf['model.using_delta_glb_trans']:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_mano_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_mano_trans.detach()
                
                cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                if cur_ts > 0:
                    # need to combine the rigid deformation and the free deformation #
                    # rigid_deformation = cur_visual_pts - self.timestep_to_active_mesh[cur_ts - 1].detach()
                    # mano_rigid_deformation = cur_mano_visual_pts - self.timestep_to_posed_mano_active_mesh[cur_ts - 1].detach()
                    
                    # cur_mano_visual_pts = self.timestep_to_posed_mano_active_mesh[cur_ts - 1].detach() + mano_rigid_deformation * (1. - self.free_def_bending_weight)
                    
                    
                    # input_ts_latents = self.free_deformation_time_latent(torch.zeros((mano_rigid_deformation.size(0),), dtype=torch.long).cuda() + cur_ts)
                    
                    # # print(f"input_ts_latents: {input_ts_latents.size()}, timestep_to_active_mesh: {self.timestep_to_active_mesh[cur_ts - 1].size()}")
                    # input_free_def_latents = torch.cat(
                    #     [input_ts_latents, self.timestep_to_posed_mano_active_mesh[cur_ts - 1].detach()], dim=-1
                    # )
                    
                    # free_deformation = self.free_deformation_network(input_free_def_latents) 
                    
                    # delta motion # 
                    # cur_mano_visual_pts = cur_mano_visual_pts + free_deformation * self.free_def_bending_weight
                    
                    self.timestep_to_posed_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    
                    cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                    
                    
                    ## augment 
                    ''' augment with the delta motion '''
                    cur_delta_offset_indexes = torch.arange(self.expanded_visual_pts_nn, dtype=torch.long).cuda() + cur_ts * self.expanded_visual_pts_nn
                    cur_delta_offset = self.mano_expanded_actuator_delta_offset(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset
                    delta_offset_reg_motion = torch.sum(cur_delta_offset ** 2, dim=-1).mean() ### regularize the delta motion ###
                    ''' augment with the delta motion '''
                    
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                    # input_free_def_latents = torch.cat(
                    #     [input_ts_latents, self.timestep_to_active_mesh[cur_ts - 1].detach()], dim=-1
                    # )
                    # # input_free_def_latents #
                    # free_deformation = self.free_deformation_network(input_free_def_latents) # input_free_def_latnets #
                    # tot_def = rigid_deformation * (1. - self.free_def_bending_weight) + self.free_def_bending_weight * free_deformation
                    # cur_visual_pts = self.timestep_to_active_mesh[cur_ts - 1].detach() + tot_def # 
                else:
                    self.timestep_to_posed_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                    
                    
                    ''' augment with the delta motion '''
                    cur_delta_offset_indexes = torch.arange(self.expanded_visual_pts_nn, dtype=torch.long).cuda() + cur_ts * self.expanded_visual_pts_nn
                    cur_delta_offset = self.mano_expanded_actuator_delta_offset(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset
                    delta_offset_reg_motion = torch.sum(cur_delta_offset ** 2, dim=-1).mean() ### regularize the delta motion ###
                    ''' augment with the delta motion '''
                    
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                
                
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                else:
                    if self.conf['model.train_states']:
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    else:
                        # should be able to set the states only viat he state values #
                        # actions['delta_glb_rot'] = cur_glb_rot #
                        # actions['delta_glb_trans'] = cur_glb_trans #
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        
                        actions['link_actions'] = actions_link_actions
                        self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()


                ### transform the visual pts ###
                cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_visual_pts = cur_visual_pts * 2. -1.
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                self.timestep_to_active_mesh[cur_ts] = cur_visual_pts #
                
                
                # timestep_to_tot_rot[cur_ts] = cur_mano_rot.detach()
                # timestep_to_tot_trans[cur_ts] = cur_mano_trans.detach()
                
                # cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                # if i_iter == 0 and cur_ts < 11:
                if  cur_ts < 11:
                # if cur_ts == 0 and correspondence_pts_idxes is None:
                    dist_robot_pts_to_mano_pts = torch.sum(
                        (cur_visual_pts[sampled_verts_idxes].unsqueeze(1) - cur_mano_visual_pts.unsqueeze(0)) ** 2, dim=-1
                    )
                    minn_dist_robot_pts_to_mano_pts, correspondence_pts_idxes = torch.min(dist_robot_pts_to_mano_pts, dim=-1)
                    minn_dist_robot_pts_to_mano_pts = torch.sqrt(minn_dist_robot_pts_to_mano_pts)
                    # dist_smaller_than_thres = minn_dist_robot_pts_to_mano_pts < 0.01
                    dist_smaller_than_thres = minn_dist_robot_pts_to_mano_pts < 0.005
                    
                    corr_correspondence_pts = cur_mano_visual_pts[correspondence_pts_idxes]
                    dist_corr_correspondence_pts_to_mano_visual_pts = torch.sum(
                        (corr_correspondence_pts.unsqueeze(1) - cur_mano_visual_pts.unsqueeze(0)) ** 2, dim=-1
                    )
                    dist_corr_correspondence_pts_to_mano_visual_pts = torch.sqrt(dist_corr_correspondence_pts_to_mano_visual_pts)
                    minn_dist_to_corr_pts, _ = torch.min(dist_corr_correspondence_pts_to_mano_visual_pts, dim=0)
                    anchored_mano_visual_pts = minn_dist_to_corr_pts < 0.05
                    
                    # if 'model.finger_cd_loss' 
                    # the anchored mano visual pts # # cd loss 
                    # if self.finger_cd_loss_coef > 0.:
                    #     correspondence_pts_idxes = anchored_mano_visual_pts
                
                # timestep_to_corr_mano_pts # 
                # timestep_to_mano_active_mesh # 
                # cur_mano_visual_pts = cur_mano_visual_pts[correspondence_pts_idxes
                corr_correspondence_pts = cur_mano_visual_pts[correspondence_pts_idxes]
                
                ####### CD loss using the anchored mano visial pts ########
                # if self.finger_cd_loss_coef > 0.:
                # self.timestep_to_corr_mano_pts[cur_ts] = corr_correspondence_pts.detach()
                self.timestep_to_corr_mano_pts[cur_ts] = cur_mano_visual_pts[anchored_mano_visual_pts].detach()
                # mano visual pts and the dist #
                sampled_cur_visual_pts = cur_visual_pts[sampled_verts_idxes]
                cd_robo_pts_to_corr_mano_pts = torch.sum(
                    (sampled_cur_visual_pts.unsqueeze(1) - cur_mano_visual_pts[anchored_mano_visual_pts].unsqueeze(0)) ** 2, dim=-1
                )
                # cd_robo_pts_to_corr_mano_pts = torch.sqrt(cd_robo_pts_to_corr_mano_pts)
                cd_robo_to_mano, _ = torch.min(cd_robo_pts_to_corr_mano_pts, dim=-1)
                cd_mano_to_robo, _ = torch.min(cd_robo_pts_to_corr_mano_pts, dim=0)
                # finger_cd_loss_coef, finger_tracking_loss_coef, tracking_loss_coef, penetrating_depth_penalty_coef # 
                # diff_robo_to_corr_mano_pts = 0.5 * (cd_robo_to_mano.mean() + cd_mano_to_robo.mean())
                diff_robo_to_corr_mano_pts = cd_mano_to_robo.mean()
                ####### CD loss using the anchored mano visial pts ########
                

                ####### Tracking loss using the mano visual pts in correspondence ########
                ### the distance from the robot pts to the mano pts #
                ### how to parameterize the motion field ? ###
                self.timestep_to_corr_mano_pts[cur_ts] = corr_correspondence_pts.detach()
                diff_robo_to_corr_mano_pts_finger_tracking = torch.sum(
                    (corr_correspondence_pts - cur_visual_pts[sampled_verts_idxes]) ** 2, dim=-1
                )
                diff_robo_to_corr_mano_pts_finger_tracking = diff_robo_to_corr_mano_pts_finger_tracking[dist_smaller_than_thres]
                diff_robo_to_corr_mano_pts_finger_tracking = diff_robo_to_corr_mano_pts_finger_tracking.mean()
                ####### Tracking loss using the mano visual pts in correspondence ########
                
                # and also the free motion network #
                ##### transform to the active mesh visual pts #####
                # the cur_ts + 1 state in the robot agent is corresponding to the cur_ts state in the simulation with manipulation #
                # cur_visual_pts_mano_pts = cur_visual_pts[self.mano_to_dyn_corr_pts_idxes]
                
                # self.timestep_to_active_mesh[cur_ts] = cur_visual_pts #
                
                
                if self.optimize_with_intermediates:
                    # timestep_to_passive_mesh_normals; if self.bending_net_type == 'active_force_field_v11'
                    if self.bending_net_type in[ 'active_force_field_v11', 'active_force_field_v12', 'active_force_field_v13', 'active_force_field_v14']:
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=cur_mano_visual_pts)
                        ### using mano pts to manipulate the object ###
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=correspondence_pts_idxes, reference_mano_pts=cur_mano_visual_pts)
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None)
                         
                        if self.optimize_expanded_pts: # 
                            # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=correspondence_pts_idxes, reference_mano_pts=None)
                            # anchored_mano_visual_pts
                            self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=anchored_mano_visual_pts, reference_mano_pts=None)
                        else:
                            self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None)
                            
                    elif self.bending_net_type in ['active_force_field_v13_lageu']:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes)
                    else:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, friction_forces=self.mano_expanded_actuator_friction_forces)
                # 
                
                ''' Computing losses '''
                cur_rhand_verts = self.rhand_verts[cur_ts]
                dist_active_pts_rhand_verts = torch.sum(
                    (cur_visual_pts.unsqueeze(1) - cur_rhand_verts.unsqueeze(0)) ** 2, dim=-1 ### nn_sampled_pts x nn_ref_pts ###
                ) # each oen should amtch one single point
                dist_rhand_verts_to_active_pts, _ = torch.min(dist_active_pts_rhand_verts, dim=0)
                dist_rhand_verts_to_active_pts = torch.mean(dist_rhand_verts_to_active_pts)
                dist_active_pts_rhand_verts, _ = torch.min(dist_active_pts_rhand_verts, dim=-1)
                dist_active_pts_rhand_verts = torch.mean(dist_active_pts_rhand_verts)
                
                # dist active pts rhand verts #
                dist_active_pts_rhand_verts = (dist_active_pts_rhand_verts + dist_rhand_verts_to_active_pts) / 2.
                
                
                if self.optimize_with_intermediates:
                    tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1)
                    # ### tracking loss with the cd loss -> to the reference motion ###
                    # loss = tracking_loss + dist_active_pts_rhand_verts
                    # ### cd loss ###
                    # loss = dist_active_pts_rhand_verts
                    # ### tracking loss with the cd loss -> to the reference motion ###
                    
                    # cur_ts_grid_forces = self.other_bending_network.timestep_to_grid_pts_forces[cur_ts]
                    # cur_ts_ref_grid_forces = self.ref_grid_forces[cur_ts]
                    # diff_cur_ref_contact_d_grid_forces = torch.sum(
                    #     (cur_ts_grid_forces[:, :3] - cur_ts_ref_grid_forces[:, :3]) ** 2, dim=-1 ## get the differences 
                    # )
                    # diff_contact_d = torch.sum(diff_cur_ref_contact_d_grid_forces)
                    # diff_contact_d = diff_contact_d * 1e4

                    # cur_ts_grid_weight ## cur_ts_grid_weight ####
                    # cur_ts_grid_weight = self.other_bending_network.timestep_to_grid_pts_weight[cur_ts]
                    # cur_ts_ref_grid_weight = self.ref_grid_weight[cur_ts]
                    # diff_grid_pts_weight = torch.sum(
                    #     (cur_ts_grid_weight - cur_ts_ref_grid_weight) ** 2, # dim=-1 ## get the differences
                    # )
                    # diff_grid_pts_weight = diff_grid_pts_weight * 1e4 # ## diff contact d ##
                    
                    diff_contact_d = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    diff_grid_pts_weight = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    # tot_diff_grid_pts_forces, tot_diff_grid_pts_weight # 
                else:
                    tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    diff_contact_d = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    diff_grid_pts_weight = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                tot_diff_grid_pts_forces.append(diff_contact_d.detach().item())
                tot_diff_grid_pts_weight.append(diff_grid_pts_weight.detach().item())
                
                loss = diff_contact_d + diff_grid_pts_weight
                
                # self.ts_to_contact_pts[ts]
                if cur_ts in self.ts_to_contact_pts:
                    cur_contact_pts = self.ts_to_contact_pts[cur_ts] ### nn_contact_pts x 3 
                    dist_cur_active_pts_to_contact_pts = torch.sum(
                        (cur_visual_pts.unsqueeze(1) - cur_contact_pts.unsqueeze(0)) ** 2, dim=-1 
                    )
                    dist_to_contact, _ = torch.min(dist_cur_active_pts_to_contact_pts, dim=0)
                    dist_to_contact = torch.mean(dist_to_contact)
                    # loss = loss + dist_to_contact
                else:
                    dist_to_contact = torch.zeros((1,), dtype=torch.float32).cuda().mean()    
                    
                
                tot_dist_to_contact.append(dist_to_contact.item()) ### tot_dist_to_contact ## 
                
                tot_dist_act_to_rhand.append(dist_active_pts_rhand_verts.item())
                
                # # add finger tracking loss ##
                # cur_robot_finger_pts = cur_visual_pts[sampled_verts_idxes][self.robot_fingers[1:], :]
                # cur_mano_finger_pts = cur_rhand_verts[self.mano_fingers[1:], :]
                # dist_robot_fingers_to_mano_fingers = torch.sum(
                #     (cur_robot_finger_pts - cur_mano_finger_pts) ** 2, dim=-1
                # )
                # loss_finger_tracking = torch.mean(dist_robot_fingers_to_mano_fingers)
                
                # loss_corr_tracking = torch.sum(
                #     (cur_visual_pts_mano_pts - cur_rhand_verts) ** 2, dim=-1
                # )
                # loss_corr_tracking = loss_corr_tracking.mean()
                
                # tot_loss_corr_tracking.append(loss_corr_tracking)
                
                # tot_loss_figner_tracking.append(loss_finger_tracking.detach().item())
                
                # loss = loss_corr_tracking
                # if self.optimize_with_intermediates:
                #     loss = loss_corr_tracking  + tracking_loss * 0.5
                
                tot_penetrating_depth_penalty.append(self.other_bending_network.penetrating_depth_penalty.item())
                
                loss_finger_tracking = diff_robo_to_corr_mano_pts * self.finger_cd_loss_coef + diff_robo_to_corr_mano_pts_finger_tracking * self.finger_tracking_loss_coef
                
                tot_loss_figner_tracking.append(loss_finger_tracking.detach().item())
                
                if not self.optimize_expanded_pts:
                    # loss = tracking_loss + loss_finger_tracking * 0.1 # + self.other_bending_network.penetrating_depth_penalty * 0.000
                    loss = loss_finger_tracking + tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty  * self.penetrating_depth_penalty_coef
                else:
                    loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty # + loss_finger_tracking
                
                if self.optimize_with_intermediates:
                    if self.bending_net_type == 'active_force_field_v11':
                        cur_penalty_dot_forces_normals = self.other_bending_network.penalty_dot_forces_normals
                        cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                    elif self.bending_net_type == 'active_force_field_v13':
                        if self.no_friction_constraint:
                            cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                        else:
                            cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                    else:
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                        cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                else:
                    cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean().cuda()
                    cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean().cuda() ##
                
                loss = loss + cur_penalty_dot_forces_normals + cur_penalty_friction_constraint
                
                self.optimizer.zero_grad()
                # loss.backward()
                loss.backward(retain_graph=True)
                self.optimizer.step()
                
                tot_losses.append(loss.detach().item())
                tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                self.update_learning_rate() ## update learning rate ##
                
                
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            tot_penalty_dot_forces_normals = sum(tot_penalty_dot_forces_normals) / float(len(tot_penalty_dot_forces_normals))
            tot_penalty_friction_constraint = sum(tot_penalty_friction_constraint) / float(len(tot_penalty_friction_constraint))
            tot_dist_act_to_rhand = sum(tot_dist_act_to_rhand) / float(len(tot_dist_act_to_rhand))
            tot_dist_to_contact = sum(tot_dist_to_contact) / float(len(tot_dist_to_contact))
            tot_diff_grid_pts_forces = sum(tot_diff_grid_pts_forces) / float(len(tot_diff_grid_pts_forces))
            tot_diff_grid_pts_weight = sum(tot_diff_grid_pts_weight) / float(len(tot_diff_grid_pts_weight))
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            tot_penetrating_depth_penalty = sum(tot_penetrating_depth_penalty) / float(len(tot_penetrating_depth_penalty))
            if len(tot_loss_figner_tracking) > 0:
                tot_loss_figner_tracking = sum(tot_loss_figner_tracking) / float(len(tot_loss_figner_tracking))
            else:
                tot_loss_figner_tracking = 0.
            # tot_loss_corr_tracking = sum(tot_loss_corr_tracking) / float(len(tot_loss_corr_tracking))
            tot_loss_corr_tracking = 0.
            if i_iter % self.report_freq == 0:
                # print(self.base_exp_dir) #
                print('iter:{:8>d} loss = {} loss_corr_tracking = {} tracking_loss = {} tot_loss_figner_tracking = {} penetration_depth_penalty = {} diff_grid_pts_forces = {} diff_grid_pts_weight = {} act_to_rhand = {} dist_to_contact = {} penalty_dir = {} penalty_friction = {} lr={}'.format(self.iter_step, tot_losses, tot_loss_corr_tracking,  tot_tracking_loss, tot_loss_figner_tracking, tot_penetrating_depth_penalty, tot_diff_grid_pts_forces, tot_diff_grid_pts_weight, tot_dist_act_to_rhand, tot_dist_to_contact, tot_penalty_dot_forces_normals, tot_penalty_friction_constraint, self.optimizer.param_groups[0]['lr']))

            if i_iter % self.val_mesh_freq == 0:
                # self.validate_mesh()
                self.validate_mesh_robo()
                # self.validate_mesh_expanded_pts()
            # self.active_robot.clear_grads()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
    
    
    def train_real_robot_actions_from_mano_model_rules_v3(self, ):
        
        ### the real robot actions from mano model rules ###
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path']
        robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        # morph robot agent #
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        ''' Load the robot hand '''
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano)
        self.mano_agent = mano_agent
        ''' Load the mano hand '''
        
        print(f"Start expanding the current visual pts...")
        expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        
        self.expanded_visual_pts_nn = expanded_visual_pts.size(0)
        ## expanded_visual_pts of the expanded visual pts #
        expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0. # 
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=778 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_robot_actuator_friction_forces.parameters())
        
        
        self.mano_expanded_actuator_delta_offset = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_delta_offset.weight)
        # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
        
        
        self.mano_expanded_actuator_delta_offset_nex = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_delta_offset_nex.weight)
        # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
        
        
        # mano_expanded_actuator_friction_forces, mano_expanded_actuator_delta_offset # 
        self.mano_expanded_actuator_friction_forces = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        # params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        
        # 
        # free_deformation_time_latent, free_deformation_network
        self.free_deformation_time_latent = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        )
        params_to_train += list(self.free_deformation_time_latent.parameters())
        
        # free deformations; time latnets #
        self.deformation_input_dim = 3 + self.bending_latent_size
        self.deformation_output_dim = 3
        ### free deformation network ### # free deformation # 
        self.free_deformation_network = self.construct_field_network(self.deformation_input_dim, self.bending_latent_size,  self.deformation_output_dim)
        params_to_train += list(self.free_deformation_network.parameters())
        
        
        
        # ### laod optimized init actions ####
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # cur_optimized_init_actions = 
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                self.mano_robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            self.mano_robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
            if 'expanded_actuator_friction_forces' in optimized_init_actions_ckpt:
                try:
                    self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_friction_forces'])
                except:
                    pass
            self.mano_expanded_actuator_delta_offset.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_delta_offset'])
            # if 'mano_expanded_actuator_delta_offset_nex' in 
        # ### load init transformations ckpts ### #
        
        # robot actions #
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        ## robot actuator friction #    
        # robot actuator friction forces #
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        
        params_to_train = []
        ''' Only set the robot friction forces optimizable '''
        # if self.optimize_robot :
        if not self.optimize_rules:
            params_to_train += list(self.robot_actions.parameters())
            params_to_train += list(self.robot_delta_states.parameters())
            params_to_train += list(self.robot_init_states.parameters())
            params_to_train += list(self.robot_glb_rotation.parameters())
            params_to_train += list(self.robot_glb_trans.parameters())
            
        ''' Only set the robot friction forces optimizable '''
        
        ## ## # expanded
        params_to_train += list(self.mano_expanded_actuator_delta_offset_nex.parameters())
        params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        
        params_to_train += list(self.robot_actuator_friction_forces.parameters())
        if self.optimize_rules:
            params_to_train += list(self.other_bending_network.parameters())
        
        
        ### motivate the robot via actions ###
        ### 
        ### not set the states -> set the actions ###
        ### loss 1 -> differences between the current states and the target states ###
        ### loss 2 -> tracking loss ###
        ### the contact establishment and the cancellation ###
        
        # get states and use the differences between the current 
        # self.robot_glb_trans = nn.Embedding( # 
        #     num_embeddings=num_steps, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())
        
        
        # # sketche out the actions # #
        # ### laod optimized init actions ####
        # if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
        #     cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
        #     # cur_optimized_init_actions = 
        #     optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
        #     self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
        #     self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
        #     if 'robot_delta_states' in optimized_init_actions_ckpt:
        #         self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
        #     self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions']) # 
        #     self.robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
        #     self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
        #     ### load init transformations ckpts ###

        ### affine ###
        ## self.maxx_robo_pts = 25. ## 
        # self.maxx_robo_pts = 25. ## 
        # self.minn_robo_pts = -15. ## 
        # self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts 
        # self.mult_const_after_cent = 0.5437551664260203 
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_transformations_fn = self.conf['model.load_optimized_init_transformations']
            ##### Load optimized initial transformations #####
            optimized_init_transformations_ckpt = torch.load(cur_optimized_init_transformations_fn, map_location=self.device, )
            self.robot_init_states.load_state_dict(optimized_init_transformations_ckpt['robot_init_states'])
            self.robot_glb_rotation.load_state_dict(optimized_init_transformations_ckpt['robot_glb_rotation'])
            # if 'robot_delta_states' in optimized_init_transformations_ckpt:
            #     self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            self.robot_actions.load_state_dict(optimized_init_transformations_ckpt['robot_actions'])
            self.robot_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['robot_actuator_friction_forces'])
            self.robot_glb_trans.load_state_dict(optimized_init_transformations_ckpt['robot_glb_trans'])
            
            if not self.load_only_glb:
                self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            
            # laod friction forces # friction forces # friction forces # 
            # if 'model.load_friction_forces' in self.conf and len(self.conf['model.load_friction_forces']) > 0:
            #     print(f"Loading friction forces checkpoint from {self.conf['model.load_friction_forces']}")
            #     friction_forces_ckpt = self.conf['model.load_friction_forces']
            #     self.robot_actuator_friction_forces.load_state_dict(torch.load(friction_forces_ckpt, map_location=self.device)['robot_actuator_friction_forces'])
            
            # if 'expanded_actuator_friction_forces' in optimized_init_transformations_ckpt:
            #     try:
            #         self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['expanded_actuator_friction_forces'])
            #     except: # load friction force # 
            #         pass
            
        if 'model.load_optimized_expanded' in self.conf and len(self.conf['model.load_optimized_expanded']) > 0:
            load_optimized_expanded_ckpt = self.conf['model.load_optimized_expanded']
            load_optimized_expanded_ckpt = torch.load(load_optimized_expanded_ckpt, map_location=self.device)
            self.mano_expanded_actuator_delta_offset.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset'])
            self.mano_expanded_actuator_friction_forces.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_friction_forces'])
            if 'mano_expanded_actuator_delta_offset_nex' in load_optimized_expanded_ckpt:
                print(f"Loading expanded actuator delta offset nex from the saved checkpoint...")
                self.mano_expanded_actuator_delta_offset_nex.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset_nex'])
        
        # # 
        # init_states glb_rotation, delta_states, robot_actions_ori, 
        # make the current states and others near to the original ones #
        # robot_init_states_ori, robot_glb_rotation_ori, robot_delta_states_ori, robot_actions_ori, robot_glb_trans_ori #
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        
        # train the robot or train the expanded points # # expanded points # #
        # YC #
        # train the robot or train the expanded points # # expanded points # #
        # self.mano_fingers = [745, 279, 320, 444, 555, 672, 234, 121]
        # self.robot_fingers = [521, 624, 846, 973, 606, 459, 383, 265]
        
        # optimize 
        # optimize with intermediates # 
        # self.mano_fingers = torch.tensor(self.mano_fingers, dtype=torch.long).cuda()
        # self.robot_fingers = torch.tensor(self.robot_fingers, dtype=torch.long).cuda()
        
        self.nn_ts = self.nn_timesteps - 1
        # self.optimize_with_intermediates = False
        
        # search for the nearest point so that it can manipulate the object #
        ## TODO: co-optimize the bending network weights? ## # optimize active object is true ##
        # if self.optimize_with_intermediates and (not self.optimize_active_object): # optimize with intermediates #  # search
        #     # if not self.optimize_active_object: #
        #     # free_deformation_time_latent, free_deformation_network #
        #     params_to_train = []
        #     params_to_train += list(self.other_bending_network.parameters())
        #     params_to_train += list(self.robot_actuator_friction_forces.parameters())
        #     # params_to_train += list(self.expanded_actuator_friction_forces.parameters()) # expanded actuator 
        #     # params_to_train += list(self.free_deformation_time_latent.parameters())
        #     # params_to_train += list(self.free_deformation_network.parameters()) # 

        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        # init_rot = R.random().as_quat()
        # init_rot = torch.from_numpy(init_rot).float().cuda()
        # self.robot_glb_rotation.weight.data[0, :] = init_rot[:]
        
        
        #### get motions --- the original motion of the expanded visual points ####
        #### rag expanded points to the target object ####
        #### use the target object motion to explain the expanded object motion ####
        
        ### Constraint set ###
        self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        # 
        # init_visual_pts, link_name_to_transformations_and_transformed_pts = get_init_state_visual_pts(ret_link_name_to_tansformations=True)
        # set_body_expanded_visual_pts
        # expanded_visual_pts = compute_expanded_visual_pts_transformation_via_current_state()
        
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts #
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        with torch.no_grad():
            for cur_ts in range(self.nn_ts):
                cur_mano_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_mano_glb_rot = cur_mano_glb_rot / torch.clamp(torch.norm(cur_mano_glb_rot, dim=-1, p=2), min=1e-7)
                cur_mano_glb_rot = dyn_model_act.quaternion_to_matrix(cur_mano_glb_rot) # mano glboal rotations #
                cur_mano_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    mano_links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.mano_agent.set_init_states_target_value(mano_links_init_states)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                else:
                    actions = {}
                    actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    mano_actions_link_actions = self.mano_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    actions['link_actions'] = mano_actions_link_actions
                    self.mano_agent.active_robot.calculate_inertia()
                    self.mano_agent.active_robot.set_actions_and_update_states(mano_actions_link_actions, cur_ts - 1, 0.2)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                cur_mano_visual_pts = self.mano_agent.active_robot.get_transformed_visual_pts() # transformed visual pts # 
                cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                cur_mano_rot = cur_mano_glb_rot
                cur_mano_trans = cur_mano_glb_trans
                ### mano visual pts ###
                cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                
                
                ''' augment with the delta motion '''
                cur_delta_offset_indexes = torch.arange(self.expanded_visual_pts_nn, dtype=torch.long).cuda() + cur_ts * self.expanded_visual_pts_nn
                cur_delta_offset = self.mano_expanded_actuator_delta_offset(cur_delta_offset_indexes)
                cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset # delta offset #
                
                cur_delta_offset_nex = self.mano_expanded_actuator_delta_offset_nex(cur_delta_offset_indexes)
                cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset_nex
                
                # delta_offset_reg_motion = torch.sum(cur_delta_offset ** 2, dim=-1).mean() ### regularize the delta motion #### save the expanded visual pts ####
                self.timestep_to_expanded_visual_pts[cur_ts] = cur_mano_visual_pts.detach().cpu().numpy()
                
                
                if cur_ts < 11:
                    cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                    cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                    cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    if cur_ts == 0:
                        links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                        self.robot_agent.set_init_states_target_value(links_init_states)
                        cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                    else:
                        if self.conf['model.train_states']:
                            cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                            states = {}
                            states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                            states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                            states['link_states'] = cur_state
                            # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                            self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                        else:
                            # should be able to set the states only viat he state values #
                            # actions['delta_glb_rot'] = cur_glb_rot #
                            # actions['delta_glb_trans'] = cur_glb_trans #
                            actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                            actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                            
                            actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                            
                            actions['link_actions'] = actions_link_actions # set actions and the s
                            # set and update 
                            self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                            
                        if cur_ts == 10:
                            cur_visual_pts, link_name_to_transformations_and_transformed_pts = self.robot_agent.get_init_state_visual_pts(ret_link_name_to_tansformations=True)
                        else:
                            cur_visual_pts = self.robot_agent.get_init_state_visual_pts()


                    ### transform the visual pts ###
                    cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                    cur_visual_pts = cur_visual_pts * 2. -1.
                    cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                    
                    cur_rot = cur_glb_rot
                    cur_trans = cur_glb_trans
                    
                    # timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                    # timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                    
                    ### transform by the glboal transformation and the translation ###
                    cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                    
                    # self.timestep_to_active_mesh[cur_ts] = cur_visual_pts #
                    
                    if cur_ts == 10:
                        link_name_to_ragged_ref_pts = {}
                        ### get the ragged points ###
                        link_name_to_link_idx = {}
                        cur_link_idx = 0
                        for link_name in link_name_to_transformations_and_transformed_pts:
                            link_name_to_link_idx[link_name] = cur_link_idx
                            cur_link_idx += 1
                        link_idx_to_link_name = {v: k for k, v in link_name_to_link_idx.items()}
                        dist_expanded_visual_pts_to_link_pts = []
                        
                        link_name_to_selected_link_idxes = {}
                        
                        link_name_to_tot_transformed_pts = {}
                        for cur_link_idx in range(len(link_name_to_transformations_and_transformed_pts)):
                            cur_link_name = link_idx_to_link_name[cur_link_idx]
                            cur_link_transformed_pts, cur_link_transformations = link_name_to_transformations_and_transformed_pts[cur_link_name]
                            
                            cur_link_transformed_pts = (cur_link_transformed_pts - self.minn_robo_pts) / self.extent_robo_pts
                            cur_link_transformed_pts = cur_link_transformed_pts * 2. -1.
                            cur_link_transformed_pts = cur_link_transformed_pts * self.mult_const_after_cent 
                            
                            
                            
                            
                            
                            cur_link_transformed_pts = torch.matmul(cur_rot, cur_link_transformed_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                            
                            link_name_to_tot_transformed_pts[cur_link_name] = cur_link_transformed_pts.detach().cpu().numpy()
                            
                            
                            permutation_idxes = torch.randperm(cur_link_transformed_pts.size(0)).cuda()[:1000]
                            
                            link_name_to_selected_link_idxes[cur_link_name] = permutation_idxes
                            
                            cur_link_transformed_pts_selected = cur_link_transformed_pts[permutation_idxes]
                            
                            dist_expanded_visual_pts_to_cur_link_pts = torch.sum(
                                (cur_mano_visual_pts.unsqueeze(1) - cur_link_transformed_pts_selected.unsqueeze(0)) ** 2, dim=-1
                            )
                            minn_dist_expanded_visual_pts_to_cur_link_pts, _ = torch.min(dist_expanded_visual_pts_to_cur_link_pts, dim=-1)
                            dist_expanded_visual_pts_to_link_pts.append(minn_dist_expanded_visual_pts_to_cur_link_pts)
                        dist_expanded_visual_pts_to_link_pts = torch.stack(dist_expanded_visual_pts_to_link_pts, dim=1)
                        # # 
                        minn_dist_expanded_visual_pts_to_link_pts, minn_idx_expanded_visual_pts_to_link_pts = torch.min(dist_expanded_visual_pts_to_link_pts, dim=-1) ## minn_idx_expanded_visual_pts_to_link_pts ##
                        tot_link_rotations = []
                        tot_link_translations = []
                        for cur_link_idx in range(len(link_name_to_transformations_and_transformed_pts)):
                            expanded_visual_pts_ragged_to_cur_link_mask = minn_idx_expanded_visual_pts_to_link_pts == cur_link_idx
                            cur_link_name = link_idx_to_link_name[cur_link_idx]
                            _, cur_link_transformations = link_name_to_transformations_and_transformed_pts[cur_link_name]
                            if expanded_visual_pts_ragged_to_cur_link_mask.float().sum() > 0:
                                expanded_visual_pts_ragged_to_cur_link = cur_mano_visual_pts[expanded_visual_pts_ragged_to_cur_link_mask]
                                cur_link_rotations, cur_link_translations = cur_link_transformations
                                # R^T (pts - center) #
                                ref_pts = torch.matmul(cur_link_rotations.transpose(1, 0), (expanded_visual_pts_ragged_to_cur_link - cur_link_translations.unsqueeze(0)).contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous()
                                link_name_to_ragged_ref_pts[cur_link_name] = ref_pts.detach() # .cpu().numpy()
                            tot_link_rotations.append(cur_link_transformations[0])
                            tot_link_translations.append(cur_link_transformations[1])
                        # self.robot_agent.set_body_expanded_visual_pts(link_name_to_ragged_ref_pts)
                        
                        tot_link_rotations = torch.stack(tot_link_rotations, dim=0)
                        tot_link_translations = torch.stack(tot_link_translations, dim=0)
                        
                        
                        ref_expanded_visual_pts = torch.matmul(cur_rot.transpose(1, 0), (cur_mano_visual_pts - cur_trans.unsqueeze(0)).contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() 
                        ref_expanded_visual_pts = ref_expanded_visual_pts / self.mult_const_after_cent
                        ref_expanded_visual_pts = (ref_expanded_visual_pts + 1.) / 2. 
                        ref_expanded_visual_pts = ref_expanded_visual_pts * self.extent_robo_pts + self.minn_robo_pts
                        
                        
                        
                        # # nn_visual_pts # 
                        expanded_visual_pts_selected_rotations = fields.batched_index_select(values=tot_link_rotations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                        expanded_visual_pts_selected_translaations = fields.batched_index_select(values=tot_link_translations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                        
                        ref_expanded_visual_pts = torch.matmul(
                            expanded_visual_pts_selected_rotations.contiguous().transpose(2, 1), (ref_expanded_visual_pts - expanded_visual_pts_selected_translaations).unsqueeze(-1)
                        ).squeeze(-1) ## nn_visual_pts x 3 ##
                        
                        
                        save_visual_pts_link_name_to_visual_pts = {
                            'cur_mano_visual_pts': cur_mano_visual_pts.detach().cpu().numpy(),
                            'link_name_to_tot_transformed_pts' : link_name_to_tot_transformed_pts
                        }
                        np.save(f"save_visual_pts_link_name_to_visual_pts.npy", save_visual_pts_link_name_to_visual_pts)
                        
        
        if self.train_actions_with_states:
            for cur_ts in range(self.nn_ts):
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                else:
                    # if self.conf['model.train_states']:
                    cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    states = {}
                    states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    states['link_states'] = cur_state
                    self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)      
            tmp_joint_nm_to_ts_to_states = self.robot_agent.get_timestep_to_states()
            joint_nm_to_ts_to_states = {}
            for cur_joint_nm in tmp_joint_nm_to_ts_to_states:
                cur_joint_ts_to_states = {}
                for cur_ts in tmp_joint_nm_to_ts_to_states[cur_joint_nm]:
                    cur_joint_ts_to_states[cur_ts] = tmp_joint_nm_to_ts_to_states[cur_joint_nm][cur_ts].detach().clone()
                joint_nm_to_ts_to_states[cur_joint_nm] = cur_joint_ts_to_states
    
        # model_path = self.conf['model.sim_model_path']
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        # self.robot_agent = robot_agent 
        
        penetration_forces = None
        
        #### get the expanded ####
        ### get the expanded transformed points at each timestep ###
        #### TODO: rotation optimization and the translation optimization ####
        for i_iter in tqdm(range(100000)): ## actions from model rules ### 
            tot_losses = []
            tot_penalty_dot_forces_normals = []
            tot_penalty_friction_constraint = []
            tot_dist_act_to_rhand = []
            tot_dist_to_contact = []
            ## tot_diff_grid_pts_forces, tot_diff_grid_pts_weight ##
            tot_diff_grid_pts_forces = []
            tot_diff_grid_pts_weight = []
            tot_loss_figner_tracking = []
            tot_loss_corr_tracking = []
            tot_tracking_loss = []
            
            self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # self.timestep_to_
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            correspondence_pts_idxes = None
            
            tot_penetrating_depth_penalty = []
            
            tot_ragged_dist = []
            
            tot_delta_offset_reg_motion = []
            
            tot_dist_mano_visual_ori_to_cur = []
            
            tot_reg_loss = []
            
            tot_diff_cur_states_to_ref_states = []
            
            penetration_forces = None
            sampled_visual_pts_joint_idxes = None
            
            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16]
            
            
            for cur_ts in range(self.nn_ts):
                actions = {}
                
                cur_mano_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_mano_glb_rot = cur_mano_glb_rot / torch.clamp(torch.norm(cur_mano_glb_rot, dim=-1, p=2), min=1e-7)
                cur_mano_glb_rot = dyn_model_act.quaternion_to_matrix(cur_mano_glb_rot) # mano glboal rotations #
                cur_mano_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    mano_links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.mano_agent.set_init_states_target_value(mano_links_init_states)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                else:
                    # if self.conf['model.train_states']:
                    #     cur_state = self.mano_robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    #     self.mano_agent.active_robot.set_delta_state_and_update(cur_state, cur_ts)
                    #     cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                    # else:
                    actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    mano_actions_link_actions = self.mano_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    actions['link_actions'] = mano_actions_link_actions
                    self.mano_agent.active_robot.calculate_inertia()
                    self.mano_agent.active_robot.set_actions_and_update_states(mano_actions_link_actions, cur_ts - 1, 0.2)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                ## mano visual pts ##
                cur_mano_visual_pts = self.mano_agent.active_robot.get_transformed_visual_pts() # transformed visual pts #
                
                
                
                
                if cur_ts > 0:
                    prev_rot = timestep_to_tot_rot[cur_ts - 1]
                    cur_mano_rot = torch.matmul(cur_mano_glb_rot, prev_rot)
                    prev_trans = timestep_to_tot_trans[cur_ts - 1]
                    cur_mano_trans = cur_mano_glb_trans + prev_trans
                else:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                if not self.conf['model.using_delta_glb_trans']:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_mano_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_mano_trans.detach()
                
                cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                if cur_ts > 0:
                    ## sue the original mano articulated motion with delta position offsets ##
                    self.timestep_to_posed_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    
                    cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                    
                    ''' augment with the delta motion '''
                    cur_delta_offset_indexes = torch.arange(self.expanded_visual_pts_nn, dtype=torch.long).cuda() + cur_ts * self.expanded_visual_pts_nn
                    cur_delta_offset = self.mano_expanded_actuator_delta_offset(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset
                    
                    cur_delta_offset_nex = self.mano_expanded_actuator_delta_offset_nex(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset_nex
                    
                    delta_offset_reg_motion = torch.sum(cur_delta_offset_nex ** 2, dim=-1).mean() ### regularize the delta motion ###
                    ''' augment with the delta motion '''
                    
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                else:
                    self.timestep_to_posed_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                    
                    ''' augment with the delta motion '''
                    cur_delta_offset_indexes = torch.arange(self.expanded_visual_pts_nn, dtype=torch.long).cuda() + cur_ts * self.expanded_visual_pts_nn
                    cur_delta_offset = self.mano_expanded_actuator_delta_offset(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset
                    
                    cur_delta_offset_nex = self.mano_expanded_actuator_delta_offset_nex(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset_nex
                    
                    delta_offset_reg_motion = torch.sum(cur_delta_offset_nex ** 2, dim=-1).mean() ### regularize the delta motion ###
                    ''' augment with the delta motion '''
                    
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                
                
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts, link_name_to_link_transformations = self.robot_agent.get_init_state_visual_pts(True)
                else:
                    if self.conf['model.train_states']:
                        if self.add_delta_state_constraints:
                            self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg] = torch.clamp(self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg], max=0.)
                        
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    else:
                        # print(f"Draing via actions")
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        
                        actions['link_actions'] = actions_link_actions
                        joint_name_to_penetration_forces_intermediates = self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1, penetration_forces=penetration_forces, sampled_visual_pts_joint_idxes=sampled_visual_pts_joint_idxes) # upate the state at cur_ts # 
                        self.joint_name_to_penetration_forces_intermediates[cur_ts] = joint_name_to_penetration_forces_intermediates
                    cur_visual_pts, link_name_to_link_transformations, visual_pts_joint_idxes = self.robot_agent.get_init_state_visual_pts(True, True)

                
                if self.train_actions_with_states:
                    nex_ts_joint_nm_to_states = self.robot_agent.get_joint_nm_to_states()
                    diff_cur_states_to_ref_states = []
                    for joint_nm in nex_ts_joint_nm_to_states:
                        cur_states = nex_ts_joint_nm_to_states[joint_nm]
                        # ref_states = joint_nm_to_ts_to_states[joint_nm][cur_ts + 1]
                        ### reference state at the time cur_ts ####
                        ref_states = joint_nm_to_ts_to_states[joint_nm][cur_ts]
                        diff_cur_joint = torch.sum(
                            (cur_states - ref_states.detach()) ** 2
                        )
                        diff_cur_states_to_ref_states.append(diff_cur_joint)
                        # print(f"cur_ts: {cur_ts}, joint_nm: {joint_nm}, cur_states: {cur_states}, ref_states: {ref_states}") #
                    
                    diff_cur_states_to_ref_states = torch.stack(diff_cur_states_to_ref_states, dim=0)
                    diff_cur_states_to_ref_states = diff_cur_states_to_ref_states.mean()
                else: # cur states to ref states #
                    diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                        
                
                # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                
                
                # minn_idx_expanded_visual_pts_to_link_pts # 
                # get the tr# the rest states # 

                ### transform the visual pts ###
                cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_visual_pts = cur_visual_pts * 2. - 1.
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                self.timestep_to_active_mesh[cur_ts] = cur_visual_pts #
                
                self.timestep_to_raw_active_meshes[cur_ts] = cur_visual_pts.detach().cpu().numpy()
                
                
                
                tot_link_rotations = []
                tot_link_translations = []
                for cur_link_idx in range(len(link_name_to_link_transformations)):
                    # expanded_visual_pts_ragged_to_cur_link_mask = minn_idx_expanded_visual_pts_to_link_pts == cur_link_idx
                    cur_link_name = link_idx_to_link_name[cur_link_idx]
                    _, cur_link_transformations = link_name_to_link_transformations[cur_link_name]
                    tot_link_rotations.append(cur_link_transformations[0])
                    tot_link_translations.append(cur_link_transformations[1])
                tot_link_rotations = torch.stack(tot_link_rotations, dim=0)
                tot_link_translations = torch.stack(tot_link_translations, dim=0)
                
                    
                
                # # empty_cache() # #
                if cur_ts == 10:
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    
                    dist_expanded_visual_pts_to_link_pts = []
                        
                    link_name_to_tot_transformed_pts = {}
                    for cur_link_idx in range(len(link_name_to_link_transformations)):
                        cur_link_name = link_idx_to_link_name[cur_link_idx]
                        cur_link_transformed_pts, cur_link_transformations = link_name_to_link_transformations[cur_link_name]
                        
                        cur_link_transformed_pts = (cur_link_transformed_pts - self.minn_robo_pts) / self.extent_robo_pts
                        cur_link_transformed_pts = cur_link_transformed_pts * 2. -1.
                        cur_link_transformed_pts = cur_link_transformed_pts * self.mult_const_after_cent 
                        
                        cur_link_transformed_pts = torch.matmul(cur_rot, cur_link_transformed_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                        
                        link_name_to_tot_transformed_pts[cur_link_name] = cur_link_transformed_pts.detach().cpu().numpy()
                        
                        
                        permutation_idxes = link_name_to_selected_link_idxes[cur_link_name]
                        cur_link_transformed_pts_selected = cur_link_transformed_pts[permutation_idxes].detach()
                        
                        dist_expanded_visual_pts_to_cur_link_pts = torch.sum(
                            (cur_mano_visual_pts.unsqueeze(1) - cur_link_transformed_pts_selected.unsqueeze(0)) ** 2, dim=-1
                        )
                        minn_dist_expanded_visual_pts_to_cur_link_pts, _ = torch.min(dist_expanded_visual_pts_to_cur_link_pts, dim=-1)
                        dist_expanded_visual_pts_to_link_pts.append(minn_dist_expanded_visual_pts_to_cur_link_pts)
                    dist_expanded_visual_pts_to_link_pts = torch.stack(dist_expanded_visual_pts_to_link_pts, dim=1)
                    # # 
                    minn_dist_expanded_visual_pts_to_link_pts, minn_idx_expanded_visual_pts_to_link_pts = torch.min(dist_expanded_visual_pts_to_link_pts, dim=-1) ## 
                        
                        
                    ref_expanded_visual_pts = torch.matmul(cur_rot.transpose(1, 0), (cur_mano_visual_pts - cur_trans.unsqueeze(0)).contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() 
                    ref_expanded_visual_pts = ref_expanded_visual_pts / self.mult_const_after_cent
                    ref_expanded_visual_pts = (ref_expanded_visual_pts + 1.) / 2. 
                    ref_expanded_visual_pts = ref_expanded_visual_pts * self.extent_robo_pts + self.minn_robo_pts
                    
                    expanded_visual_pts_selected_rotations = fields.batched_index_select(values=tot_link_rotations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                    expanded_visual_pts_selected_translaations = fields.batched_index_select(values=tot_link_translations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                    
                    ref_expanded_visual_pts = torch.matmul(
                        expanded_visual_pts_selected_rotations.contiguous().transpose(2, 1), (ref_expanded_visual_pts - expanded_visual_pts_selected_translaations).unsqueeze(-1)
                    ).squeeze(-1).detach() ## nn_visual_pts x 3 ##
                    
                    ragged_dist = torch.mean(minn_dist_expanded_visual_pts_to_link_pts)
                else:
                    ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()

                #     ragged_dist = torch.mean(minn_dist_expanded_visual_pts_to_link_pts)
                # else: # ragged dist #                                    # # only optimize 
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()

                # # nn_visual_pts # 
                expanded_visual_pts_selected_rotations = fields.batched_index_select(values=tot_link_rotations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                expanded_visual_pts_selected_translaations = fields.batched_index_select(values=tot_link_translations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                
                # print(f"expanded_visual_pts_selected_rotations: {expanded_visual_pts_selected_rotations.size()}, expanded_visual_pts_selected_translaations: {expanded_visual_pts_selected_translaations.size()}, ref_expanded_visual_pts: {ref_expanded_visual_pts.size()}")
                # cur_transformed_expanded_visual_pts : nn_visual_pts x 3 #
                cur_transformed_expanded_visual_pts = torch.matmul(
                    expanded_visual_pts_selected_rotations, ref_expanded_visual_pts.unsqueeze(-1)
                ).squeeze(-1) + expanded_visual_pts_selected_translaations
                
                cur_transformed_expanded_visual_pts = (cur_transformed_expanded_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_transformed_expanded_visual_pts = cur_transformed_expanded_visual_pts * 2. -1.
                cur_transformed_expanded_visual_pts = cur_transformed_expanded_visual_pts * self.mult_const_after_cent # mult_const #
                
                ### transform by the glboal transformation and the translation ###
                cur_transformed_expanded_visual_pts = torch.matmul(cur_rot, cur_transformed_expanded_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                
                if self.optimize_expanded_pts:
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                else:
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_transformed_expanded_visual_pts
    
                # self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                
                
                # delta_offset_reg_motion, ragged_dist, dist_transformed_expanded_visual_pts_to_ori_visual_pts #
                dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.sum(
                    (cur_transformed_expanded_visual_pts - cur_mano_visual_pts) ** 2, dim=-1
                )
                dist_transformed_expanded_visual_pts_to_ori_visual_pts = dist_transformed_expanded_visual_pts_to_ori_visual_pts.mean()
                
                        
                
                # timestep_to_tot_rot[cur_ts] = cur_mano_rot.detach()
                # timestep_to_tot_trans[cur_ts] = cur_mano_trans.detach()
                
                # 
                # cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                # if i_iter == 0 and cur_ts < 11:
                if  cur_ts < 11:
                # if cur_ts == 0 and correspondence_pts_idxes is None:
                    dist_robot_pts_to_mano_pts = torch.sum(
                        (cur_visual_pts[sampled_verts_idxes].unsqueeze(1) - cur_mano_visual_pts.unsqueeze(0)) ** 2, dim=-1
                    )
                    minn_dist_robot_pts_to_mano_pts, correspondence_pts_idxes = torch.min(dist_robot_pts_to_mano_pts, dim=-1)
                    minn_dist_robot_pts_to_mano_pts = torch.sqrt(minn_dist_robot_pts_to_mano_pts)
                    # dist_smaller_than_thres = minn_dist_robot_pts_to_mano_pts < 0.01
                    dist_smaller_than_thres = minn_dist_robot_pts_to_mano_pts < 0.005
                    
                    corr_correspondence_pts = cur_mano_visual_pts[correspondence_pts_idxes]
                    dist_corr_correspondence_pts_to_mano_visual_pts = torch.sum(
                        (corr_correspondence_pts.unsqueeze(1) - cur_mano_visual_pts.unsqueeze(0)) ** 2, dim=-1
                    )
                    dist_corr_correspondence_pts_to_mano_visual_pts = torch.sqrt(dist_corr_correspondence_pts_to_mano_visual_pts)
                    minn_dist_to_corr_pts, _ = torch.min(dist_corr_correspondence_pts_to_mano_visual_pts, dim=0)
                    anchored_mano_visual_pts = minn_dist_to_corr_pts < 0.05
                    
                    # if 'model.finger_cd_loss' # distance related quantities -> object offsets #
                    # 
                    # the anchored mano visual pts # # cd loss 
                    # if self.finger_cd_loss_coef > 0.:
                    #     correspondence_pts_idxes = anchored_mano_visual_pts
                
                # timestep_to_corr_mano_pts # 
                # timestep_to_mano_active_mesh # 
                # cur_mano_visual_pts = cur_mano_visual_pts[correspondence_pts_idxes
                corr_correspondence_pts = cur_mano_visual_pts[correspondence_pts_idxes]
                
                ####### CD loss using the anchored mano visial pts ########
                # if self.finger_cd_loss_coef > 0.:
                # self.timestep_to_corr_mano_pts[cur_ts] = corr_correspondence_pts.detach()
                self.timestep_to_corr_mano_pts[cur_ts] = cur_mano_visual_pts[anchored_mano_visual_pts].detach()
                # mano visual pts and the dist #
                sampled_cur_visual_pts = cur_visual_pts[sampled_verts_idxes]
                cd_robo_pts_to_corr_mano_pts = torch.sum(
                    (sampled_cur_visual_pts.unsqueeze(1) - cur_mano_visual_pts[anchored_mano_visual_pts].unsqueeze(0)) ** 2, dim=-1
                )
                # cd_robo_pts_to_corr_mano_pts = torch.sqrt(cd_robo_pts_to_corr_mano_pts)
                cd_robo_to_mano, _ = torch.min(cd_robo_pts_to_corr_mano_pts, dim=-1)
                cd_mano_to_robo, _ = torch.min(cd_robo_pts_to_corr_mano_pts, dim=0)
                # finger_cd_loss_coef, finger_tracking_loss_coef, tracking_loss_coef, penetrating_depth_penalty_coef # 
                # diff_robo_to_corr_mano_pts = 0.5 * (cd_robo_to_mano.mean() + cd_mano_to_robo.mean())
                diff_robo_to_corr_mano_pts = cd_mano_to_robo.mean()
                ####### CD loss using the anchored mano visial pts ########
                

                ####### Tracking loss using the mano visual pts in correspondence ########
                ### the distance from the robot pts to the mano pts #
                ### how to parameterize the motion field ? ###
                self.timestep_to_corr_mano_pts[cur_ts] = corr_correspondence_pts.detach()
                diff_robo_to_corr_mano_pts_finger_tracking = torch.sum(
                    (corr_correspondence_pts - cur_visual_pts[sampled_verts_idxes]) ** 2, dim=-1
                )
                diff_robo_to_corr_mano_pts_finger_tracking = diff_robo_to_corr_mano_pts_finger_tracking[dist_smaller_than_thres]
                diff_robo_to_corr_mano_pts_finger_tracking = diff_robo_to_corr_mano_pts_finger_tracking.mean()
                ####### Tracking loss using the mano visual pts in correspondence ########
                
                self.timestep_to_corr_mano_pts[cur_ts] = cur_transformed_expanded_visual_pts.detach()
                
                
                
                
                # and also the free motion network #
                ##### transform to the active mesh visual pts #####
                # the cur_ts + 1 state in the robot agent is corresponding to the cur_ts state in the simulation with manipulation #
                # cur_visual_pts_mano_pts = cur_visual_pts[self.mano_to_dyn_corr_pts_idxes]
                
                # self.timestep_to_active_mesh[cur_ts] = cur_visual_pts #
                
                if self.optimize_with_intermediates:
                    # timestep_to_passive_mesh_normals; if self.bending_net_type == 'active_force_field_v11'
                    if self.bending_net_type in[ 'active_force_field_v11', 'active_force_field_v12', 'active_force_field_v13', 'active_force_field_v14']:
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=cur_mano_visual_pts)
                        
                        ### using mano pts to manipulate the object ###
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=correspondence_pts_idxes, reference_mano_pts=cur_mano_visual_pts)
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None)
                        
                        # optimize expanded pts # 
                        if self.optimize_expanded_pts or self.optimize_expanded_ragged_pts:
                            self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None)
                        else:  # # op
                            ### using robot sampled points ###
                            self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None, fix_obj=False)
                        
                        # #### # # #### # #### #
                        # #### # # #### # #### #
                        # if self.optimize_expanded_pts: # optimized expanded pts #
                        #     # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=correspondence_pts_idxes, reference_mano_pts=None)
                        #     # anchored_mano_visual_pts
                        #     self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None)
                        # else:
                        #     ### using expanded points ###
                        #     # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None)
                            
                        #     ### using robot sampled points ### # using # # using # # 
                        #     self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None, fix_obj=False if i_iter == 0 else True)
                            
                    elif self.bending_net_type in ['active_force_field_v13_lageu']:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes)
                    else:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, friction_forces=self.mano_expanded_actuator_friction_forces)
                # 
                
                
                if self.train_with_forces_to_active: #  train with forces to active ##
                    # penetration_forces #
                    if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                        net_penetrating_forces = self.other_bending_network.penetrating_forces
                        net_penetrating_points = self.other_bending_network.penetrating_points
                        # cur_penetrating_forces = []
                        # for i_pts in range(net_penetrating_points.size(0)):
                        #     cur_penetration_point = net_penetrating_points[i_pts]
                        #     cur_penetration_point_forces = net_penetrating_forces[i_pts]
                        #     cur_penetrating_forces.append((cur_penetration_point, cur_penetration_point_forces))
                        # penetration_forces = cur_penetrating_forces
                        
                        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                        self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                        self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                        
                        
                         ### transform the visual pts ###
                        # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        # cur_visual_pts = cur_visual_pts * 2. - 1.
                        # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                        
                        sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[sampled_verts_idxes][self.other_bending_network.penetrating_indicator]
                        # sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[sampled_verts_idxes]
                        
                        
                        net_penetrating_forces = torch.matmul(
                            cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                        net_penetrating_forces = net_penetrating_forces / 2
                        net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                        
                        net_penetrating_points = torch.matmul(
                            cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                        net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                        net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                        
                        
                        
                        
                        # print(f"nn_penetrating points: {net_penetrating_points.size(0)}, {net_penetrating_forces.size()}")
                        #### penetration forces ####
                        penetration_forces = {
                            'penetration_forces': net_penetrating_forces, 'penetration_forces_points': net_penetrating_points
                        }
                        # cur_rot = cur_glb_rot
                        # cur_trans = cur_glb_trans
                    else:
                        penetration_forces = None
                        
                        
                        # timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                        # timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                        
                        
                        # ### transform by the glboal transformation and the translation ###
                        # cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                    
                
                
                ''' Computing losses ''' # 
                # cur_rhand_verts = self.rhand_verts[cur_ts]
                # dist_active_pts_rhand_verts = torch.sum(
                #     (cur_visual_pts.unsqueeze(1) - cur_rhand_verts.unsqueeze(0)) ** 2, dim=-1 ### nn_sampled_pts x nn_ref_pts ###
                # ) # each oen should amtch one single point
                # dist_rhand_verts_to_active_pts, _ = torch.min(dist_active_pts_rhand_verts, dim=0)
                # dist_rhand_verts_to_active_pts = torch.mean(dist_rhand_verts_to_active_pts)
                # dist_active_pts_rhand_verts, _ = torch.min(dist_active_pts_rhand_verts, dim=-1)
                # dist_active_pts_rhand_verts = torch.mean(dist_active_pts_rhand_verts)
                
                # # dist active pts rhand verts # # # rhand_verts #
                # dist_active_pts_rhand_verts = (dist_active_pts_rhand_verts + dist_rhand_verts_to_active_pts) / 2.
                
                ### optimize with intermediates ###
                if self.optimize_with_intermediates:
                    tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1) # 
                else:
                    tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    # diff_contact_d = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    # diff_grid_pts_weight = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                # tot_diff_grid_pts_forces.append(diff_contact_d.detach().item()) # #
                # tot_diff_grid_pts_weight.append(diff_grid_pts_weight.detach().item()) # #
                # tot_diff_grid_pts_weight #
                
                # tot penetrating 
                tot_penetrating_depth_penalty.append(self.other_bending_network.penetrating_depth_penalty.item())
                
                loss_finger_tracking = diff_robo_to_corr_mano_pts * self.finger_cd_loss_coef + diff_robo_to_corr_mano_pts_finger_tracking * self.finger_tracking_loss_coef
                
                tot_loss_figner_tracking.append(loss_finger_tracking.detach().item())
                
                tot_ragged_dist.append(ragged_dist.detach().item())
                tot_delta_offset_reg_motion.append(delta_offset_reg_motion.detach().item())
                
                tot_dist_mano_visual_ori_to_cur.append(dist_transformed_expanded_visual_pts_to_ori_visual_pts.detach().item())
                
                tot_diff_cur_states_to_ref_states.append(diff_cur_states_to_ref_states.detach().item())
                
                
                # robot_init_states_ori, robot_glb_rotation_ori, robot_delta_states_ori, robot_actions_ori, robot_glb_trans_ori #
                reg_init_states = torch.sum((self.robot_init_states_ori - self.robot_init_states.weight) ** 2, dim=-1).mean()
                reg_glb_rotation = torch.sum((self.robot_glb_rotation_ori - self.robot_glb_rotation.weight) ** 2, dim=-1).mean()
                reg_delta_states = torch.sum((self.robot_delta_states_ori - self.robot_delta_states.weight) ** 2, dim=-1).mean()
                # reg_actions = torch.sum((self.robot_actions_ori - self.robot_actions.weight) ** 2, dim=-1).mean()
                reg_glb_trans = torch.sum((self.robot_glb_trans_ori - self.robot_glb_trans.weight) ** 2, dim=-1).mean()
                reg_loss = reg_init_states + reg_glb_rotation + reg_delta_states + reg_glb_trans
                
                tot_reg_loss.append(reg_loss.detach().item())
                
                loss = tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef  + ragged_dist * self.ragged_dist_coef  + dist_transformed_expanded_visual_pts_to_ori_visual_pts * 1. + reg_loss + diff_cur_states_to_ref_states * self.loss_weight_diff_states
                
                # loss = diff_cur_states_to_ref_states
                
                # if not self.optimize_expanded_pts:
                #     # loss = tracking_loss + loss_finger_tracking * 0.1 # + self.other_bending_network.penetrating_depth_penalty * 0.000
                #     # loss = loss_finger_tracking + tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty  * self.penetrating_depth_penalty_coef
                #     loss = tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef  + ragged_dist * self.ragged_dist_coef  + dist_transformed_expanded_visual_pts_to_ori_visual_pts * 1. # + delta_offset_reg_motion
                # else:
                #     # delta_offset_reg_motion, ragged_dist, dist_transformed_expanded_visual_pts_to_ori_visual_pts #
                #     # loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty # + loss_finger_tracking
                #     loss = tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef + delta_offset_reg_motion + ragged_dist * self.ragged_dist_coef  + dist_transformed_expanded_visual_pts_to_ori_visual_pts
                
                # # opitmize with intermediates #
                if self.optimize_with_intermediates:
                    if self.bending_net_type == 'active_force_field_v11':
                        cur_penalty_dot_forces_normals = self.other_bending_network.penalty_dot_forces_normals
                        cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                    elif self.bending_net_type == 'active_force_field_v13':
                        if self.no_friction_constraint:
                            cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                        else:
                            cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                    else:
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                        cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                else:
                    cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean().cuda()
                    cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean().cuda()
                # 
                loss = loss + cur_penalty_dot_forces_normals + cur_penalty_friction_constraint
                # 
                self.optimizer.zero_grad()
                # loss.backward()
                loss.backward(retain_graph=True)
                self.optimizer.step()
                
                tot_losses.append(loss.detach().item())
                tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                self.update_learning_rate() ## update learning rate ##
                
                
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            tot_penalty_dot_forces_normals = sum(tot_penalty_dot_forces_normals) / float(len(tot_penalty_dot_forces_normals))
            tot_penalty_friction_constraint = sum(tot_penalty_friction_constraint) / float(len(tot_penalty_friction_constraint))
            # tot_dist_act_to_rhand = sum(tot_dist_act_to_rhand) / float(len(tot_dist_act_to_rhand)) 
            # tot_dist_to_contact = sum(tot_dist_to_contact) / float(len(tot_dist_to_contact)) 
            # tot_diff_grid_pts_forces = sum(tot_diff_grid_pts_forces) / float(len(tot_diff_grid_pts_forces)) # 
            # tot_diff_grid_pts_weight = sum(tot_diff_grid_pts_weight) / float(len(tot_diff_grid_pts_weight)) # 
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            tot_penetrating_depth_penalty = sum(tot_penetrating_depth_penalty) / float(len(tot_penetrating_depth_penalty))
            tot_ragged_dist = sum(tot_ragged_dist) / float(len(tot_ragged_dist))
            tot_delta_offset_reg_motion = sum(tot_delta_offset_reg_motion) / float(len(tot_delta_offset_reg_motion))
            tot_dist_mano_visual_ori_to_cur = sum(tot_dist_mano_visual_ori_to_cur) / float(len(tot_dist_mano_visual_ori_to_cur))
            tot_reg_loss = sum(tot_reg_loss) / float(len(tot_reg_loss))
            tot_diff_cur_states_to_ref_states = sum(tot_diff_cur_states_to_ref_states) / float(len(tot_diff_cur_states_to_ref_states))
            if len(tot_loss_figner_tracking) > 0:
                tot_loss_figner_tracking = sum(tot_loss_figner_tracking) / float(len(tot_loss_figner_tracking))
            else:
                tot_loss_figner_tracking = 0.
            # tot_loss_corr_tracking = sum(tot_loss_corr_tracking) / float(len(tot_loss_corr_tracking))
            tot_loss_corr_tracking = 0.
            #
            # cur_sv_penetration_points_fn = os.path.join(self.base_exp_dir, "meshes", f"penetration_points_{i_iter}.npy")
            # # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            # cur_sv_penetration_points = {
            #     'timestep_to_raw_active_meshes': self.timestep_to_raw_active_meshes,
            #     'timestep_to_penetration_points': self.timestep_to_penetration_points,
            #     'timestep_to_penetration_points_forces': self.timestep_to_penetration_points_forces
            # }
            # np.save(cur_sv_penetration_points_fn, cur_sv_penetration_points)
            
            
            
            
            if i_iter % self.report_freq == 0:
                # print(self.base_exp_dir) # ### tot reg loss ###
                # print('iter:{:8>d} loss = {} loss_corr_tracking = {} tracking_loss = {} tot_loss_figner_tracking = {} tot_ragged_dist = {} tot_delta_offset_reg_motion = {} tot_dist_mano_visual_ori_to_cur = {} penetration_depth_penalty = {} penalty_dir = {} penalty_friction = {} reg_loss = {} diff_states = {} lr={}'.format(self.iter_step, tot_losses, tot_loss_corr_tracking,  tot_tracking_loss, tot_loss_figner_tracking, tot_ragged_dist, tot_delta_offset_reg_motion, tot_dist_mano_visual_ori_to_cur, tot_penetrating_depth_penalty, tot_penalty_dot_forces_normals, tot_penalty_friction_constraint, tot_reg_loss, tot_diff_cur_states_to_ref_states, self.optimizer.param_groups[0]['lr']))
                
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} loss_corr_tracking = {} tracking_loss = {} tot_loss_figner_tracking = {} tot_ragged_dist = {} tot_delta_offset_reg_motion = {} tot_dist_mano_visual_ori_to_cur = {} penetration_depth_penalty = {} penalty_dir = {} penalty_friction = {} reg_loss = {} diff_states = {} lr={}'.format(self.iter_step, tot_losses, tot_loss_corr_tracking,  tot_tracking_loss, tot_loss_figner_tracking, tot_ragged_dist, tot_delta_offset_reg_motion, tot_dist_mano_visual_ori_to_cur, tot_penetrating_depth_penalty, tot_penalty_dot_forces_normals, tot_penalty_friction_constraint, tot_reg_loss, tot_diff_cur_states_to_ref_states, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                

            if i_iter % self.val_mesh_freq == 0:
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                # self.validate_mesh_expanded_pts() 
            # self.active_robot.clear_grads() 
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
    
    
    def train_real_robot_actions_from_mano_model_rules_v4(self, ):
        
        ### the real robot actions from mano model rules ###
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path']
        robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        # morph robot agent #
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano)
        self.mano_agent = mano_agent
        ''' Load the mano hand '''
        
        print(f"Start expanding the current visual pts...")
        expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        
        self.expanded_visual_pts_nn = expanded_visual_pts.size(0)
        ## expanded_visual_pts of the expanded visual pts ##
        expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=778 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_robot_actuator_friction_forces.parameters())
        
        
        self.mano_expanded_actuator_delta_offset = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_delta_offset.weight)
        # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
        
        
        self.mano_expanded_actuator_delta_offset_nex = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_delta_offset_nex.weight)
        # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
        
        
        # mano_expanded_actuator_friction_forces, mano_expanded_actuator_delta_offset # 
        self.mano_expanded_actuator_friction_forces = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        # params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        
        # 
        # free_deformation_time_latent, free_deformation_network
        self.free_deformation_time_latent = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        )
        params_to_train += list(self.free_deformation_time_latent.parameters())
        
        # free deformations; time latnets #
        self.deformation_input_dim = 3 + self.bending_latent_size
        self.deformation_output_dim = 3
        ### free deformation network ### # free deformation # 
        self.free_deformation_network = self.construct_field_network(self.deformation_input_dim, self.bending_latent_size,  self.deformation_output_dim)
        params_to_train += list(self.free_deformation_network.parameters())
        
        
        
        # ### laod optimized init actions ####
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # cur_optimized_init_actions = 
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                self.mano_robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            self.mano_robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
            if 'expanded_actuator_friction_forces' in optimized_init_actions_ckpt:
                try:
                    self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_friction_forces'])
                except:
                    pass
            self.mano_expanded_actuator_delta_offset.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_delta_offset'])
        # ### load init transformations ckpts ### #
        
        # robot actions #
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        ## robot actuator friction #    
        # robot actuator friction forces #
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        params_to_train = []
        ''' Only set the robot friction forces optimizable '''
        if not self.optimize_rules:
            params_to_train += list(self.robot_actions.parameters())
            params_to_train += list(self.robot_delta_states.parameters())
            # params_to_train += list(self.robot_init_states.parameters())
            # params_to_train += list(self.robot_glb_rotation.parameters())
            # params_to_train += list(self.robot_glb_trans.parameters())
            
        ''' Only set the robot friction forces optimizable '''
        params_to_train += list(self.mano_expanded_actuator_delta_offset_nex.parameters())
        params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        
        params_to_train += list(self.robot_actuator_friction_forces.parameters())
        if self.optimize_rules:
            params_to_train += list(self.other_bending_network.parameters())
        
        
        ### motivate the robot via actions ###
        ### not set the states -> set the actions ###
        ### loss 1 -> differences between the current states and the target states ###
        ### loss 2 -> tracking loss ###
        ### the contact establishment and the cancellation ###
        
        # get states and use the differences between the current 
        # self.robot_glb_trans = nn.Embedding( # 
        #     num_embeddings=num_steps, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())
        
        
        # # sketche out the actions # #
        # ### laod optimized init actions ####
        # if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
        #     cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
        #     # cur_optimized_init_actions = 
        #     optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
        #     self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
        #     self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
        #     if 'robot_delta_states' in optimized_init_actions_ckpt:
        #         self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
        #     self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions']) # 
        #     self.robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
        #     self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
        #     ### load init transformations ckpts ###

        ### affine ###
        ## self.maxx_robo_pts = 25. ## 
        # self.maxx_robo_pts = 25. ## 
        # self.minn_robo_pts = -15. ## 
        # self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts 
        # self.mult_const_after_cent = 0.5437551664260203 
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_transformations_fn = self.conf['model.load_optimized_init_transformations']
            ##### Load optimized initial transformations #####
            optimized_init_transformations_ckpt = torch.load(cur_optimized_init_transformations_fn, map_location=self.device, )
            self.robot_init_states.load_state_dict(optimized_init_transformations_ckpt['robot_init_states'])
            self.robot_glb_rotation.load_state_dict(optimized_init_transformations_ckpt['robot_glb_rotation'])
            # if 'robot_delta_states' in optimized_init_transformations_ckpt:
            #     self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            self.robot_actions.load_state_dict(optimized_init_transformations_ckpt['robot_actions'])
            self.robot_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['robot_actuator_friction_forces'])
            self.robot_glb_trans.load_state_dict(optimized_init_transformations_ckpt['robot_glb_trans'])
            
            if not self.load_only_glb:
                self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            
            # laod friction forces # friction forces # friction forces # 
            # if 'model.load_friction_forces' in self.conf and len(self.conf['model.load_friction_forces']) > 0:
            #     print(f"Loading friction forces checkpoint from {self.conf['model.load_friction_forces']}")
            #     friction_forces_ckpt = self.conf['model.load_friction_forces']
            #     self.robot_actuator_friction_forces.load_state_dict(torch.load(friction_forces_ckpt, map_location=self.device)['robot_actuator_friction_forces'])
            
            # if 'expanded_actuator_friction_forces' in optimized_init_transformations_ckpt:
            #     try:
            #         self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['expanded_actuator_friction_forces'])
            #     except: # load friction force # 
            #         pass
            
        if 'model.load_optimized_expanded' in self.conf and len(self.conf['model.load_optimized_expanded']) > 0:
            load_optimized_expanded_ckpt = self.conf['model.load_optimized_expanded']
            load_optimized_expanded_ckpt = torch.load(load_optimized_expanded_ckpt, map_location=self.device)
            self.mano_expanded_actuator_delta_offset.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset'])
            self.mano_expanded_actuator_friction_forces.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_friction_forces'])
            if 'mano_expanded_actuator_delta_offset_nex' in load_optimized_expanded_ckpt:
                print(f"Loading expanded actuator delta offset nex from the saved checkpoint...")
                self.mano_expanded_actuator_delta_offset_nex.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset_nex'])
        
        # # 
        # init_states glb_rotation, delta_states, robot_actions_ori, 
        # make the current states and others near to the original ones #
        # robot_init_states_ori, robot_glb_rotation_ori, robot_delta_states_ori, robot_actions_ori, robot_glb_trans_ori #
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        
        
        # train the robot or train the expanded points ## expanded points ##
        # self.mano_fingers = [745, 279, 320, 444, 555, 672, 234, 121]
        # self.robot_fingers = [521, 624, 846, 973, 606, 459, 383, 265]
        
        # optimize 
        # optimize with intermediates # 
        # self.mano_fingers = torch.tensor(self.mano_fingers, dtype=torch.long).cuda()
        # self.robot_fingers = torch.tensor(self.robot_fingers, dtype=torch.long).cuda()
        
        self.nn_ts = self.nn_timesteps - 1
        # self.optimize_with_intermediates = False
        
        # search for the nearest point so that it can manipulate the object #
        ## TODO: co-optimize the bending network weights? ## # optimize active object is true ##
        # if self.optimize_with_intermediates and (not self.optimize_active_object): # optimize with intermediates #  # search
        #     # if not self.optimize_active_object: #
        #     # free_deformation_time_latent, free_deformation_network #
        #     params_to_train = []
        #     params_to_train += list(self.other_bending_network.parameters())
        #     params_to_train += list(self.robot_actuator_friction_forces.parameters())
        #     # params_to_train += list(self.expanded_actuator_friction_forces.parameters()) # expanded actuator 
        #     # params_to_train += list(self.free_deformation_time_latent.parameters())
        #     # params_to_train += list(self.free_deformation_network.parameters()) # 

        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        # init_rot = R.random().as_quat()
        # init_rot = torch.from_numpy(init_rot).float().cuda()
        # self.robot_glb_rotation.weight.data[0, :] = init_rot[:]
        
        
        #### get motions --- the original motion of the expanded visual points ####
        #### rag expanded points to the target object ####
        #### use the target object motion to explain the expanded object motion ####
        
        ### Constraint set ###
        self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        self.timestep_to_active_mesh = {}
        # 
        # init_visual_pts, link_name_to_transformations_and_transformed_pts = get_init_state_visual_pts(ret_link_name_to_tansformations=True)
        # set_body_expanded_visual_pts
        # expanded_visual_pts = compute_expanded_visual_pts_transformation_via_current_state()
        
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts # ## 
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        with torch.no_grad():
            for cur_ts in range(self.nn_ts):
                cur_mano_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_mano_glb_rot = cur_mano_glb_rot / torch.clamp(torch.norm(cur_mano_glb_rot, dim=-1, p=2), min=1e-7)
                cur_mano_glb_rot = dyn_model_act.quaternion_to_matrix(cur_mano_glb_rot) # mano glboal rotations #
                cur_mano_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    mano_links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.mano_agent.set_init_states_target_value(mano_links_init_states)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                else:
                    actions = {}
                    actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    mano_actions_link_actions = self.mano_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    actions['link_actions'] = mano_actions_link_actions
                    self.mano_agent.active_robot.calculate_inertia()
                    self.mano_agent.active_robot.set_actions_and_update_states(mano_actions_link_actions, cur_ts - 1, 0.2)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                cur_mano_visual_pts = self.mano_agent.active_robot.get_transformed_visual_pts() # transformed visual pts # 
                cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                cur_mano_rot = cur_mano_glb_rot
                cur_mano_trans = cur_mano_glb_trans
                ### mano visual pts ###
                cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                
                
                ''' augment with the delta motion '''
                cur_delta_offset_indexes = torch.arange(self.expanded_visual_pts_nn, dtype=torch.long).cuda() + cur_ts * self.expanded_visual_pts_nn
                cur_delta_offset = self.mano_expanded_actuator_delta_offset(cur_delta_offset_indexes)
                cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset # delta offset #
                
                cur_delta_offset_nex = self.mano_expanded_actuator_delta_offset_nex(cur_delta_offset_indexes)
                cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset_nex
                
                # delta_offset_reg_motion = torch.sum(cur_delta_offset ** 2, dim=-1).mean() ### regularize the delta motion #### save the expanded visual pts ####
                self.timestep_to_expanded_visual_pts[cur_ts] = cur_mano_visual_pts.detach().cpu().numpy()
                
                
                # if cur_ts < 11:
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                else:
                    if self.conf['model.train_states']:
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    else:
                        # should be able to set the states only viat he state values #
                        # actions['delta_glb_rot'] = cur_glb_rot #
                        # actions['delta_glb_trans'] = cur_glb_trans #
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        
                        actions['link_actions'] = actions_link_actions # set actions and the s
                        # set and update 
                        self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        
                    if cur_ts == 10:
                        cur_visual_pts, link_name_to_transformations_and_transformed_pts = self.robot_agent.get_init_state_visual_pts(ret_link_name_to_tansformations=True)
                    else:
                        cur_visual_pts = self.robot_agent.get_init_state_visual_pts()

                ### transform the visual pts ###
                cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_visual_pts = cur_visual_pts * 2. -1.
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                # timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                # timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                self.timestep_to_active_mesh[cur_ts] = cur_visual_pts.detach()
                
                if cur_ts == 10:
                    link_name_to_ragged_ref_pts = {}
                    ### get the ragged points ###
                    link_name_to_link_idx = {}
                    cur_link_idx = 0
                    for link_name in link_name_to_transformations_and_transformed_pts:
                        link_name_to_link_idx[link_name] = cur_link_idx
                        cur_link_idx += 1
                    link_idx_to_link_name = {v: k for k, v in link_name_to_link_idx.items()}
                    dist_expanded_visual_pts_to_link_pts = []
                    
                    link_name_to_selected_link_idxes = {}
                    
                    link_name_to_tot_transformed_pts = {}
                    for cur_link_idx in range(len(link_name_to_transformations_and_transformed_pts)):
                        cur_link_name = link_idx_to_link_name[cur_link_idx]
                        cur_link_transformed_pts, cur_link_transformations = link_name_to_transformations_and_transformed_pts[cur_link_name]
                        
                        cur_link_transformed_pts = (cur_link_transformed_pts - self.minn_robo_pts) / self.extent_robo_pts
                        cur_link_transformed_pts = cur_link_transformed_pts * 2. -1.
                        cur_link_transformed_pts = cur_link_transformed_pts * self.mult_const_after_cent 
                        
                        
                        
                        cur_link_transformed_pts = torch.matmul(cur_rot, cur_link_transformed_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                        
                        link_name_to_tot_transformed_pts[cur_link_name] = cur_link_transformed_pts.detach().cpu().numpy()
                        
                        
                        permutation_idxes = torch.randperm(cur_link_transformed_pts.size(0)).cuda()[:1000]
                        
                        link_name_to_selected_link_idxes[cur_link_name] = permutation_idxes
                        
                        cur_link_transformed_pts_selected = cur_link_transformed_pts[permutation_idxes]
                        
                        dist_expanded_visual_pts_to_cur_link_pts = torch.sum(
                            (cur_mano_visual_pts.unsqueeze(1) - cur_link_transformed_pts_selected.unsqueeze(0)) ** 2, dim=-1
                        )
                        minn_dist_expanded_visual_pts_to_cur_link_pts, _ = torch.min(dist_expanded_visual_pts_to_cur_link_pts, dim=-1)
                        dist_expanded_visual_pts_to_link_pts.append(minn_dist_expanded_visual_pts_to_cur_link_pts)
                    dist_expanded_visual_pts_to_link_pts = torch.stack(dist_expanded_visual_pts_to_link_pts, dim=1)
                    # # 
                    minn_dist_expanded_visual_pts_to_link_pts, minn_idx_expanded_visual_pts_to_link_pts = torch.min(dist_expanded_visual_pts_to_link_pts, dim=-1) ## minn_idx_expanded_visual_pts_to_link_pts ##
                    tot_link_rotations = []
                    tot_link_translations = []
                    for cur_link_idx in range(len(link_name_to_transformations_and_transformed_pts)):
                        expanded_visual_pts_ragged_to_cur_link_mask = minn_idx_expanded_visual_pts_to_link_pts == cur_link_idx
                        cur_link_name = link_idx_to_link_name[cur_link_idx]
                        _, cur_link_transformations = link_name_to_transformations_and_transformed_pts[cur_link_name]
                        if expanded_visual_pts_ragged_to_cur_link_mask.float().sum() > 0:
                            expanded_visual_pts_ragged_to_cur_link = cur_mano_visual_pts[expanded_visual_pts_ragged_to_cur_link_mask]
                            cur_link_rotations, cur_link_translations = cur_link_transformations
                            # R^T (pts - center) #
                            ref_pts = torch.matmul(cur_link_rotations.transpose(1, 0), (expanded_visual_pts_ragged_to_cur_link - cur_link_translations.unsqueeze(0)).contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous()
                            link_name_to_ragged_ref_pts[cur_link_name] = ref_pts.detach() # .cpu().numpy()
                        tot_link_rotations.append(cur_link_transformations[0])
                        tot_link_translations.append(cur_link_transformations[1])
                    # self.robot_agent.set_body_expanded_visual_pts(link_name_to_ragged_ref_pts)
                    
                    tot_link_rotations = torch.stack(tot_link_rotations, dim=0)
                    tot_link_translations = torch.stack(tot_link_translations, dim=0)
                    
                    
                    ref_expanded_visual_pts = torch.matmul(cur_rot.transpose(1, 0), (cur_mano_visual_pts - cur_trans.unsqueeze(0)).contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() 
                    ref_expanded_visual_pts = ref_expanded_visual_pts / self.mult_const_after_cent
                    ref_expanded_visual_pts = (ref_expanded_visual_pts + 1.) / 2. 
                    ref_expanded_visual_pts = ref_expanded_visual_pts * self.extent_robo_pts + self.minn_robo_pts
                    
                    
                    
                    # # nn_visual_pts # 
                    expanded_visual_pts_selected_rotations = fields.batched_index_select(values=tot_link_rotations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                    expanded_visual_pts_selected_translaations = fields.batched_index_select(values=tot_link_translations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                    
                    ref_expanded_visual_pts = torch.matmul(
                        expanded_visual_pts_selected_rotations.contiguous().transpose(2, 1), (ref_expanded_visual_pts - expanded_visual_pts_selected_translaations).unsqueeze(-1)
                    ).squeeze(-1) ## nn_visual_pts x 3 ##
                    
                    
                    save_visual_pts_link_name_to_visual_pts = {
                        'cur_mano_visual_pts': cur_mano_visual_pts.detach().cpu().numpy(),
                        'link_name_to_tot_transformed_pts' : link_name_to_tot_transformed_pts
                    }
                    np.save(f"save_visual_pts_link_name_to_visual_pts.npy", save_visual_pts_link_name_to_visual_pts)
        
        
        if self.train_actions_with_states:
            for cur_ts in range(self.nn_ts):
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                else:
                    # if self.conf['model.train_states']:
                    cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    states = {}
                    states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    states['link_states'] = cur_state
                    self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)      
            tmp_joint_nm_to_ts_to_states = self.robot_agent.get_timestep_to_states()
            joint_nm_to_ts_to_states = {}
            for cur_joint_nm in tmp_joint_nm_to_ts_to_states:
                cur_joint_ts_to_states = {}
                for cur_ts in tmp_joint_nm_to_ts_to_states[cur_joint_nm]:
                    cur_joint_ts_to_states[cur_ts] = tmp_joint_nm_to_ts_to_states[cur_joint_nm][cur_ts].detach().clone()
                joint_nm_to_ts_to_states[cur_joint_nm] = cur_joint_ts_to_states
    
        # model_path = self.conf['model.sim_model_path'] #
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None) ## robot_agent ##
        # self.robot_agent = robot_agent 
        
        penetration_forces = None
        
        
        
        #### get the expanded ####
        ### get the expanded transformed points at each timestep ###
        #### TODO: rotation optimization and the translation optimization ####
        for i_iter in tqdm(range(100000)): ## actions from model rules ### ## 
            tot_losses = []
            tot_penalty_dot_forces_normals = []
            tot_penalty_friction_constraint = []
            tot_dist_act_to_rhand = []
            tot_dist_to_contact = []
            ## tot_diff_grid_pts_forces, tot_diff_grid_pts_weight ##
            tot_diff_grid_pts_forces = []
            tot_diff_grid_pts_weight = []
            tot_loss_figner_tracking = []
            tot_loss_corr_tracking = []
            tot_tracking_loss = []
            
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # self.timestep_to_
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            correspondence_pts_idxes = None
            
            tot_penetrating_depth_penalty = []
            
            tot_ragged_dist = []
            
            tot_delta_offset_reg_motion = []
            
            tot_dist_mano_visual_ori_to_cur = []
            
            tot_reg_loss = []
            
            tot_diff_cur_states_to_ref_states = []
            
            tot_diff_tangential_forces = []
            
            penetration_forces = None
            sampled_visual_pts_joint_idxes = None
            
            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_force_d = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16]
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            for cur_ts in range(self.nn_ts):
                actions = {}
                
                cur_mano_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_mano_glb_rot = cur_mano_glb_rot / torch.clamp(torch.norm(cur_mano_glb_rot, dim=-1, p=2), min=1e-7)
                cur_mano_glb_rot = dyn_model_act.quaternion_to_matrix(cur_mano_glb_rot) # mano glboal rotations #
                cur_mano_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    mano_links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.mano_agent.set_init_states_target_value(mano_links_init_states)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                else:
                    # if self.conf['model.train_states']:
                    #     cur_state = self.mano_robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    #     self.mano_agent.active_robot.set_delta_state_and_update(cur_state, cur_ts)
                    #     cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                    # else:
                    actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    mano_actions_link_actions = self.mano_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    actions['link_actions'] = mano_actions_link_actions
                    self.mano_agent.active_robot.calculate_inertia()
                    self.mano_agent.active_robot.set_actions_and_update_states(mano_actions_link_actions, cur_ts - 1, 0.2)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                ## mano visual pts ##
                cur_mano_visual_pts = self.mano_agent.active_robot.get_transformed_visual_pts() # transformed visual pts #
                
                
                
                
                if cur_ts > 0:
                    prev_rot = timestep_to_tot_rot[cur_ts - 1]
                    cur_mano_rot = torch.matmul(cur_mano_glb_rot, prev_rot)
                    prev_trans = timestep_to_tot_trans[cur_ts - 1]
                    cur_mano_trans = cur_mano_glb_trans + prev_trans
                else:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                if not self.conf['model.using_delta_glb_trans']:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_mano_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_mano_trans.detach()
                
                cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                if cur_ts > 0:
                    ## sue the original mano articulated motion with delta position offsets ##
                    self.timestep_to_posed_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    
                    cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                    
                    ''' augment with the delta motion '''
                    cur_delta_offset_indexes = torch.arange(self.expanded_visual_pts_nn, dtype=torch.long).cuda() + cur_ts * self.expanded_visual_pts_nn
                    cur_delta_offset = self.mano_expanded_actuator_delta_offset(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset
                    
                    cur_delta_offset_nex = self.mano_expanded_actuator_delta_offset_nex(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset_nex
                    
                    delta_offset_reg_motion = torch.sum(cur_delta_offset_nex ** 2, dim=-1).mean() ### regularize the delta motion ###
                    ''' augment with the delta motion '''
                    
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                else:
                    self.timestep_to_posed_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                    
                    ''' augment with the delta motion '''
                    cur_delta_offset_indexes = torch.arange(self.expanded_visual_pts_nn, dtype=torch.long).cuda() + cur_ts * self.expanded_visual_pts_nn
                    cur_delta_offset = self.mano_expanded_actuator_delta_offset(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset
                    
                    cur_delta_offset_nex = self.mano_expanded_actuator_delta_offset_nex(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset_nex
                    
                    delta_offset_reg_motion = torch.sum(cur_delta_offset_nex ** 2, dim=-1).mean() ### regularize the delta motion ###
                    ''' augment with the delta motion '''
                    
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                
                
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts, link_name_to_link_transformations = self.robot_agent.get_init_state_visual_pts(True)
                else:
                    if self.conf['model.train_states']:
                        if self.add_delta_state_constraints:
                            self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg] = torch.clamp(self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg], max=0.)
                        
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    else:
                        # print(f"Draing via actions")
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        
                        actions['link_actions'] = actions_link_actions
                        joint_name_to_penetration_forces_intermediates = self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1, penetration_forces=penetration_forces, sampled_visual_pts_joint_idxes=sampled_visual_pts_joint_idxes) # upate the state at cur_ts # 
                        # joint_name_to_penetration_forces_intermediates = self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1, penetration_forces=None, sampled_visual_pts_joint_idxes=sampled_visual_pts_joint_idxes) # upate the state at cur_ts # 
                        self.joint_name_to_penetration_forces_intermediates[cur_ts] = joint_name_to_penetration_forces_intermediates
                    cur_visual_pts, link_name_to_link_transformations, visual_pts_joint_idxes = self.robot_agent.get_init_state_visual_pts(True, True)

                if self.train_actions_with_states:
                    nex_ts_joint_nm_to_states = self.robot_agent.get_joint_nm_to_states()
                    diff_cur_states_to_ref_states = []
                    for joint_nm in nex_ts_joint_nm_to_states:
                        cur_states = nex_ts_joint_nm_to_states[joint_nm]
                        # ref_states = joint_nm_to_ts_to_states[joint_nm][cur_ts + 1]
                        ### reference state at the time cur_ts ####
                        ref_states = joint_nm_to_ts_to_states[joint_nm][cur_ts]
                        diff_cur_joint = torch.sum(
                            (cur_states - ref_states.detach()) ** 2
                        )
                        diff_cur_states_to_ref_states.append(diff_cur_joint)
                        # print(f"cur_ts: {cur_ts}, joint_nm: {joint_nm}, cur_states: {cur_states}, ref_states: {ref_states}") #
                    
                    diff_cur_states_to_ref_states = torch.stack(diff_cur_states_to_ref_states, dim=0)
                    diff_cur_states_to_ref_states = diff_cur_states_to_ref_states.mean()
                else: # cur states to ref states #
                    diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                ### transform the visual pts ###
                cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_visual_pts = cur_visual_pts * 2. - 1.
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                self.timestep_to_active_mesh[cur_ts] = cur_visual_pts #
                
                self.timestep_to_raw_active_meshes[cur_ts] = cur_visual_pts.detach().cpu().numpy()
                
                tot_link_rotations = []
                tot_link_translations = []
                for cur_link_idx in range(len(link_name_to_link_transformations)):
                    # expanded_visual_pts_ragged_to_cur_link_mask = minn_idx_expanded_visual_pts_to_link_pts == cur_link_idx
                    cur_link_name = link_idx_to_link_name[cur_link_idx]
                    _, cur_link_transformations = link_name_to_link_transformations[cur_link_name]
                    tot_link_rotations.append(cur_link_transformations[0])
                    tot_link_translations.append(cur_link_transformations[1])
                tot_link_rotations = torch.stack(tot_link_rotations, dim=0)
                tot_link_translations = torch.stack(tot_link_translations, dim=0)
                
                # # empty_cache() # #
                if cur_ts == 10:
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    
                    dist_expanded_visual_pts_to_link_pts = []
                        
                    link_name_to_tot_transformed_pts = {}
                    for cur_link_idx in range(len(link_name_to_link_transformations)):
                        cur_link_name = link_idx_to_link_name[cur_link_idx]
                        cur_link_transformed_pts, cur_link_transformations = link_name_to_link_transformations[cur_link_name]
                        
                        cur_link_transformed_pts = (cur_link_transformed_pts - self.minn_robo_pts) / self.extent_robo_pts
                        cur_link_transformed_pts = cur_link_transformed_pts * 2. -1.
                        cur_link_transformed_pts = cur_link_transformed_pts * self.mult_const_after_cent 
                        
                        cur_link_transformed_pts = torch.matmul(cur_rot, cur_link_transformed_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                        
                        link_name_to_tot_transformed_pts[cur_link_name] = cur_link_transformed_pts.detach().cpu().numpy()
                        
                        
                        permutation_idxes = link_name_to_selected_link_idxes[cur_link_name]
                        cur_link_transformed_pts_selected = cur_link_transformed_pts[permutation_idxes].detach()
                        
                        dist_expanded_visual_pts_to_cur_link_pts = torch.sum(
                            (cur_mano_visual_pts.unsqueeze(1) - cur_link_transformed_pts_selected.unsqueeze(0)) ** 2, dim=-1
                        )
                        minn_dist_expanded_visual_pts_to_cur_link_pts, _ = torch.min(dist_expanded_visual_pts_to_cur_link_pts, dim=-1)
                        dist_expanded_visual_pts_to_link_pts.append(minn_dist_expanded_visual_pts_to_cur_link_pts)
                    dist_expanded_visual_pts_to_link_pts = torch.stack(dist_expanded_visual_pts_to_link_pts, dim=1)
                    # # 
                    minn_dist_expanded_visual_pts_to_link_pts, minn_idx_expanded_visual_pts_to_link_pts = torch.min(dist_expanded_visual_pts_to_link_pts, dim=-1) ## 
                    
                    
                    ref_expanded_visual_pts = torch.matmul(cur_rot.transpose(1, 0), (cur_mano_visual_pts - cur_trans.unsqueeze(0)).contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() 
                    ref_expanded_visual_pts = ref_expanded_visual_pts / self.mult_const_after_cent
                    ref_expanded_visual_pts = (ref_expanded_visual_pts + 1.) / 2. 
                    ref_expanded_visual_pts = ref_expanded_visual_pts * self.extent_robo_pts + self.minn_robo_pts
                    
                    expanded_visual_pts_selected_rotations = fields.batched_index_select(values=tot_link_rotations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                    expanded_visual_pts_selected_translaations = fields.batched_index_select(values=tot_link_translations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                    
                    ref_expanded_visual_pts = torch.matmul(
                        expanded_visual_pts_selected_rotations.contiguous().transpose(2, 1), (ref_expanded_visual_pts - expanded_visual_pts_selected_translaations).unsqueeze(-1)
                    ).squeeze(-1).detach() ## nn_visual_pts x 3 ##
                    
                    ragged_dist = torch.mean(minn_dist_expanded_visual_pts_to_link_pts)
                else:
                    ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                #     ragged_dist = torch.mean(minn_dist_expanded_visual_pts_to_link_pts)
                # else:
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean() # # 

                # nn_visual_pts #
                expanded_visual_pts_selected_rotations = fields.batched_index_select(values=tot_link_rotations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                expanded_visual_pts_selected_translaations = fields.batched_index_select(values=tot_link_translations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                
                # print(f"expanded_visual_pts_selected_rotations: {expanded_visual_pts_selected_rotations.size()}, expanded_visual_pts_selected_translaations: {expanded_visual_pts_selected_translaations.size()}, ref_expanded_visual_pts: {ref_expanded_visual_pts.size()}")
                # cur_transformed_expanded_visual_pts : nn_visual_pts x 3 #
                cur_transformed_expanded_visual_pts = torch.matmul(
                    expanded_visual_pts_selected_rotations, ref_expanded_visual_pts.unsqueeze(-1)
                ).squeeze(-1) + expanded_visual_pts_selected_translaations
                
                cur_transformed_expanded_visual_pts = (cur_transformed_expanded_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_transformed_expanded_visual_pts = cur_transformed_expanded_visual_pts * 2. -1.
                cur_transformed_expanded_visual_pts = cur_transformed_expanded_visual_pts * self.mult_const_after_cent # mult_const #
                
                ### transform by the glboal transformation and the translation ###
                cur_transformed_expanded_visual_pts = torch.matmul(cur_rot, cur_transformed_expanded_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                
                if self.optimize_expanded_pts:
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                else:
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_transformed_expanded_visual_pts
    
                # self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                
                # for a
                # delta_offset_reg_motion, ragged_dist, dist_transformed_expanded_visual_pts_to_ori_visual_pts #
                dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.sum(
                    (cur_transformed_expanded_visual_pts - cur_mano_visual_pts) ** 2, dim=-1
                )
                dist_transformed_expanded_visual_pts_to_ori_visual_pts = dist_transformed_expanded_visual_pts_to_ori_visual_pts.mean()
                
                        
                
                # timestep_to_tot_rot[cur_ts] = cur_mano_rot.detach()
                # timestep_to_tot_trans[cur_ts] = cur_mano_trans.detach()
                # #### tiemstep to tot trans #### # 
                # cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                # if i_iter == 0 and cur_ts < 11:
                if  cur_ts < 11:
                # if cur_ts == 0 and correspondence_pts_idxes is None:
                    dist_robot_pts_to_mano_pts = torch.sum(
                        (cur_visual_pts[sampled_verts_idxes].unsqueeze(1) - cur_mano_visual_pts.unsqueeze(0)) ** 2, dim=-1
                    )
                    minn_dist_robot_pts_to_mano_pts, correspondence_pts_idxes = torch.min(dist_robot_pts_to_mano_pts, dim=-1)
                    minn_dist_robot_pts_to_mano_pts = torch.sqrt(minn_dist_robot_pts_to_mano_pts)
                    # dist_smaller_than_thres = minn_dist_robot_pts_to_mano_pts < 0.01
                    dist_smaller_than_thres = minn_dist_robot_pts_to_mano_pts < 0.005
                    
                    corr_correspondence_pts = cur_mano_visual_pts[correspondence_pts_idxes]
                    dist_corr_correspondence_pts_to_mano_visual_pts = torch.sum(
                        (corr_correspondence_pts.unsqueeze(1) - cur_mano_visual_pts.unsqueeze(0)) ** 2, dim=-1
                    )
                    dist_corr_correspondence_pts_to_mano_visual_pts = torch.sqrt(dist_corr_correspondence_pts_to_mano_visual_pts)
                    minn_dist_to_corr_pts, _ = torch.min(dist_corr_correspondence_pts_to_mano_visual_pts, dim=0)
                    anchored_mano_visual_pts = minn_dist_to_corr_pts < 0.05
                    
                    # if 'model.finger_cd_loss' # distance related quantities -> object offsets #
                    # 
                    # the anchored mano visual pts # # cd loss 
                    # if self.finger_cd_loss_coef > 0.:
                    #     correspondence_pts_idxes = anchored_mano_visual_pts
                
                # timestep_to_corr_mano_pts # 
                # timestep_to_mano_active_mesh # 
                # cur_mano_visual_pts = cur_mano_visual_pts[correspondence_pts_idxes
                corr_correspondence_pts = cur_mano_visual_pts[correspondence_pts_idxes]
                
                ####### CD loss using the anchored mano visial pts ########
                # if self.finger_cd_loss_coef > 0.:
                # self.timestep_to_corr_mano_pts[cur_ts] = corr_correspondence_pts.detach()
                self.timestep_to_corr_mano_pts[cur_ts] = cur_mano_visual_pts[anchored_mano_visual_pts].detach()
                # mano visual pts and the dist #
                sampled_cur_visual_pts = cur_visual_pts[sampled_verts_idxes]
                cd_robo_pts_to_corr_mano_pts = torch.sum(
                    (sampled_cur_visual_pts.unsqueeze(1) - cur_mano_visual_pts[anchored_mano_visual_pts].unsqueeze(0)) ** 2, dim=-1
                )
                # cd_robo_pts_to_corr_mano_pts = torch.sqrt(cd_robo_pts_to_corr_mano_pts)
                cd_robo_to_mano, _ = torch.min(cd_robo_pts_to_corr_mano_pts, dim=-1)
                cd_mano_to_robo, _ = torch.min(cd_robo_pts_to_corr_mano_pts, dim=0)
                # finger_cd_loss_coef, finger_tracking_loss_coef, tracking_loss_coef, penetrating_depth_penalty_coef # 
                # diff_robo_to_corr_mano_pts = 0.5 * (cd_robo_to_mano.mean() + cd_mano_to_robo.mean())
                diff_robo_to_corr_mano_pts = cd_mano_to_robo.mean()
                ####### CD loss using the anchored mano visial pts ########

                ####### Tracking loss using the mano visual pts in correspondence ########
                ### the distance from the robot pts to the mano pts #
                ### how to parameterize the motion field ? ###
                self.timestep_to_corr_mano_pts[cur_ts] = corr_correspondence_pts.detach()
                diff_robo_to_corr_mano_pts_finger_tracking = torch.sum(
                    (corr_correspondence_pts - cur_visual_pts[sampled_verts_idxes]) ** 2, dim=-1
                )
                diff_robo_to_corr_mano_pts_finger_tracking = diff_robo_to_corr_mano_pts_finger_tracking[dist_smaller_than_thres]
                diff_robo_to_corr_mano_pts_finger_tracking = diff_robo_to_corr_mano_pts_finger_tracking.mean()
                ####### Tracking loss using the mano visual pts in correspondence ########
                
                self.timestep_to_corr_mano_pts[cur_ts] = cur_transformed_expanded_visual_pts.detach()
                
                
                
                
                # and also the free motion network #
                ##### transform to the active mesh visual pts #####
                # the cur_ts + 1 state in the robot agent is corresponding to the cur_ts state in the simulation with manipulation #
                # cur_visual_pts_mano_pts = cur_visual_pts[self.mano_to_dyn_corr_pts_idxes]
                
                # self.timestep_to_active_mesh[cur_ts] = cur_visual_pts #
                
                if self.optimize_with_intermediates:
                    # timestep_to_passive_mesh_normals; if self.bending_net_type == 'active_force_field_v11'
                    if self.bending_net_type in[ 'active_force_field_v11', 'active_force_field_v12', 'active_force_field_v13', 'active_force_field_v14', 'active_force_field_v15', 'active_force_field_v16', 'active_force_field_v18']:
                        # self.other_bending_network(input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=cur_mano_visual_pts)
                        
                        ### using mano pts to manipulate the object ###
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=correspondence_pts_idxes, reference_mano_pts=cur_mano_visual_pts)
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None)
                        
                        # optimize expanded pts # 
                        if self.optimize_expanded_pts or self.optimize_expanded_ragged_pts:
                            with torch.autograd.set_detect_anomaly(True):
                                self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None)
                        else:
                            if self.use_penalty_based_friction and (not self.use_disp_based_friction):
                                contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)
                            else:
                                ### using robot sampled points ### # 
                                contact_pairs_set = self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)
                        
                        # if self.optimize_expanded_pts: # optimized expanded pts #
                        #     # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=correspondence_pts_idxes, reference_mano_pts=None)
                        #     # anchored_mano_visual_pts
                        #     self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None)
                        # else:
                        #     ### using expanded points ###
                        #     # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None)
                            
                        #     ### using robot sampled points ### # using # # using # # 
                        #     self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None, fix_obj=False if i_iter == 0 else True)
                            
                    elif self.bending_net_type in ['active_force_field_v13_lageu']:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes)
                    else:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, friction_forces=self.mano_expanded_actuator_friction_forces)
                # 
                
                
                if self.train_with_forces_to_active: # 
                    # penetration_forces #
                    if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                        net_penetrating_forces = self.other_bending_network.penetrating_forces
                        net_penetrating_points = self.other_bending_network.penetrating_points
                        # cur_penetrating_forces = []
                        # for i_pts in range(net_penetrating_points.size(0)):
                        #     cur_penetration_point = net_penetrating_points[i_pts]
                        #     cur_penetration_point_forces = net_penetrating_forces[i_pts]
                        #     cur_penetrating_forces.append((cur_penetration_point, cur_penetration_point_forces))
                        # penetration_forces = cur_penetrating_forces
                        
                        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                        self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                        self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                        
                        
                        ### transform the visual pts ###
                        # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        # cur_visual_pts = cur_visual_pts * 2. - 1.
                        # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                        
                        sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[sampled_verts_idxes][self.other_bending_network.penetrating_indicator]
                        # sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[sampled_verts_idxes]
                        
                        
                        net_penetrating_forces = torch.matmul(
                            cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                        net_penetrating_forces = net_penetrating_forces / 2
                        net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                        
                        net_penetrating_points = torch.matmul(
                            cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                        net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                        net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                        
                        
                        
                        
                        # print(f"nn_penetrating points: {net_penetrating_points.size(0)}, {net_penetrating_forces.size()}")
                        #### penetration forces ####
                        penetration_forces = {
                            'penetration_forces': net_penetrating_forces, 'penetration_forces_points': net_penetrating_points
                        }
                        # cur_rot = cur_glb_rot
                        # cur_trans = cur_glb_trans
                    else:
                        penetration_forces = None
                        
                        
                        # timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                        # timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                        
                        
                        # ### transform by the glboal transformation and the translation ###
                        # cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                    
                
                if contact_pairs_set is not None:
                    self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                    
                self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                if self.other_bending_network.penalty_based_friction_forces is not None:
                    self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                
                    
                ''' Computing losses ''' # 
                # cur_rhand_verts = self.rhand_verts[cur_ts]
                # dist_active_pts_rhand_verts = torch.sum(
                #     (cur_visual_pts.unsqueeze(1) - cur_rhand_verts.unsqueeze(0)) ** 2, dim=-1 ### nn_sampled_pts x nn_ref_pts ###
                # ) # each oen should amtch one single point
                # dist_rhand_verts_to_active_pts, _ = torch.min(dist_active_pts_rhand_verts, dim=0)
                # dist_rhand_verts_to_active_pts = torch.mean(dist_rhand_verts_to_active_pts)
                # dist_active_pts_rhand_verts, _ = torch.min(dist_active_pts_rhand_verts, dim=-1)
                # dist_active_pts_rhand_verts = torch.mean(dist_active_pts_rhand_verts)
                
                # # dist active pts rhand verts # # # rhand_verts #
                # dist_active_pts_rhand_verts = (dist_active_pts_rhand_verts + dist_rhand_verts_to_active_pts) / 2.
                
                ### optimize with intermediates ###
                if self.optimize_with_intermediates:
                    tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1) # 
                else:
                    tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    # diff_contact_d = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    # diff_grid_pts_weight = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                # tot_diff_grid_pts_forces.append(diff_contact_d.detach().item()) # #
                # tot_diff_grid_pts_weight.append(diff_grid_pts_weight.detach().item()) # #
                # tot_diff_grid_pts_weight #
                
                # tot penetrating 
                tot_penetrating_depth_penalty.append(self.other_bending_network.penetrating_depth_penalty.item())
                
                loss_finger_tracking = diff_robo_to_corr_mano_pts * self.finger_cd_loss_coef + diff_robo_to_corr_mano_pts_finger_tracking * self.finger_tracking_loss_coef
                
                tot_loss_figner_tracking.append(loss_finger_tracking.detach().item())
                
                tot_ragged_dist.append(ragged_dist.detach().item())
                tot_delta_offset_reg_motion.append(delta_offset_reg_motion.detach().item())
                
                tot_dist_mano_visual_ori_to_cur.append(dist_transformed_expanded_visual_pts_to_ori_visual_pts.detach().item())
                
                tot_diff_cur_states_to_ref_states.append(diff_cur_states_to_ref_states.detach().item())
                
                ### tangential forces ###
                # robot_init_states_ori, robot_glb_rotation_ori, robot_delta_states_ori, robot_actions_ori, robot_glb_trans_ori #
                reg_init_states = torch.sum((self.robot_init_states_ori - self.robot_init_states.weight) ** 2, dim=-1).mean()
                reg_glb_rotation = torch.sum((self.robot_glb_rotation_ori - self.robot_glb_rotation.weight) ** 2, dim=-1).mean()
                reg_delta_states = torch.sum((self.robot_delta_states_ori - self.robot_delta_states.weight) ** 2, dim=-1).mean()
                # reg # reg actions #
                reg_actions = torch.sum((self.robot_actions_ori - self.robot_actions.weight) ** 2, dim=-1).mean()
                reg_glb_trans = torch.sum((self.robot_glb_trans_ori - self.robot_glb_trans.weight) ** 2, dim=-1).mean()
                reg_loss = reg_init_states + reg_glb_rotation + reg_delta_states + reg_glb_trans + reg_actions
                
                tot_reg_loss.append(reg_loss.detach().item())
                
                # #### penalty_friction_tangential_forces, tangential_forces ####
                cur_penalty_friction_tangential_forces = self.other_bending_network.penalty_friction_tangential_forces
                cur_tangential_forces = self.other_bending_network.tangential_forces
                
                
                contact_active_idxes = self.other_bending_network.contact_active_idxes
                
                # if contact_active_idxes is not None:
                #     diff_penalty_friction_forces_with_optimized_forces = torch.sum(
                #         (cur_tangential_forces[contact_active_idxes] - cur_penalty_friction_tangential_forces[contact_active_idxes]) ** 2, dim=-1
                #     )
                # else:
                diff_penalty_friction_forces_with_optimized_forces = torch.sum(
                    (cur_tangential_forces - cur_penalty_friction_tangential_forces) ** 2, dim=-1
                )
                # diff_penalty_friction_forces_with_optimized_forces = diff_penalty_friction_forces_with_optimized_forces.mean()
                diff_penalty_friction_forces_with_optimized_forces = diff_penalty_friction_forces_with_optimized_forces.sum()
                
                
                tot_diff_tangential_forces.append(diff_penalty_friction_forces_with_optimized_forces.detach().item())
                
                # loss = tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef  + ragged_dist * self.ragged_dist_coef  + dist_transformed_expanded_visual_pts_to_ori_visual_pts * 1. + reg_loss + diff_cur_states_to_ref_states * self.loss_weight_diff_states + diff_penalty_friction_forces_with_optimized_forces * self.loss_tangential_diff_coef
                
                if self.use_penalty_based_friction:
                    loss = tracking_loss * self.tracking_loss_coef + reg_loss * self.reg_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                    # ## reg loss coef ## #
                else:
                    loss = tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef  + ragged_dist * self.ragged_dist_coef  + dist_transformed_expanded_visual_pts_to_ori_visual_pts * 1. + reg_loss + diff_cur_states_to_ref_states * self.loss_weight_diff_states + diff_penalty_friction_forces_with_optimized_forces * self.loss_tangential_diff_coef
                    
                
                # loss = diff_cur_states_to_ref_states
                
                # if not self.optimize_expanded_pts:
                #     # loss = tracking_loss + loss_finger_tracking * 0.1 # + self.other_bending_network.penetrating_depth_penalty * 0.000
                #     # loss = loss_finger_tracking + tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty  * self.penetrating_depth_penalty_coef
                #     loss = tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef  + ragged_dist * self.ragged_dist_coef  + dist_transformed_expanded_visual_pts_to_ori_visual_pts * 1. # + delta_offset_reg_motion
                # else:
                #     # delta_offset_reg_motion, ragged_dist, dist_transformed_expanded_visual_pts_to_ori_visual_pts #
                #     # loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty # + loss_finger_tracking
                #     loss = tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef + delta_offset_reg_motion + ragged_dist * self.ragged_dist_coef  + dist_transformed_expanded_visual_pts_to_ori_visual_pts
                
                # # opitmize with intermediates #
                if self.optimize_with_intermediates:
                    if self.bending_net_type == 'active_force_field_v11':
                        cur_penalty_dot_forces_normals = self.other_bending_network.penalty_dot_forces_normals
                        cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                    elif self.bending_net_type == 'active_force_field_v13':
                        if self.no_friction_constraint:
                            cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                        else:
                            cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                    else:
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                        cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                else:
                    cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean().cuda()
                    cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean().cuda()
                # 
                if not self.use_penalty_based_friction:
                    loss = loss + cur_penalty_dot_forces_normals + cur_penalty_friction_constraint
                # 
                self.optimizer.zero_grad()
                # loss.backward()
                with torch.autograd.set_detect_anomaly(True):
                    loss.backward(retain_graph=True)
                self.optimizer.step()
                
                tot_losses.append(loss.detach().item())
                tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                self.update_learning_rate() ## update learning rate ##
                
                
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            tot_penalty_dot_forces_normals = sum(tot_penalty_dot_forces_normals) / float(len(tot_penalty_dot_forces_normals))
            tot_penalty_friction_constraint = sum(tot_penalty_friction_constraint) / float(len(tot_penalty_friction_constraint))
            # tot_dist_act_to_rhand = sum(tot_dist_act_to_rhand) / float(len(tot_dist_act_to_rhand)) 
            # tot_dist_to_contact = sum(tot_dist_to_contact) / float(len(tot_dist_to_contact)) 
            # tot_diff_grid_pts_forces = sum(tot_diff_grid_pts_forces) / float(len(tot_diff_grid_pts_forces)) # 
            # tot_diff_grid_pts_weight = sum(tot_diff_grid_pts_weight) / float(len(tot_diff_grid_pts_weight)) # 
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            tot_penetrating_depth_penalty = sum(tot_penetrating_depth_penalty) / float(len(tot_penetrating_depth_penalty))
            tot_ragged_dist = sum(tot_ragged_dist) / float(len(tot_ragged_dist))
            tot_delta_offset_reg_motion = sum(tot_delta_offset_reg_motion) / float(len(tot_delta_offset_reg_motion))
            tot_dist_mano_visual_ori_to_cur = sum(tot_dist_mano_visual_ori_to_cur) / float(len(tot_dist_mano_visual_ori_to_cur))
            tot_reg_loss = sum(tot_reg_loss) / float(len(tot_reg_loss))
            tot_diff_cur_states_to_ref_states = sum(tot_diff_cur_states_to_ref_states) / float(len(tot_diff_cur_states_to_ref_states))
            
            ### sum of the tangential forces and the diff between tangential forces ###
            tot_diff_tangential_forces = sum(tot_diff_tangential_forces) / float(len(tot_diff_tangential_forces))
            
            if len(tot_loss_figner_tracking) > 0:
                tot_loss_figner_tracking = sum(tot_loss_figner_tracking) / float(len(tot_loss_figner_tracking))
            else:
                tot_loss_figner_tracking = 0.
            # tot_loss_corr_tracking = sum(tot_loss_corr_tracking) / float(len(tot_loss_corr_tracking))
            tot_loss_corr_tracking = 0.
            #
            # cur_sv_penetration_points_fn = os.path.join(self.base_exp_dir, "meshes", f"penetration_points_{i_iter}.npy")
            # # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            # cur_sv_penetration_points = {
            #     'timestep_to_raw_active_meshes': self.timestep_to_raw_active_meshes,
            #     'timestep_to_penetration_points': self.timestep_to_penetration_points,
            #     'timestep_to_penetration_points_forces': self.timestep_to_penetration_points_forces
            # }
            # np.save(cur_sv_penetration_points_fn, cur_sv_penetration_points)
            
            
            
            
            if i_iter % self.report_freq == 0:
                # print(self.base_exp_dir) # ### tot reg loss ###
                # print('iter:{:8>d} loss = {} loss_corr_tracking = {} tracking_loss = {} tot_loss_figner_tracking = {} tot_ragged_dist = {} tot_delta_offset_reg_motion = {} tot_dist_mano_visual_ori_to_cur = {} penetration_depth_penalty = {} penalty_dir = {} penalty_friction = {} reg_loss = {} diff_states = {} lr={}'.format(self.iter_step, tot_losses, tot_loss_corr_tracking,  tot_tracking_loss, tot_loss_figner_tracking, tot_ragged_dist, tot_delta_offset_reg_motion, tot_dist_mano_visual_ori_to_cur, tot_penetrating_depth_penalty, tot_penalty_dot_forces_normals, tot_penalty_friction_constraint, tot_reg_loss, tot_diff_cur_states_to_ref_states, self.optimizer.param_groups[0]['lr']))
                
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} loss_corr_tracking = {} tracking_loss = {} tot_loss_figner_tracking = {} tot_ragged_dist = {} tot_delta_offset_reg_motion = {} tot_dist_mano_visual_ori_to_cur = {} penetration_depth_penalty = {} penalty_dir = {} penalty_friction = {} reg_loss = {} diff_states = {} diff_tangential_forces = {} lr={}'.format(self.iter_step, tot_losses, tot_loss_corr_tracking,  tot_tracking_loss, tot_loss_figner_tracking, tot_ragged_dist, tot_delta_offset_reg_motion, tot_dist_mano_visual_ori_to_cur, tot_penetrating_depth_penalty, tot_penalty_dot_forces_normals, tot_penalty_friction_constraint, tot_reg_loss, tot_diff_cur_states_to_ref_states, tot_diff_tangential_forces, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                

            if i_iter % self.val_mesh_freq == 0:
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                # self.validate_mesh_expanded_pts() 
            # self.active_robot.clear_grads() 
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
    
    
    
    def train_real_robot_actions_from_mano_model_rules_v5(self, ):
        
        ### the real robot actions from mano model rules ###
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load 
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path']
        robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano)  # robot
        self.mano_agent = mano_agent
        ''' Load the mano hand '''
        
        print(f"Start expanding the current visual pts...")
        expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        
        self.expanded_visual_pts_nn = expanded_visual_pts.size(0)
        ## expanded_visual_pts of the expanded visual pts ##
        expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=778 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_robot_actuator_friction_forces.parameters())
        
        
        self.mano_expanded_actuator_delta_offset = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_delta_offset.weight)
        # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
        
        
        self.mano_expanded_actuator_delta_offset_nex = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_delta_offset_nex.weight)
        # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
        
        
        # mano_expanded_actuator_friction_forces, mano_expanded_actuator_delta_offset # 
        self.mano_expanded_actuator_friction_forces = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        # params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        
        # 
        # free_deformation_time_latent, free_deformation_network
        self.free_deformation_time_latent = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        )
        params_to_train += list(self.free_deformation_time_latent.parameters())
        
        # free deformations; time latnets #
        self.deformation_input_dim = 3 + self.bending_latent_size
        self.deformation_output_dim = 3
        ### free deformation network ### # free deformation # 
        self.free_deformation_network = self.construct_field_network(self.deformation_input_dim, self.bending_latent_size,  self.deformation_output_dim)
        params_to_train += list(self.free_deformation_network.parameters())
        
        
        
        # ### laod optimized init actions ####
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # cur_optimized_init_actions = 
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                self.mano_robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            self.mano_robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
            if 'expanded_actuator_friction_forces' in optimized_init_actions_ckpt:
                try:
                    self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_friction_forces'])
                except:
                    pass
            self.mano_expanded_actuator_delta_offset.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_delta_offset'])
        # ### load init transformations ckpts ### #
        
        # robot actions #
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        ## robot actuator friction #    
        # robot actuator friction forces #
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        params_to_train = []
        ''' Only set the robot friction forces optimizable '''
        if not self.optimize_rules:
            params_to_train += list(self.robot_actions.parameters())
            params_to_train += list(self.robot_delta_states.parameters())
            params_to_train += list(self.robot_init_states.parameters())
            params_to_train += list(self.robot_glb_rotation.parameters())
            params_to_train += list(self.robot_glb_trans.parameters())
            
        ''' Only set the robot friction forces optimizable '''
        params_to_train += list(self.mano_expanded_actuator_delta_offset_nex.parameters())
        params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        
        params_to_train += list(self.robot_actuator_friction_forces.parameters())
        if self.optimize_rules:
            params_to_train += list(self.other_bending_network.parameters())
        
        # replay in the diffhand simulator ...? #
        
        # set root states and set rot translations #
        
        ### motivate the robot via actions ###
        ### not set the states -> set the actions ###
        ### loss 1 -> differences between the current states and the target states ###
        ### loss 2 -> tracking loss ###
        ### the contact establishment and the cancellation ###
        
        # get states and use the differences between the current 
        # self.robot_glb_trans = nn.Embedding( # 
        #     num_embeddings=num_steps, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())
        
        
        # # sketche out the actions # #
        # ### laod optimized init actions ####
        # if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
        #     cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
        #     # cur_optimized_init_actions = 
        #     optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
        #     self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
        #     self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
        #     if 'robot_delta_states' in optimized_init_actions_ckpt:
        #         self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
        #     self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions']) # 
        #     self.robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
        #     self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
        #     ### load init transformations ckpts ###

        ### affine ###
        # self.maxx_robo_pts = 25. ##
        # self.minn_robo_pts = -15. ##
        # self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        # self.mult_const_after_cent = 0.5437551664260203
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_transformations_fn = self.conf['model.load_optimized_init_transformations']
            ##### Load optimized initial transformations #####
            optimized_init_transformations_ckpt = torch.load(cur_optimized_init_transformations_fn, map_location=self.device, )
            self.robot_init_states.load_state_dict(optimized_init_transformations_ckpt['robot_init_states'])
            self.robot_glb_rotation.load_state_dict(optimized_init_transformations_ckpt['robot_glb_rotation'])
            # if 'robot_delta_states' in optimized_init_transformations_ckpt:
            #     self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            self.robot_actions.load_state_dict(optimized_init_transformations_ckpt['robot_actions'])
            self.robot_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['robot_actuator_friction_forces'])
            self.robot_glb_trans.load_state_dict(optimized_init_transformations_ckpt['robot_glb_trans'])
            
            if not self.load_only_glb:
                self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            
        if 'model.load_optimized_expanded' in self.conf and len(self.conf['model.load_optimized_expanded']) > 0:
            load_optimized_expanded_ckpt = self.conf['model.load_optimized_expanded']
            load_optimized_expanded_ckpt = torch.load(load_optimized_expanded_ckpt, map_location=self.device)
            self.mano_expanded_actuator_delta_offset.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset'])
            self.mano_expanded_actuator_friction_forces.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_friction_forces'])
            if 'mano_expanded_actuator_delta_offset_nex' in load_optimized_expanded_ckpt:
                print(f"Loading expanded actuator delta offset nex from the saved checkpoint...")
                self.mano_expanded_actuator_delta_offset_nex.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset_nex'])
        
        # # 
        # init_states glb_rotation, delta_states, robot_actions_ori, 
        # make the current states and others near to the original ones #
        # robot_init_states_ori, robot_glb_rotation_ori, robot_delta_states_ori, robot_actions_ori, robot_glb_trans_ori #
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        
        
        # train the robot or train the expanded points #
        # self.mano_fingers = [745, 279, 320, 444, 555, 672, 234, 121]
        # self.robot_fingers = [521, 624, 846, 973, 606, 459, 383, 265]
        
        # optimize with intermediates #
        # self.mano_fingers = torch.tensor(self.mano_fingers, dtype=torch.long).cuda()
        # self.robot_fingers = torch.tensor(self.robot_fingers, dtype=torch.long).cuda()
        
        self.nn_ts = self.nn_timesteps - 1
        # self.optimize_with_intermediates = False
        
        # search for the nearest point so that it can manipulate the object #
        ## TODO: co-optimize the bending network weights? ## # optimize active object is true ##
        # if self.optimize_with_intermediates and (not self.optimize_active_object): # optimize with intermediates #
        #     # if not self.optimize_active_object: #
        #     # free_deformation_time_latent, free_deformation_network #
        #     params_to_train = []
        #     params_to_train += list(self.other_bending_network.parameters())
        #     params_to_train += list(self.robot_actuator_friction_forces.parameters())
        #     # params_to_train += list(self.expanded_actuator_friction_forces.parameters()) # expanded actuator 

        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        # init_rot = R.random().as_quat()
        # init_rot = torch.from_numpy(init_rot).float().cuda()
        # self.robot_glb_rotation.weight.data[0, :] = init_rot[:] # init rot
        
        
        #### get motions --- the original motion of the expanded visual points ####
        #### rag expanded points to the target object ####
        #### use the target object motion to explain the expanded object motion ####
        
        ### Constraint set ###
        self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        # self.timestep_to_active_mesh = {}
        # 
        # init_visual_pts, link_name_to_transformations_and_transformed_pts = get_init_state_visual_pts(ret_link_name_to_tansformations=True)
        # set_body_expanded_visual_pts
        # expanded_visual_pts = compute_expanded_visual_pts_transformation_via_current_state()
        
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts #
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        with torch.no_grad():
            for cur_ts in range(self.nn_ts):
                cur_mano_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_mano_glb_rot = cur_mano_glb_rot / torch.clamp(torch.norm(cur_mano_glb_rot, dim=-1, p=2), min=1e-7)
                cur_mano_glb_rot = dyn_model_act.quaternion_to_matrix(cur_mano_glb_rot) # mano glboal rotations #
                cur_mano_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    mano_links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.mano_agent.set_init_states_target_value(mano_links_init_states)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                else:
                    actions = {}
                    actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    mano_actions_link_actions = self.mano_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    actions['link_actions'] = mano_actions_link_actions
                    self.mano_agent.active_robot.calculate_inertia()
                    self.mano_agent.active_robot.set_actions_and_update_states(mano_actions_link_actions, cur_ts - 1, 0.2)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                cur_mano_visual_pts = self.mano_agent.active_robot.get_transformed_visual_pts() # transformed visual pts # 
                cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                cur_mano_rot = cur_mano_glb_rot
                cur_mano_trans = cur_mano_glb_trans
                ### mano visual pts ###
                cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                
                
                ''' augment with the delta motion '''
                cur_delta_offset_indexes = torch.arange(self.expanded_visual_pts_nn, dtype=torch.long).cuda() + cur_ts * self.expanded_visual_pts_nn
                cur_delta_offset = self.mano_expanded_actuator_delta_offset(cur_delta_offset_indexes)
                cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset # delta offset #
                
                cur_delta_offset_nex = self.mano_expanded_actuator_delta_offset_nex(cur_delta_offset_indexes)
                cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset_nex
                
                # delta_offset_reg_motion = torch.sum(cur_delta_offset ** 2, dim=-1).mean() ### regularize the delta motion #### save the expanded visual pts ####
                self.timestep_to_expanded_visual_pts[cur_ts] = cur_mano_visual_pts.detach().cpu().numpy()
                
                
                # if cur_ts < 11:
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                else:
                    if self.conf['model.train_states']:
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    else:
                        # should be able to set the states only viat he state values #
                        # actions['delta_glb_rot'] = cur_glb_rot #
                        # actions['delta_glb_trans'] = cur_glb_trans #
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        
                        actions['link_actions'] = actions_link_actions # set actions and the s
                        # set and update 
                        self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        
                    if cur_ts == 10:
                        cur_visual_pts, link_name_to_transformations_and_transformed_pts = self.robot_agent.get_init_state_visual_pts(ret_link_name_to_tansformations=True)
                    else:
                        cur_visual_pts = self.robot_agent.get_init_state_visual_pts()

                ### transform the visual pts ###
                cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_visual_pts = cur_visual_pts * 2. -1.
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                # timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                # timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                if not self.use_mano_inputs:
                    self.timestep_to_active_mesh[cur_ts] = cur_visual_pts.detach()
                
                if cur_ts == 10:
                    link_name_to_ragged_ref_pts = {}
                    ### get the ragged points ###
                    link_name_to_link_idx = {}
                    cur_link_idx = 0
                    for link_name in link_name_to_transformations_and_transformed_pts:
                        link_name_to_link_idx[link_name] = cur_link_idx
                        cur_link_idx += 1
                    link_idx_to_link_name = {v: k for k, v in link_name_to_link_idx.items()}
                    dist_expanded_visual_pts_to_link_pts = []
                    
                    link_name_to_selected_link_idxes = {}
                    link_name_to_tot_transformed_pts = {}
                    
                    for cur_link_idx in range(len(link_name_to_transformations_and_transformed_pts)):
                        cur_link_name = link_idx_to_link_name[cur_link_idx]
                        cur_link_transformed_pts, cur_link_transformations = link_name_to_transformations_and_transformed_pts[cur_link_name]
                        
                        cur_link_transformed_pts = (cur_link_transformed_pts - self.minn_robo_pts) / self.extent_robo_pts
                        cur_link_transformed_pts = cur_link_transformed_pts * 2. -1.
                        cur_link_transformed_pts = cur_link_transformed_pts * self.mult_const_after_cent 
                        
                        
                        
                        cur_link_transformed_pts = torch.matmul(cur_rot, cur_link_transformed_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                        
                        link_name_to_tot_transformed_pts[cur_link_name] = cur_link_transformed_pts.detach().cpu().numpy()
                        
                        
                        permutation_idxes = torch.randperm(cur_link_transformed_pts.size(0)).cuda()[:1000]
                        
                        link_name_to_selected_link_idxes[cur_link_name] = permutation_idxes
                        
                        cur_link_transformed_pts_selected = cur_link_transformed_pts[permutation_idxes]
                        
                        dist_expanded_visual_pts_to_cur_link_pts = torch.sum(
                            (cur_mano_visual_pts.unsqueeze(1) - cur_link_transformed_pts_selected.unsqueeze(0)) ** 2, dim=-1
                        )
                        minn_dist_expanded_visual_pts_to_cur_link_pts, _ = torch.min(dist_expanded_visual_pts_to_cur_link_pts, dim=-1)
                        dist_expanded_visual_pts_to_link_pts.append(minn_dist_expanded_visual_pts_to_cur_link_pts)
                    dist_expanded_visual_pts_to_link_pts = torch.stack(dist_expanded_visual_pts_to_link_pts, dim=1)
                    # # 
                    minn_dist_expanded_visual_pts_to_link_pts, minn_idx_expanded_visual_pts_to_link_pts = torch.min(dist_expanded_visual_pts_to_link_pts, dim=-1) ## minn_idx_expanded_visual_pts_to_link_pts ##
                    tot_link_rotations = []
                    tot_link_translations = []
                    for cur_link_idx in range(len(link_name_to_transformations_and_transformed_pts)):
                        expanded_visual_pts_ragged_to_cur_link_mask = minn_idx_expanded_visual_pts_to_link_pts == cur_link_idx
                        cur_link_name = link_idx_to_link_name[cur_link_idx]
                        _, cur_link_transformations = link_name_to_transformations_and_transformed_pts[cur_link_name]
                        if expanded_visual_pts_ragged_to_cur_link_mask.float().sum() > 0:
                            expanded_visual_pts_ragged_to_cur_link = cur_mano_visual_pts[expanded_visual_pts_ragged_to_cur_link_mask]
                            cur_link_rotations, cur_link_translations = cur_link_transformations
                            # R^T (pts - center) #
                            ref_pts = torch.matmul(cur_link_rotations.transpose(1, 0), (expanded_visual_pts_ragged_to_cur_link - cur_link_translations.unsqueeze(0)).contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous()
                            link_name_to_ragged_ref_pts[cur_link_name] = ref_pts.detach() # .cpu().numpy()
                        tot_link_rotations.append(cur_link_transformations[0])
                        tot_link_translations.append(cur_link_transformations[1])
                    # self.robot_agent.set_body_expanded_visual_pts(link_name_to_ragged_ref_pts)
                    
                    tot_link_rotations = torch.stack(tot_link_rotations, dim=0)
                    tot_link_translations = torch.stack(tot_link_translations, dim=0)
                    
                    
                    ref_expanded_visual_pts = torch.matmul(cur_rot.transpose(1, 0), (cur_mano_visual_pts - cur_trans.unsqueeze(0)).contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() 
                    ref_expanded_visual_pts = ref_expanded_visual_pts / self.mult_const_after_cent
                    ref_expanded_visual_pts = (ref_expanded_visual_pts + 1.) / 2. 
                    ref_expanded_visual_pts = ref_expanded_visual_pts * self.extent_robo_pts + self.minn_robo_pts
                    
                    
                    
                    # # nn_visual_pts # 
                    expanded_visual_pts_selected_rotations = fields.batched_index_select(values=tot_link_rotations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                    expanded_visual_pts_selected_translaations = fields.batched_index_select(values=tot_link_translations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                    
                    ref_expanded_visual_pts = torch.matmul(
                        expanded_visual_pts_selected_rotations.contiguous().transpose(2, 1), (ref_expanded_visual_pts - expanded_visual_pts_selected_translaations).unsqueeze(-1)
                    ).squeeze(-1) ## nn_visual_pts x 3 ##
                    
                    
                    save_visual_pts_link_name_to_visual_pts = {
                        'cur_mano_visual_pts': cur_mano_visual_pts.detach().cpu().numpy(),
                        'link_name_to_tot_transformed_pts' : link_name_to_tot_transformed_pts
                    }
                    np.save(f"save_visual_pts_link_name_to_visual_pts.npy", save_visual_pts_link_name_to_visual_pts)
        
        
        if self.train_actions_with_states:
            for cur_ts in range(self.nn_ts):
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                else:
                    # if self.conf['model.train_states']:
                    cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    states = {}
                    states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    states['link_states'] = cur_state
                    self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)      
            tmp_joint_nm_to_ts_to_states = self.robot_agent.get_timestep_to_states()
            joint_nm_to_ts_to_states = {}
            for cur_joint_nm in tmp_joint_nm_to_ts_to_states:
                cur_joint_ts_to_states = {}
                for cur_ts in tmp_joint_nm_to_ts_to_states[cur_joint_nm]:
                    cur_joint_ts_to_states[cur_ts] = tmp_joint_nm_to_ts_to_states[cur_joint_nm][cur_ts].detach().clone()
                joint_nm_to_ts_to_states[cur_joint_nm] = cur_joint_ts_to_states
    
        # model_path = self.conf['model.sim_model_path']
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        # self.robot_agent = robot_agent 
        
        penetration_forces = None
        
        
        
        #### get the expanded ####
        ### get the expanded transformed points at each timestep ###
        #### TODO: rotation optimization and the translation optimization ####
        for i_iter in tqdm(range(100000)): ## actions from model rules ### 
            tot_losses = []
            tot_penalty_dot_forces_normals = []
            tot_penalty_friction_constraint = []
            tot_dist_act_to_rhand = []
            tot_dist_to_contact = []
            ## tot_diff_grid_pts_forces, tot_diff_grid_pts_weight ##
            tot_diff_grid_pts_forces = []
            tot_diff_grid_pts_weight = []
            tot_loss_figner_tracking = []
            tot_loss_corr_tracking = []
            tot_tracking_loss = []
            
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # self.timestep_to_
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            correspondence_pts_idxes = None
            
            tot_penetrating_depth_penalty = []
            
            tot_ragged_dist = []
            
            tot_delta_offset_reg_motion = []
            
            tot_dist_mano_visual_ori_to_cur = []
            
            tot_reg_loss = []
            
            tot_diff_cur_states_to_ref_states = []
            
            tot_diff_tangential_forces = []
            
            penetration_forces = None
            sampled_visual_pts_joint_idxes = None
            
            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_force_d = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16]
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            for cur_ts in range(self.nn_ts):
                actions = {}
                
                cur_mano_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_mano_glb_rot = cur_mano_glb_rot / torch.clamp(torch.norm(cur_mano_glb_rot, dim=-1, p=2), min=1e-7)
                cur_mano_glb_rot = dyn_model_act.quaternion_to_matrix(cur_mano_glb_rot) # mano glboal rotations #
                cur_mano_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    mano_links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.mano_agent.set_init_states_target_value(mano_links_init_states)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                else:
                    # if self.conf['model.train_states']:
                    #     cur_state = self.mano_robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    #     self.mano_agent.active_robot.set_delta_state_and_update(cur_state, cur_ts)
                    #     cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                    # else:
                    actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    mano_actions_link_actions = self.mano_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    actions['link_actions'] = mano_actions_link_actions
                    self.mano_agent.active_robot.calculate_inertia()
                    self.mano_agent.active_robot.set_actions_and_update_states(mano_actions_link_actions, cur_ts - 1, 0.2)
                    cur_mano_visual_pts = self.mano_agent.get_init_state_visual_pts()
                ## mano visual pts ##
                cur_mano_visual_pts = self.mano_agent.active_robot.get_transformed_visual_pts() # transformed visual pts #
                
                
                
                
                if cur_ts > 0:
                    prev_rot = timestep_to_tot_rot[cur_ts - 1]
                    cur_mano_rot = torch.matmul(cur_mano_glb_rot, prev_rot)
                    prev_trans = timestep_to_tot_trans[cur_ts - 1]
                    cur_mano_trans = cur_mano_glb_trans + prev_trans
                else:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                if not self.conf['model.using_delta_glb_trans']:
                    cur_mano_rot = cur_mano_glb_rot
                    cur_mano_trans = cur_mano_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_mano_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_mano_trans.detach()
                
                cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                if cur_ts > 0:
                    ## sue the original mano articulated motion with delta position offsets ##
                    self.timestep_to_posed_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    
                    cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                    
                    ''' augment with the delta motion '''
                    cur_delta_offset_indexes = torch.arange(self.expanded_visual_pts_nn, dtype=torch.long).cuda() + cur_ts * self.expanded_visual_pts_nn
                    cur_delta_offset = self.mano_expanded_actuator_delta_offset(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset
                    
                    cur_delta_offset_nex = self.mano_expanded_actuator_delta_offset_nex(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset_nex
                    
                    delta_offset_reg_motion = torch.sum(cur_delta_offset_nex ** 2, dim=-1).mean() ### regularize the delta motion ###
                    ''' augment with the delta motion '''
                    
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                else:
                    self.timestep_to_posed_mano_active_mesh[cur_ts] = cur_mano_visual_pts.detach()
                    cur_mano_visual_pts = torch.matmul(cur_mano_rot, cur_mano_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_mano_trans.unsqueeze(0)
                    
                    ''' augment with the delta motion '''
                    cur_delta_offset_indexes = torch.arange(self.expanded_visual_pts_nn, dtype=torch.long).cuda() + cur_ts * self.expanded_visual_pts_nn
                    cur_delta_offset = self.mano_expanded_actuator_delta_offset(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset
                    
                    cur_delta_offset_nex = self.mano_expanded_actuator_delta_offset_nex(cur_delta_offset_indexes)
                    cur_mano_visual_pts = cur_mano_visual_pts + cur_delta_offset_nex
                    
                    delta_offset_reg_motion = torch.sum(cur_delta_offset_nex ** 2, dim=-1).mean() ### regularize the delta motion ###
                    ''' augment with the delta motion '''
                    
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                
                
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts, link_name_to_link_transformations = self.robot_agent.get_init_state_visual_pts(True)
                else:
                    if self.conf['model.train_states']:
                        if self.add_delta_state_constraints:
                            self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg] = torch.clamp(self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg], max=0.)
                        
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    else:
                        # print(f"Draing via actions")
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        
                        actions['link_actions'] = actions_link_actions
                        joint_name_to_penetration_forces_intermediates = self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1, penetration_forces=penetration_forces, sampled_visual_pts_joint_idxes=sampled_visual_pts_joint_idxes) # upate the state at cur_ts # 
                        # joint_name_to_penetration_forces_intermediates = self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1, penetration_forces=None, sampled_visual_pts_joint_idxes=sampled_visual_pts_joint_idxes) # upate the state at cur_ts # 
                        self.joint_name_to_penetration_forces_intermediates[cur_ts] = joint_name_to_penetration_forces_intermediates
                    cur_visual_pts, link_name_to_link_transformations, visual_pts_joint_idxes = self.robot_agent.get_init_state_visual_pts(True, True)

                if self.train_actions_with_states:
                    nex_ts_joint_nm_to_states = self.robot_agent.get_joint_nm_to_states()
                    diff_cur_states_to_ref_states = []
                    for joint_nm in nex_ts_joint_nm_to_states:
                        cur_states = nex_ts_joint_nm_to_states[joint_nm]
                        # ref_states = joint_nm_to_ts_to_states[joint_nm][cur_ts + 1]
                        ### reference state at the time cur_ts ####
                        ref_states = joint_nm_to_ts_to_states[joint_nm][cur_ts]
                        diff_cur_joint = torch.sum(
                            (cur_states - ref_states.detach()) ** 2
                        )
                        diff_cur_states_to_ref_states.append(diff_cur_joint)
                        # print(f"cur_ts: {cur_ts}, joint_nm: {joint_nm}, cur_states: {cur_states}, ref_states: {ref_states}") #
                    
                    diff_cur_states_to_ref_states = torch.stack(diff_cur_states_to_ref_states, dim=0)
                    diff_cur_states_to_ref_states = diff_cur_states_to_ref_states.mean()
                else: # cur states to ref states #
                    diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                ### transform the visual pts ###
                cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_visual_pts = cur_visual_pts * 2. - 1.
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                if not self.use_mano_inputs:
                    self.timestep_to_active_mesh[cur_ts] = cur_visual_pts #
                
                self.timestep_to_raw_active_meshes[cur_ts] = cur_visual_pts.detach().cpu().numpy()
                
                tot_link_rotations = []
                tot_link_translations = []
                for cur_link_idx in range(len(link_name_to_link_transformations)):
                    # expanded_visual_pts_ragged_to_cur_link_mask = minn_idx_expanded_visual_pts_to_link_pts == cur_link_idx
                    cur_link_name = link_idx_to_link_name[cur_link_idx]
                    _, cur_link_transformations = link_name_to_link_transformations[cur_link_name]
                    tot_link_rotations.append(cur_link_transformations[0])
                    tot_link_translations.append(cur_link_transformations[1])
                tot_link_rotations = torch.stack(tot_link_rotations, dim=0)
                tot_link_translations = torch.stack(tot_link_translations, dim=0)
                
                # # empty_cache() # #
                if cur_ts == 10:
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    torch.cuda.empty_cache()
                    
                    dist_expanded_visual_pts_to_link_pts = []
                        
                    link_name_to_tot_transformed_pts = {}
                    for cur_link_idx in range(len(link_name_to_link_transformations)):
                        cur_link_name = link_idx_to_link_name[cur_link_idx]
                        cur_link_transformed_pts, cur_link_transformations = link_name_to_link_transformations[cur_link_name]
                        
                        cur_link_transformed_pts = (cur_link_transformed_pts - self.minn_robo_pts) / self.extent_robo_pts
                        cur_link_transformed_pts = cur_link_transformed_pts * 2. -1.
                        cur_link_transformed_pts = cur_link_transformed_pts * self.mult_const_after_cent 
                        
                        cur_link_transformed_pts = torch.matmul(cur_rot, cur_link_transformed_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                        
                        link_name_to_tot_transformed_pts[cur_link_name] = cur_link_transformed_pts.detach().cpu().numpy()
                        
                        
                        permutation_idxes = link_name_to_selected_link_idxes[cur_link_name]
                        cur_link_transformed_pts_selected = cur_link_transformed_pts[permutation_idxes].detach()
                        
                        dist_expanded_visual_pts_to_cur_link_pts = torch.sum(
                            (cur_mano_visual_pts.unsqueeze(1) - cur_link_transformed_pts_selected.unsqueeze(0)) ** 2, dim=-1
                        )
                        minn_dist_expanded_visual_pts_to_cur_link_pts, _ = torch.min(dist_expanded_visual_pts_to_cur_link_pts, dim=-1)
                        dist_expanded_visual_pts_to_link_pts.append(minn_dist_expanded_visual_pts_to_cur_link_pts)
                    dist_expanded_visual_pts_to_link_pts = torch.stack(dist_expanded_visual_pts_to_link_pts, dim=1)
                    # # 
                    minn_dist_expanded_visual_pts_to_link_pts, minn_idx_expanded_visual_pts_to_link_pts = torch.min(dist_expanded_visual_pts_to_link_pts, dim=-1) ## 
                    
                    
                    ref_expanded_visual_pts = torch.matmul(cur_rot.transpose(1, 0), (cur_mano_visual_pts - cur_trans.unsqueeze(0)).contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() 
                    ref_expanded_visual_pts = ref_expanded_visual_pts / self.mult_const_after_cent
                    ref_expanded_visual_pts = (ref_expanded_visual_pts + 1.) / 2. 
                    ref_expanded_visual_pts = ref_expanded_visual_pts * self.extent_robo_pts + self.minn_robo_pts
                    
                    expanded_visual_pts_selected_rotations = fields.batched_index_select(values=tot_link_rotations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                    expanded_visual_pts_selected_translaations = fields.batched_index_select(values=tot_link_translations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                    
                    ref_expanded_visual_pts = torch.matmul(
                        expanded_visual_pts_selected_rotations.contiguous().transpose(2, 1), (ref_expanded_visual_pts - expanded_visual_pts_selected_translaations).unsqueeze(-1)
                    ).squeeze(-1).detach() ## nn_visual_pts x 3 ##
                    
                    ragged_dist = torch.mean(minn_dist_expanded_visual_pts_to_link_pts)
                else:
                    ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                #     ragged_dist = torch.mean(minn_dist_expanded_visual_pts_to_link_pts)
                # else:
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean() # # 

                # nn_visual_pts #
                expanded_visual_pts_selected_rotations = fields.batched_index_select(values=tot_link_rotations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                expanded_visual_pts_selected_translaations = fields.batched_index_select(values=tot_link_translations, indices=minn_idx_expanded_visual_pts_to_link_pts, dim=0)
                
                # print(f"expanded_visual_pts_selected_rotations: {expanded_visual_pts_selected_rotations.size()}, expanded_visual_pts_selected_translaations: {expanded_visual_pts_selected_translaations.size()}, ref_expanded_visual_pts: {ref_expanded_visual_pts.size()}")
                # cur_transformed_expanded_visual_pts : nn_visual_pts x 3 #
                cur_transformed_expanded_visual_pts = torch.matmul(
                    expanded_visual_pts_selected_rotations, ref_expanded_visual_pts.unsqueeze(-1)
                ).squeeze(-1) + expanded_visual_pts_selected_translaations
                
                cur_transformed_expanded_visual_pts = (cur_transformed_expanded_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_transformed_expanded_visual_pts = cur_transformed_expanded_visual_pts * 2. -1.
                cur_transformed_expanded_visual_pts = cur_transformed_expanded_visual_pts * self.mult_const_after_cent # mult_const #
                
                ### transform by the glboal transformation and the translation ###
                cur_transformed_expanded_visual_pts = torch.matmul(cur_rot, cur_transformed_expanded_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                
                if self.optimize_expanded_pts:
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                else:
                    self.timestep_to_mano_active_mesh[cur_ts] = cur_transformed_expanded_visual_pts
    
                # self.timestep_to_mano_active_mesh[cur_ts] = cur_mano_visual_pts
                
                # for a
                # delta_offset_reg_motion, ragged_dist, dist_transformed_expanded_visual_pts_to_ori_visual_pts #
                dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.sum(
                    (cur_transformed_expanded_visual_pts - cur_mano_visual_pts) ** 2, dim=-1
                )
                dist_transformed_expanded_visual_pts_to_ori_visual_pts = dist_transformed_expanded_visual_pts_to_ori_visual_pts.mean()
                
                        
                
                # timestep_to_tot_rot[cur_ts] = cur_mano_rot.detach()
                # timestep_to_tot_trans[cur_ts] = cur_mano_trans.detach()
                # #### tiemstep to tot trans #### # 
                # cur_mano_visual_pts = cur_mano_visual_pts * self.mano_mult_const_after_cent
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                # if i_iter == 0 and cur_ts < 11:
                if  cur_ts < 11:
                # if cur_ts == 0 and correspondence_pts_idxes is None:
                    dist_robot_pts_to_mano_pts = torch.sum(
                        (cur_visual_pts[sampled_verts_idxes].unsqueeze(1) - cur_mano_visual_pts.unsqueeze(0)) ** 2, dim=-1
                    )
                    minn_dist_robot_pts_to_mano_pts, correspondence_pts_idxes = torch.min(dist_robot_pts_to_mano_pts, dim=-1)
                    minn_dist_robot_pts_to_mano_pts = torch.sqrt(minn_dist_robot_pts_to_mano_pts)
                    # dist_smaller_than_thres = minn_dist_robot_pts_to_mano_pts < 0.01
                    dist_smaller_than_thres = minn_dist_robot_pts_to_mano_pts < 0.005
                    
                    corr_correspondence_pts = cur_mano_visual_pts[correspondence_pts_idxes]
                    dist_corr_correspondence_pts_to_mano_visual_pts = torch.sum(
                        (corr_correspondence_pts.unsqueeze(1) - cur_mano_visual_pts.unsqueeze(0)) ** 2, dim=-1
                    )
                    dist_corr_correspondence_pts_to_mano_visual_pts = torch.sqrt(dist_corr_correspondence_pts_to_mano_visual_pts)
                    minn_dist_to_corr_pts, _ = torch.min(dist_corr_correspondence_pts_to_mano_visual_pts, dim=0)
                    anchored_mano_visual_pts = minn_dist_to_corr_pts < 0.05
                    
                    # if 'model.finger_cd_loss' # distance related quantities -> object offsets #
                    # 
                    # the anchored mano visual pts # # cd loss 
                    # if self.finger_cd_loss_coef > 0.:
                    #     correspondence_pts_idxes = anchored_mano_visual_pts
                
                # timestep_to_corr_mano_pts # 
                # timestep_to_mano_active_mesh # 
                # cur_mano_visual_pts = cur_mano_visual_pts[correspondence_pts_idxes
                corr_correspondence_pts = cur_mano_visual_pts[correspondence_pts_idxes]
                
                ####### CD loss using the anchored mano visial pts ########
                # if self.finger_cd_loss_coef > 0.:
                # self.timestep_to_corr_mano_pts[cur_ts] = corr_correspondence_pts.detach()
                self.timestep_to_corr_mano_pts[cur_ts] = cur_mano_visual_pts[anchored_mano_visual_pts].detach()
                # mano visual pts and the dist #
                sampled_cur_visual_pts = cur_visual_pts[sampled_verts_idxes]
                cd_robo_pts_to_corr_mano_pts = torch.sum(
                    (sampled_cur_visual_pts.unsqueeze(1) - cur_mano_visual_pts[anchored_mano_visual_pts].unsqueeze(0)) ** 2, dim=-1
                )
                # cd_robo_pts_to_corr_mano_pts = torch.sqrt(cd_robo_pts_to_corr_mano_pts)
                cd_robo_to_mano, _ = torch.min(cd_robo_pts_to_corr_mano_pts, dim=-1)
                cd_mano_to_robo, _ = torch.min(cd_robo_pts_to_corr_mano_pts, dim=0)
                # finger_cd_loss_coef, finger_tracking_loss_coef, tracking_loss_coef, penetrating_depth_penalty_coef # 
                # diff_robo_to_corr_mano_pts = 0.5 * (cd_robo_to_mano.mean() + cd_mano_to_robo.mean())
                diff_robo_to_corr_mano_pts = cd_mano_to_robo.mean()
                ####### CD loss using the anchored mano visial pts ########

                ####### Tracking loss using the mano visual pts in correspondence ########
                ### the distance from the robot pts to the mano pts #
                ### how to parameterize the motion field ? ###
                self.timestep_to_corr_mano_pts[cur_ts] = corr_correspondence_pts.detach()
                diff_robo_to_corr_mano_pts_finger_tracking = torch.sum(
                    (corr_correspondence_pts - cur_visual_pts[sampled_verts_idxes]) ** 2, dim=-1
                )
                diff_robo_to_corr_mano_pts_finger_tracking = diff_robo_to_corr_mano_pts_finger_tracking[dist_smaller_than_thres]
                diff_robo_to_corr_mano_pts_finger_tracking = diff_robo_to_corr_mano_pts_finger_tracking.mean()
                ####### Tracking loss using the mano visual pts in correspondence ########
                
                self.timestep_to_corr_mano_pts[cur_ts] = cur_transformed_expanded_visual_pts.detach()
                
                
                
                if self.optimize_with_intermediates:
                    # timestep_to_passive_mesh_normals; if self.bending_net_type == 'active_force_field_v11'
                    if self.bending_net_type in[ 'active_force_field_v11', 'active_force_field_v12', 'active_force_field_v13', 'active_force_field_v14', 'active_force_field_v15', 'active_force_field_v16', 'active_force_field_v17', 'active_force_field_v18']:
                        # self.other_bending_network(input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=cur_mano_visual_pts)
                        
                        ### using mano pts to manipulate the object ###
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=correspondence_pts_idxes, reference_mano_pts=cur_mano_visual_pts)
                        # self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None)
                        
                        # optimize expanded pts # 
                        if self.optimize_expanded_pts or self.optimize_expanded_ragged_pts:
                            with torch.autograd.set_detect_anomaly(True):
                                self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_mano_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None)
                        else:
                            if self.use_mano_inputs:
                                contact_pairs_set = self.other_bending_network.forward2(input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set) # contac 
                            elif self.use_penalty_based_friction and (not self.use_disp_based_friction):
                                contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)
                            else:
                                ### using robot sampled points ### # 
                                contact_pairs_set = self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)
                    elif self.bending_net_type in ['active_force_field_v13_lageu']:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.mano_expanded_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes)
                    else:
                        self.other_bending_network( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, friction_forces=self.mano_expanded_actuator_friction_forces)
                # 
                
                
                if self.train_with_forces_to_active and (not self.use_mano_inputs): # 
                    # penetration_forces #
                    if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                        net_penetrating_forces = self.other_bending_network.penetrating_forces
                        net_penetrating_points = self.other_bending_network.penetrating_points
                        # cur_penetrating_forces = []
                        # for i_pts in range(net_penetrating_points.size(0)):
                        #     cur_penetration_point = net_penetrating_points[i_pts]
                        #     cur_penetration_point_forces = net_penetrating_forces[i_pts]
                        #     cur_penetrating_forces.append((cur_penetration_point, cur_penetration_point_forces))
                        # penetration_forces = cur_penetrating_forces
                        
                        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                        self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                        self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                        
                        
                        ### transform the visual pts ###
                        # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        # cur_visual_pts = cur_visual_pts * 2. - 1.
                        # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                        
                        sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[sampled_verts_idxes][self.other_bending_network.penetrating_indicator]
                        # sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[sampled_verts_idxes]
                        
                        
                        net_penetrating_forces = torch.matmul(
                            cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                        net_penetrating_forces = net_penetrating_forces / 2
                        net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                        
                        net_penetrating_points = torch.matmul(
                            cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                        net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                        net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                        
                        
                        
                        
                        # print(f"nn_penetrating points: {net_penetrating_points.size(0)}, {net_penetrating_forces.size()}")
                        #### penetration forces ####
                        penetration_forces = {
                            'penetration_forces': net_penetrating_forces, 'penetration_forces_points': net_penetrating_points
                        }
                        # cur_rot = cur_glb_rot
                        # cur_trans = cur_glb_trans
                    else:
                        penetration_forces = None
                        
                        
                        # timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                        # timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                        
                        
                        # ### transform by the glboal transformation and the translation ###
                        # cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                    
                
                if contact_pairs_set is not None:
                    self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                    
                self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                if self.other_bending_network.penalty_based_friction_forces is not None:
                    self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                
                    
                ''' Computing losses ''' # 
                # cur_rhand_verts = self.rhand_verts[cur_ts]
                # dist_active_pts_rhand_verts = torch.sum(
                #     (cur_visual_pts.unsqueeze(1) - cur_rhand_verts.unsqueeze(0)) ** 2, dim=-1 ### nn_sampled_pts x nn_ref_pts ###
                # ) # each oen should amtch one single point
                # dist_rhand_verts_to_active_pts, _ = torch.min(dist_active_pts_rhand_verts, dim=0)
                # dist_rhand_verts_to_active_pts = torch.mean(dist_rhand_verts_to_active_pts)
                # dist_active_pts_rhand_verts, _ = torch.min(dist_active_pts_rhand_verts, dim=-1)
                # dist_active_pts_rhand_verts = torch.mean(dist_active_pts_rhand_verts)
                
                # # dist active pts rhand verts # # # rhand_verts #
                # dist_active_pts_rhand_verts = (dist_active_pts_rhand_verts + dist_rhand_verts_to_active_pts) / 2.
                
                ### optimize with intermediates ###
                if self.optimize_with_intermediates:
                    tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1) # 
                else:
                    tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    # diff_contact_d = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    # diff_grid_pts_weight = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                # tot_diff_grid_pts_forces.append(diff_contact_d.detach().item()) # #
                # tot_diff_grid_pts_weight.append(diff_grid_pts_weight.detach().item()) # #
                # tot_diff_grid_pts_weight #
                
                # tot penetrating 
                tot_penetrating_depth_penalty.append(self.other_bending_network.penetrating_depth_penalty.item())
                
                loss_finger_tracking = diff_robo_to_corr_mano_pts * self.finger_cd_loss_coef + diff_robo_to_corr_mano_pts_finger_tracking * self.finger_tracking_loss_coef
                
                tot_loss_figner_tracking.append(loss_finger_tracking.detach().item())
                
                tot_ragged_dist.append(ragged_dist.detach().item())
                tot_delta_offset_reg_motion.append(delta_offset_reg_motion.detach().item())
                
                tot_dist_mano_visual_ori_to_cur.append(dist_transformed_expanded_visual_pts_to_ori_visual_pts.detach().item())
                
                tot_diff_cur_states_to_ref_states.append(diff_cur_states_to_ref_states.detach().item())
                
                ### tangential forces ###
                # robot_init_states_ori, robot_glb_rotation_ori, robot_delta_states_ori, robot_actions_ori, robot_glb_trans_ori #
                reg_init_states = torch.sum((self.robot_init_states_ori - self.robot_init_states.weight) ** 2, dim=-1).mean()
                reg_glb_rotation = torch.sum((self.robot_glb_rotation_ori - self.robot_glb_rotation.weight) ** 2, dim=-1).mean()
                reg_delta_states = torch.sum((self.robot_delta_states_ori - self.robot_delta_states.weight) ** 2, dim=-1).mean()
                # reg # reg actions #
                reg_actions = torch.sum((self.robot_actions_ori - self.robot_actions.weight) ** 2, dim=-1).mean()
                reg_glb_trans = torch.sum((self.robot_glb_trans_ori - self.robot_glb_trans.weight) ** 2, dim=-1).mean()
                reg_loss = reg_init_states + reg_glb_rotation + reg_delta_states + reg_glb_trans + reg_actions
                
                tot_reg_loss.append(reg_loss.detach().item())
                
                # #### penalty_friction_tangential_forces, tangential_forces ####
                cur_penalty_friction_tangential_forces = self.other_bending_network.penalty_friction_tangential_forces
                cur_tangential_forces = self.other_bending_network.tangential_forces
                
                
                contact_active_idxes = self.other_bending_network.contact_active_idxes
                
                # if contact_active_idxes is not None:
                #     diff_penalty_friction_forces_with_optimized_forces = torch.sum(
                #         (cur_tangential_forces[contact_active_idxes] - cur_penalty_friction_tangential_forces[contact_active_idxes]) ** 2, dim=-1
                #     )
                # else:
                diff_penalty_friction_forces_with_optimized_forces = torch.sum(
                    (cur_tangential_forces - cur_penalty_friction_tangential_forces) ** 2, dim=-1
                )
                # diff_penalty_friction_forces_with_optimized_forces = diff_penalty_friction_forces_with_optimized_forces.mean()
                diff_penalty_friction_forces_with_optimized_forces = diff_penalty_friction_forces_with_optimized_forces.sum()
                
                
                tot_diff_tangential_forces.append(diff_penalty_friction_forces_with_optimized_forces.detach().item())
                
                # loss = tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef  + ragged_dist * self.ragged_dist_coef  + dist_transformed_expanded_visual_pts_to_ori_visual_pts * 1. + reg_loss + diff_cur_states_to_ref_states * self.loss_weight_diff_states + diff_penalty_friction_forces_with_optimized_forces * self.loss_tangential_diff_coef
                
                if self.use_penalty_based_friction:
                    loss = tracking_loss * self.tracking_loss_coef + reg_loss * self.reg_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                    # ## reg loss coef ## #
                else:
                    loss = tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef  + ragged_dist * self.ragged_dist_coef  + dist_transformed_expanded_visual_pts_to_ori_visual_pts * 1. + reg_loss + diff_cur_states_to_ref_states * self.loss_weight_diff_states + diff_penalty_friction_forces_with_optimized_forces * self.loss_tangential_diff_coef
                    
                
                # loss = diff_cur_states_to_ref_states
                
                # if not self.optimize_expanded_pts:
                #     # loss = tracking_loss + loss_finger_tracking * 0.1 # + self.other_bending_network.penetrating_depth_penalty * 0.000
                #     # loss = loss_finger_tracking + tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty  * self.penetrating_depth_penalty_coef
                #     loss = tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef  + ragged_dist * self.ragged_dist_coef  + dist_transformed_expanded_visual_pts_to_ori_visual_pts * 1. # + delta_offset_reg_motion
                # else:
                #     # delta_offset_reg_motion, ragged_dist, dist_transformed_expanded_visual_pts_to_ori_visual_pts #
                #     # loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty # + loss_finger_tracking
                #     loss = tracking_loss * self.tracking_loss_coef + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef + delta_offset_reg_motion + ragged_dist * self.ragged_dist_coef  + dist_transformed_expanded_visual_pts_to_ori_visual_pts
                
                # # opitmize with intermediates #
                if self.optimize_with_intermediates:
                    if self.bending_net_type == 'active_force_field_v11':
                        cur_penalty_dot_forces_normals = self.other_bending_network.penalty_dot_forces_normals
                        cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                    elif self.bending_net_type == 'active_force_field_v13':
                        if self.no_friction_constraint:
                            cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                        else:
                            cur_penalty_friction_constraint = self.other_bending_network.penalty_friction_constraint
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                    else:
                        cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean()
                        cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean()
                else:
                    cur_penalty_dot_forces_normals = torch.zeros((1,), dtype=torch.float32).mean().cuda()
                    cur_penalty_friction_constraint = torch.zeros((1,), dtype=torch.float32).mean().cuda()
                # 
                if not self.use_penalty_based_friction:
                    loss = loss + cur_penalty_dot_forces_normals + cur_penalty_friction_constraint
                # 
                self.optimizer.zero_grad()
                # loss.backward()
                try:
                    with torch.autograd.set_detect_anomaly(True):
                        loss.backward(retain_graph=True)
                except:
                    traceback.print_stack()
                    exit(1)
                self.optimizer.step()
                
                tot_losses.append(loss.detach().item())
                tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                self.update_learning_rate() ## update learning rate ##
                
                
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            tot_penalty_dot_forces_normals = sum(tot_penalty_dot_forces_normals) / float(len(tot_penalty_dot_forces_normals))
            tot_penalty_friction_constraint = sum(tot_penalty_friction_constraint) / float(len(tot_penalty_friction_constraint))
            # tot_dist_act_to_rhand = sum(tot_dist_act_to_rhand) / float(len(tot_dist_act_to_rhand)) 
            # tot_dist_to_contact = sum(tot_dist_to_contact) / float(len(tot_dist_to_contact)) 
            # tot_diff_grid_pts_forces = sum(tot_diff_grid_pts_forces) / float(len(tot_diff_grid_pts_forces)) # 
            # tot_diff_grid_pts_weight = sum(tot_diff_grid_pts_weight) / float(len(tot_diff_grid_pts_weight)) # 
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            tot_penetrating_depth_penalty = sum(tot_penetrating_depth_penalty) / float(len(tot_penetrating_depth_penalty))
            tot_ragged_dist = sum(tot_ragged_dist) / float(len(tot_ragged_dist))
            tot_delta_offset_reg_motion = sum(tot_delta_offset_reg_motion) / float(len(tot_delta_offset_reg_motion))
            tot_dist_mano_visual_ori_to_cur = sum(tot_dist_mano_visual_ori_to_cur) / float(len(tot_dist_mano_visual_ori_to_cur))
            tot_reg_loss = sum(tot_reg_loss) / float(len(tot_reg_loss))
            tot_diff_cur_states_to_ref_states = sum(tot_diff_cur_states_to_ref_states) / float(len(tot_diff_cur_states_to_ref_states))
            
            ### sum of the tangential forces and the diff between tangential forces ###
            tot_diff_tangential_forces = sum(tot_diff_tangential_forces) / float(len(tot_diff_tangential_forces))
            
            if len(tot_loss_figner_tracking) > 0:
                tot_loss_figner_tracking = sum(tot_loss_figner_tracking) / float(len(tot_loss_figner_tracking))
            else:
                tot_loss_figner_tracking = 0.
            # tot_loss_corr_tracking = sum(tot_loss_corr_tracking) / float(len(tot_loss_corr_tracking))
            tot_loss_corr_tracking = 0.
            #
            # cur_sv_penetration_points_fn = os.path.join(self.base_exp_dir, "meshes", f"penetration_points_{i_iter}.npy")
            # # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            # cur_sv_penetration_points = {
            #     'timestep_to_raw_active_meshes': self.timestep_to_raw_active_meshes,
            #     'timestep_to_penetration_points': self.timestep_to_penetration_points,
            #     'timestep_to_penetration_points_forces': self.timestep_to_penetration_points_forces
            # }
            # np.save(cur_sv_penetration_points_fn, cur_sv_penetration_points)
            
            
            
            
            if i_iter % self.report_freq == 0:
                # print(self.base_exp_dir) # ### tot reg loss ###
                # print('iter:{:8>d} loss = {} loss_corr_tracking = {} tracking_loss = {} tot_loss_figner_tracking = {} tot_ragged_dist = {} tot_delta_offset_reg_motion = {} tot_dist_mano_visual_ori_to_cur = {} penetration_depth_penalty = {} penalty_dir = {} penalty_friction = {} reg_loss = {} diff_states = {} lr={}'.format(self.iter_step, tot_losses, tot_loss_corr_tracking,  tot_tracking_loss, tot_loss_figner_tracking, tot_ragged_dist, tot_delta_offset_reg_motion, tot_dist_mano_visual_ori_to_cur, tot_penetrating_depth_penalty, tot_penalty_dot_forces_normals, tot_penalty_friction_constraint, tot_reg_loss, tot_diff_cur_states_to_ref_states, self.optimizer.param_groups[0]['lr']))
                
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} loss_corr_tracking = {} tracking_loss = {} tot_loss_figner_tracking = {} tot_ragged_dist = {} tot_delta_offset_reg_motion = {} tot_dist_mano_visual_ori_to_cur = {} penetration_depth_penalty = {} penalty_dir = {} penalty_friction = {} reg_loss = {} diff_states = {} diff_tangential_forces = {} lr={}'.format(self.iter_step, tot_losses, tot_loss_corr_tracking,  tot_tracking_loss, tot_loss_figner_tracking, tot_ragged_dist, tot_delta_offset_reg_motion, tot_dist_mano_visual_ori_to_cur, tot_penetrating_depth_penalty, tot_penalty_dot_forces_normals, tot_penalty_friction_constraint, tot_reg_loss, tot_diff_cur_states_to_ref_states, tot_diff_tangential_forces, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                

            if i_iter % self.val_mesh_freq == 0:
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                # self.validate_mesh_expanded_pts() 
            # self.active_robot.clear_grads() 
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
    
    
    
    def train_real_robot_actions_from_mano_model_rules_v5_diffhand(self, ):
        
        ### the real robot actions from mano model rules ### # logging #
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load 
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path']
        robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        ''' Load robot hand in DiffHand simulator '''
        redmax_sim = redmax.Simulation(model_path)
        redmax_sim.reset(backward_flag = True) # redmax_sim -- 
        # ### redmax_ndof_u, redmax_ndof_r ### #
        redmax_ndof_u = redmax_sim.ndof_u
        redmax_ndof_r = redmax_sim.ndof_r
        redmax_ndof_m = redmax_sim.ndof_m ### ndof_m ### # redma # x_sim 
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano)  # robot
        self.mano_agent = mano_agent
        # ''' Load the mano hand '''
        
        
        ''' Expnad the current visual points ''' 
        expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        self.expanded_visual_pts_nn = expanded_visual_pts.size(0)
        expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        # ''' Expnad the current visual points '''  # 
        
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        ''' Define MANO robot actions, delta_states, init_states, frictions, and others '''
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=778 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_robot_actuator_friction_forces.parameters())
        
        
        self.mano_expanded_actuator_delta_offset = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_delta_offset.weight)
        # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
        
        
        self.mano_expanded_actuator_delta_offset_nex = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_delta_offset_nex.weight)
        # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
        
        
        
        self.mano_expanded_actuator_friction_forces = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        # params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        
        self.free_deformation_time_latent = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        )
        params_to_train += list(self.free_deformation_time_latent.parameters())
        
        
        self.deformation_input_dim = 3 + self.bending_latent_size
        self.deformation_output_dim = 3
        ### free deformation network ### # free deformation # 
        self.free_deformation_network = self.construct_field_network(self.deformation_input_dim, self.bending_latent_size,  self.deformation_output_dim)
        params_to_train += list(self.free_deformation_network.parameters())
        
        
        ''' Load optimized MANO hand actions and states '''
        # ### laod optimized init actions ####
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # cur_optimized_init_actions = 
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                self.mano_robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            self.mano_robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
            if 'expanded_actuator_friction_forces' in optimized_init_actions_ckpt:
                try:
                    self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_friction_forces'])
                except:
                    pass
            self.mano_expanded_actuator_delta_offset.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_delta_offset'])
        # ### load init transformations ckpts ### #
        
        # robot actions #
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        ## ##
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        ## robot actuator friction #
        # robot actuator friction forces #
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        params_to_train = []
        
        ''' Only set the robot friction forces optimizable '''
        if not self.optimize_rules:
            params_to_train += list(self.robot_actions.parameters())
            params_to_train += list(self.robot_delta_states.parameters())
            params_to_train += list(self.robot_init_states.parameters())
            params_to_train += list(self.robot_glb_rotation.parameters())
            params_to_train += list(self.robot_glb_trans.parameters())
        
        ''' Add params for expanded points '''
        params_to_train += list(self.mano_expanded_actuator_delta_offset_nex.parameters())
        params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        if self.optimize_rules:
            params_to_train += list(self.other_bending_network.parameters())
        
        
        ''' Scaling constants '''
        ### affine ###
        # self.maxx_robo_pts = 25. ##
        # self.minn_robo_pts = -15. ##
        # self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        # self.mult_const_after_cent = 0.5437551664260203
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        # maxx robo pts ##
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        ''' Load optimized robot hands '''
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_transformations_fn = self.conf['model.load_optimized_init_transformations']
            ##### Load optimized initial transformations #####
            optimized_init_transformations_ckpt = torch.load(cur_optimized_init_transformations_fn, map_location=self.device, )
            self.robot_init_states.load_state_dict(optimized_init_transformations_ckpt['robot_init_states'])
            self.robot_glb_rotation.load_state_dict(optimized_init_transformations_ckpt['robot_glb_rotation'])
            # if 'robot_delta_states' in optimized_init_transformations_ckpt:
            #     self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            self.robot_actions.load_state_dict(optimized_init_transformations_ckpt['robot_actions'])
            self.robot_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['robot_actuator_friction_forces'])
            self.robot_glb_trans.load_state_dict(optimized_init_transformations_ckpt['robot_glb_trans'])
            
            if not self.load_only_glb:
                self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            
        if 'model.load_optimized_expanded' in self.conf and len(self.conf['model.load_optimized_expanded']) > 0:
            load_optimized_expanded_ckpt = self.conf['model.load_optimized_expanded']
            load_optimized_expanded_ckpt = torch.load(load_optimized_expanded_ckpt, map_location=self.device)
            self.mano_expanded_actuator_delta_offset.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset'])
            self.mano_expanded_actuator_friction_forces.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_friction_forces'])
            if 'mano_expanded_actuator_delta_offset_nex' in load_optimized_expanded_ckpt:
                print(f"Loading expanded actuator delta offset nex from the saved checkpoint...")
                self.mano_expanded_actuator_delta_offset_nex.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset_nex'])
        
        
        
        # robot_glb_rotation, robot_glb_trans #
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        
        
        # # train the robot or train the expanded points #
        # self.mano_fingers = [745, 279, 320, 444, 555, 672, 234, 121]
        # self.robot_fingers = [521, 624, 846, 973, 606, 459, 383, 265]
        
        # # optimize with intermediates #
        # self.mano_fingers = torch.tensor(self.mano_fingers, dtype=torch.long).cuda()
        # self.robot_fingers = torch.tensor(self.robot_fingers, dtype=torch.long).cuda()
        
        self.nn_ts = self.nn_timesteps - 1
        # self.optimize_with_intermediates = False
        
        # search for the nearest point so that it can manipulate the object #
        ## TODO: co-optimize the bending network weights? ## # optimize active object is true ##
        # if self.optimize_with_intermediates and (not self.optimize_active_object): # optimize with intermediates #
        #     # if not self.optimize_active_object: #
        #     # free_deformation_time_latent, free_deformation_network #
        #     params_to_train = []
        #     params_to_train += list(self.other_bending_network.parameters())
        #     params_to_train += list(self.robot_actuator_friction_forces.parameters())
        #     # params_to_train += list(self.expanded_actuator_friction_forces.parameters()) # expanded actuator 
        
        ''' Set actions for the redmax simulation and add parameters to params-to-train '''
        # robot actions #
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        
        self.redmax_robot_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        
        params_to_train = []
        params_to_train += list(self.redmax_robot_actions.parameters())


        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        # init_rot = R.random().as_quat()
        # init_rot = torch.from_numpy(init_rot).float().cuda()
        # self.robot_glb_rotation.weight.data[0, :] = init_rot[:] # init rot
        
        
        
        ### Constraint set ###
        self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        self.timestep_to_active_mesh = {}
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts #
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        
        # states -> get states -> only update the acitons #
        with torch.no_grad():
            for cur_ts in range(self.nn_ts):
                ''' Get rotations, translations, and actions of the current robot '''
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                else:
                    if self.conf['model.train_states']:
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    else:
                        actions = {}
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        actions['link_actions'] = actions_link_actions
                        self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        
                    if cur_ts == 10:
                        cur_visual_pts, link_name_to_transformations_and_transformed_pts = self.robot_agent.get_init_state_visual_pts(ret_link_name_to_tansformations=True)
                    else:
                        cur_visual_pts = self.robot_agent.get_init_state_visual_pts()

                ### transform the visual pts ###
                cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_visual_pts = cur_visual_pts * 2. -1.
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                if not self.use_mano_inputs:
                    self.timestep_to_active_mesh[cur_ts] = cur_visual_pts.detach()
                # timestep_to_active_mesh_opt_ours_sim #
                self.timestep_to_active_mesh_opt_ours_sim[cur_ts] = cur_visual_pts[sampled_verts_idxes].detach()
                
                
        if self.train_actions_with_states: # 
            for cur_ts in range(self.nn_ts):
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                else:
                    # if self.conf['model.train_states']: # 
                    cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    states = {}
                    states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    states['link_states'] = cur_state
                    self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)      
            tmp_joint_nm_to_ts_to_states = self.robot_agent.get_timestep_to_states()
            joint_nm_to_ts_to_states = {}
            for cur_joint_nm in tmp_joint_nm_to_ts_to_states:
                cur_joint_ts_to_states = {}
                for cur_ts in tmp_joint_nm_to_ts_to_states[cur_joint_nm]:
                    cur_joint_ts_to_states[cur_ts] = tmp_joint_nm_to_ts_to_states[cur_joint_nm][cur_ts].detach().clone()
                joint_nm_to_ts_to_states[cur_joint_nm] = cur_joint_ts_to_states
    

        
        ''' optimiz the delta states '''
        # nn_iters_opt_delta_states = 100
        # params_to_train_delta_states = []
        # params_to_train_delta_states += list(self.robot_delta_states.parameters())
        # self.delta_states_optimizer = torch.optim.Adam(params_to_train_delta_states, lr=self.learning_rate)
        
        # for i_iter in tqdm(range(nn_iters_opt_delta_states)):
        #     tot_diff_visual_pts = []
        #     cur_ts_to_optimized_visual_pts = {}
        #     for cur_ts in range(self.nn_ts):
        #         cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
        #         cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
        #         cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
        #         cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
        #         if cur_ts == 0:
        #             links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
        #             self.robot_agent.set_init_states_target_value(links_init_states)
        #             cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
        #         else:
        #             # if self.conf['model.train_states']: # 
        #             cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
        #             states = {}
        #             states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
        #             states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
        #             states['link_states'] = cur_state
        #             self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)    
        #             cur_visual_pts = self.robot_agent.get_init_state_visual_pts()

        #         ### transform the visual pts ###
        #         cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
        #         cur_visual_pts = cur_visual_pts * 2. -1.
        #         cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                
        #         cur_rot = cur_glb_rot
        #         cur_trans = cur_glb_trans
                
        #         ### transform by the glboal transformation and the translation ###
        #         cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) 
        #         diff_visual_pts = torch.sum(
        #             (cur_visual_pts[sampled_verts_idxes] - self.timestep_to_active_mesh_opt_ours_sim[cur_ts].detach()) ** 2, dim=-1
        #         )
        #         diff_visual_pts = diff_visual_pts.mean()
            
        #         self.delta_states_optimizer.zero_grad()
        #         diff_visual_pts.backward()
        #         self.delta_states_optimizer.step()
        #         tot_diff_visual_pts.append(diff_visual_pts.detach().item())
                
                
        #         cur_ts_to_optimized_visual_pts[cur_ts] = cur_visual_pts[sampled_verts_idxes].detach().cpu().numpy()
            
        #     self.cur_ts_to_optimized_visual_pts = cur_ts_to_optimized_visual_pts
                
        #     tot_diff_visual_pts = sum(tot_diff_visual_pts) / float(len(tot_diff_visual_pts))
        #     cur_log_sv_str = 'iter:{:8>d} loss = {} lr={}'.format(i_iter, tot_diff_visual_pts, self.delta_states_optimizer.param_groups[0]['lr'])
        #     # get states and then use the obtianed states for actions optim?
        #     print(cur_log_sv_str)
            
        #     self.iter_step = i_iter
            
        #     if i_iter % 10 == 0:
        #         self.validate_mesh_robo_b()
        #         self.save_checkpoint_delta_states()
        
        
        self.iter_step = 0
        
        load_delta_states_fn = "/data3/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_/checkpoints/robo_delta_states_ckpt_000090.pth"
        if len(load_delta_states_fn) > 0:
            delta_states_ckpt = torch.load(load_delta_states_fn, map_location=self.device, )
            self.robot_delta_states.load_state_dict(delta_states_ckpt['robot_delta_states'])
        # self.

        robo_states = []
        for cur_ts in range(self.nn_ts):
            if cur_ts == 0:
                cur_state = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()[:redmax_ndof_r]
            else:
                cur_delta_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0).detach().cpu().numpy()[:redmax_ndof_r]
                cur_state = robo_states[-1] + cur_delta_state
            # if cur_ts > 0:
            robo_states.append(cur_state)
        robo_states = np.stack(robo_states[1:], axis=0) # nn_states_to_optimize x ndof_r
        
        
        ''' Set redmax robot actions '''
        # nn_iters_opt_redmax_actions = 10000
        # nn_iters_opt_redmax_actions = 100
        # nn_iters_opt_redmax_actions = 17
        nn_substeps = 10
        params_to_train_actions = []
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps * nn_substeps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        params_to_train_actions += list(self.redmax_robot_actions.parameters())
        
        self.actions_optimizer = torch.optim.Adam(params_to_train_actions, lr=self.learning_rate)
        
        params_to_train_kines = []
        params_to_train_kines += list(self.robot_glb_rotation.parameters())
        params_to_train_kines += list(self.robot_glb_trans.parameters())
        self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        
        
        load_redmax_robot_actions_fn = "/data3/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_diffh_subs_10_/checkpoints/redmax_robot_actions_ckpt_001900.pth"
        if len(load_redmax_robot_actions_fn) > 0:
            redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
            self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        
        ''' Optimize redmax robot actions '''
        # for i_iter in tqdm(range(nn_iters_opt_redmax_actions)):
        #     redmax_sim.reset(backward_flag = True) 
        #     robo_intermediates_states = []
        #     tot_redmax_actions = []
        #     tot_visual_pts = []
        #     df_dqs = []
        #     tot_diff_qs = []
        #     for cur_ts in range(self.nn_ts):
        #         cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
        #         cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
        #         cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
        #         cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
        #         if cur_ts == 0:
        #             robot_init_state = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
        #             redmax_q_init = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
        #             redmax_q_init = redmax_q_init[:redmax_ndof_r]
        #             redmax_sim.set_q_init(redmax_q_init)
                    
        #             if i_iter % 100 == 0 or (i_iter == nn_iters_opt_redmax_actions - 1):
        #                 links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
        #                 self.robot_agent.set_init_states_target_value(links_init_states)
        #                 cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
        #         else:
                    
        #             for i_sub_step in range(nn_substeps):
        #                 cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
        #                 redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
        #                 redmax_u = redmax_actions.detach().cpu().numpy()
        #                 redmax_u = redmax_u[: redmax_ndof_u]
        #                 redmax_sim.set_u(redmax_u) # sim
        #                 tot_redmax_actions.append(redmax_actions)
        #                 ''' set and set the virtual forces ''' ## redmax_sim
        #                 virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
        #                 redmax_sim.set_forces(virtual_force) ### virtual forces ###

        #                 redmax_sim.forward(1, verbose = False)
        #                 redmax_sim_q = redmax_sim.get_q()
        #                 if i_sub_step == nn_substeps - 1:
        #                     robo_intermediates_states.append(redmax_sim_q)
        #                     df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts - 1]))
        #                     cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts - 1]) ** 2)
        #                     tot_diff_qs.append(cur_diff_qs.item())
        #                 else:
        #                     df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                    
        #             if i_iter % 100 == 0 or (i_iter == nn_iters_opt_redmax_actions - 1):
        #                 cur_delta_state = redmax_sim_q - (robo_intermediates_states[-1 - 1] if cur_ts > 1 else redmax_q_init)
        #                 cur_delta_state = torch.from_numpy(cur_delta_state).float().cuda()
        #                 states = {}
        #                 states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
        #                 states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
        #                 states['link_states'] = cur_delta_state
                        
        #                 self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)    
        #                 cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
        #         if i_iter % 100 == 0 or (i_iter == nn_iters_opt_redmax_actions - 1):
        #             ### transform the visual pts ###
        #             cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
        #             cur_visual_pts = cur_visual_pts * 2. -1.
        #             cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                     
        #             cur_rot = cur_glb_rot
        #             cur_trans = cur_glb_trans
                    
        #             ### transform by the glboal transformation and the translation ###
        #             cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) 
                    
        #             tot_visual_pts.append(cur_visual_pts[sampled_verts_idxes].detach().cpu().numpy())
                    
        #     # robo_intermediates_states = np.stack(robo_intermediates_states, axis=0) # nn_timesteps x ndof_r #
        #     # diff_qs = np.sum(
        #     #      (robo_intermediates_states - robo_states) ** 2, axis=-1
        #     # )
        #     # diff_qs = np.mean(diff_qs)
        #     # df_dq = 2 * (robo_intermediates_states - robo_states)
            
        #     df_dq = np.stack(df_dqs, axis=0)
        #     nn_forward_ts = df_dq.shape[0]
        #     df_dq = np.reshape(df_dq, (-1,))
        #     df_du = np.zeros((nn_forward_ts * redmax_ndof_u,))
            
            
        #     redmax_sim.backward_info.set_flags(False, False, False, True) 
        #     # nn_forward_ts = len(tot_grad_qs)
        #     # tot_grad_qs = np.concatenate(tot_grad_qs, axis=0)
        #     redmax_sim.backward_info.df_dq = df_dq
        #     redmax_sim.backward_info.df_du = df_du
        #     redmax_sim.backward()
        #     # redmax
        #     redmax_sim_robot_action_grad = np.copy(redmax_sim.backward_results.df_du)
        #     redmax_sim_robot_action_grad_th = torch.from_numpy(redmax_sim_robot_action_grad).float().cuda()
        #     redmax_sim_robot_action_grad_th = redmax_sim_robot_action_grad_th.contiguous().view(nn_forward_ts, -1).contiguous()
            
        #     tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) ## nn_forward_steps x nn_us_dim # 
        #     surrogate_loss = torch.sum(
        #         tot_redmax_actions[:, :redmax_ndof_u] * redmax_sim_robot_action_grad_th, dim=-1
        #     )
        #     surrogate_loss = surrogate_loss.mean()
            
        #     self.actions_optimizer.zero_grad()
        #     surrogate_loss.backward()
        #     self.actions_optimizer.step()
            
        #     diff_qs = sum(tot_diff_qs) / float(len(tot_diff_qs))
        #     cur_log_sv_str = 'iter:{:8>d} diff_qs = {} surrogate_loss = {} lr={}'.format(i_iter, diff_qs, surrogate_loss.item(), self.actions_optimizer.param_groups[0]['lr'])
        #     # get states and then use the obtianed states for actions optim?
        #     print(cur_log_sv_str)
            
            
            
        #     self.iter_step = i_iter
        #     if i_iter % 100 == 0 or (i_iter == nn_iters_opt_redmax_actions - 1):
        #         tot_visual_pts = np.stack(tot_visual_pts, axis=0)
        #         self.tot_visual_pts = tot_visual_pts
        #         self.validate_mesh_robo_c()
        #         self.save_checkpoint_redmax_robot_actions()
                    

        ''' Load optimized redmax robot actions '''
        # load_redmax_robot_actions_fn = "/data3/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_diffh_subs_10_/checkpoints/redmax_robot_actions_ckpt_001900.pth"
        # if len(load_redmax_robot_actions_fn) > 0:
        #     redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
        #     self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        # # self.


        # model_path = self.conf['model.sim_model_path']
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        # self.robot_agent = robot_agent
        
        # delta states -> 
        
        # the simulator env?  #
        ### load optimized actions ###
        if 'model.load_redmax_robot_actions' in self.conf and len(self.conf['model.load_redmax_robot_actions']) > 0: 
            load_redmax_robot_actions_fn = self.conf['model.load_redmax_robot_actions']
            redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
            self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        
        # robot_glb_rotation, robot_glb_trans #
        # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
        ori_redmax_robot_actions = self.redmax_robot_actions.weight.data.clone()    
        ori_redmax_glb_rot = self.robot_glb_rotation.weight.data.clone()
        ori_redmax_glb_trans = self.robot_glb_trans.weight.data.clone()
        
        
        self.iter_step = 0
        # penetration_forces = None
        self.validate_mesh_robo_a()
        
        
        for i_iter in tqdm(range(100000)):
            tot_losses = []
            # tot_penalty_dot_forces_normals = []
            # tot_penalty_friction_constraint = []
            # tot_loss_figner_tracking = []
            # tot_loss_corr_tracking = []
            tot_tracking_loss = []
            
            # timestep 
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # self.timestep_to_
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            correspondence_pts_idxes = None
            
            # tot_penetrating_depth_penalty = []
            # tot_ragged_dist = []
            # tot_delta_offset_reg_motion = []
            # tot_dist_mano_visual_ori_to_cur = []
            # tot_reg_loss = []
            # tot_diff_cur_states_to_ref_states = []
            # tot_diff_tangential_forces = []
            # penetration_forces = None
            # sampled_visual_pts_joint_idxes = None
            
            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_force_d = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            self.ts_to_redmax_states = {}
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16]
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            
            redmax_sim.reset(backward_flag = True)
            
            tot_grad_qs = []
            
            robo_intermediates_states = []
            
            tot_penetration_depth = []
            
            robo_actions_diff_loss = []
            
            # init global transformations ##
            # cur_ts_redmax_delta_rotations = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_delta_rotations = torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda()
            
            for cur_ts in range(self.nn_ts):
                tot_redmax_actions = []
                
                # redmax_robot_states # # # #
                self.redmax_robot_states.weight.data[:, :] = 0.
                if self.redmax_robot_states.weight.grad is not None:
                    self.redmax_robot_states.weight.grad.data[:, :] =  self.redmax_robot_states.weight.grad.data[:, :] * 0.
                    
                actions = {}
                
                self.free_def_bending_weight = 0.0
           
                
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                cur_glb_rot = cur_glb_rot + cur_ts_redmax_delta_rotations
                
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                # cur_glb_rot_quat = cur_glb_rot.clone()
                
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                # cur_ts_redmax_delta_rotations, cur_ts_redmax_robot_trans # 
                # cur_ts_delta_rot, cur_ts_redmax_robot_trans #
                # # print(f"ts: {cur_ts}, cur_ts_redmax_delta_rotations: {cur_ts_redmax_delta_rotations}, cur_glb_rot: {cur_glb_rot_quat}")
                # cur_ts_delta_rot = dyn_model_act.quaternion_to_matrix(cur_ts_redmax_delta_rotations) # mano glboa
                # # cur_ts_delta_trans = 
                
                # # # cur_ts_delta_rot, cur_ts_redmax_robot_trans # # #
                # cur_glb_rot = torch.matmul(cur_ts_delta_rot, cur_glb_rot)
                cur_glb_trans = cur_glb_trans + cur_ts_redmax_robot_trans # 
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts, link_name_to_link_transformations = self.robot_agent.get_init_state_visual_pts(True)
                    
                    ''' Set init q of the redmax '''
                    # ### redmax_ndof_u, redmax_ndof_r ### #
                    redmax_q_init = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
                    redmax_q_init = redmax_q_init[:redmax_ndof_r]
                    
                    # 
                    redmax_sim.set_q_init(redmax_q_init)
                    
                    redmax_q_init_th = torch.from_numpy(redmax_q_init).float().cuda()
                    self.redmax_robot_states.weight.data[cur_ts, :redmax_q_init_th.size(0)] = redmax_q_init_th[:]
                    
                    self.ts_to_redmax_states[cur_ts] = redmax_q_init.copy()
                    
                else:
                    if self.conf['model.train_states']:
                        if self.add_delta_state_constraints:
                            self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg] = torch.clamp(self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg], max=0.)
                        
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        # self.redmax_robot_states.weight.data[0, :cur_state.]
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    else:
                        for i_sub_step in range(nn_substeps):
                            cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
                            redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
                            redmax_u = redmax_actions.detach().cpu().numpy()
                            redmax_u = redmax_u[: redmax_ndof_u]
                            redmax_sim.set_u(redmax_u) # sim
                            tot_redmax_actions.append(redmax_actions)
                            ''' set and set the virtual forces ''' ## redmax_sim
                            ###  TODO: get and set the virtual forces ###
                            # link_maximal_contact_forces # 
                            ### TODO: add the virtual force objects and set the virtual forces ###
                            # virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                            
                            ''' Set virtual forces ''' ### set the 
                            virtual_force = link_maximal_contact_forces.detach().cpu().numpy()
                            virtual_force = np.reshape(virtual_force, (virtual_force.shape[0] * virtual_force.shape[1]))
                            # # virtual forces # #
                            redmax_sim.set_forces(virtual_force) ### virtual forces ###

                            redmax_sim.forward(1, verbose = False)
                            # 
                            
                            # redmax_sim_q = redmax_sim.get_q()
                            # if i_sub_step == nn_substeps - 1:
                            #     robo_intermediates_states.append(redmax_sim_q)
                            #     df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts - 1]))
                            #     cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts - 1]) ** 2)
                            #     tot_diff_qs.append(cur_diff_qs.item())
                            # else:
                            #     df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                        redmax_sim_q = redmax_sim.get_q()
                        robo_intermediates_states.append(redmax_sim_q)
                        
                        # print(f"self.redmax_robot_states.weight: {self.redmax_robot_states.weight.data.size()}")
                        # self.redmax_robot_states.weight[cur_ts, :redmax_sim_q.shape[0]] = torch.from_numpy(redmax_sim_q).float().cuda()[:]
                        
                        
                        # self.redmax_robot_states.weight.data[cur_ts, :redmax_sim_q.shape[0]] = self.redmax_robot_states.weight.data[cur_ts, :redmax_sim_q.shape[0]] * 0. + torch.from_numpy(redmax_sim_q).float().cuda()[:]
                        
                        # cur_delta_state = self.redmax_robot_states(torch.zeros((1,)).long().cuda() + cur_ts) - self.redmax_robot_states(torch.zeros((1,)).long().cuda() + cur_ts - 1).detach()
                        
                        redmax_robot_states_th = torch.from_numpy(redmax_sim_q).float().cuda()
                        redmax_robot_states_th.requires_grad = True
                        redmax_robot_states_th.requires_grad_ = True
                        cur_delta_state = redmax_robot_states_th - (torch.from_numpy(robo_intermediates_states[-1 - 1]).float().cuda() if cur_ts > 1 else torch.from_numpy(redmax_q_init).float().cuda())
                        
                        
                        
                        
                        # self.redmax_robot_states 
                        # self.redmax_robot_states
                        # self.redma
                        # self
                        
                        
                        # cur_delta_state = redmax_sim_q - (robo_intermediates_states[-1 - 1] if cur_ts > 1 else redmax_q_init)
                        # cur_delta_state = torch.from_numpy(cur_delta_state).float().cuda()
                        
                        
                        cur_delta_state = cur_delta_state.squeeze(0)[:redmax_ndof_r]
                        # cur_delta_state = redmax_sim_q - (robo_intermediates_states[-1 - 1] if cur_ts > 1 else redmax_q_init)
                        # cur_delta_state = torch.from_numpy(cur_delta_state).float().cuda()
                        
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_delta_state
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)    
                        # cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                        
                        # # if i_iter % 100 == 0:
                        # #     cur_delta_state = redmax_sim_q - (robo_intermediates_states[-1 - 1] if cur_ts > 1 else redmax_q_init)
                        # #     cur_delta_state = torch.from_numpy(cur_delta_state).float().cuda()
                        # #     states = {}
                        # #     states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        # #     states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        # #     states['link_states'] = cur_delta_state
                            
                        # #     self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)    
                        # #     cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                        
                        
                        # # print(f"Draing via actions")
                        # actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        # actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        # actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        # actions['link_actions'] = actions_link_actions
                        
                        
                        # ''' set action and get states from the redmax simulator '''
                        # redmax_u = actions_link_actions.detach().cpu().numpy()
                        
                        # # redmax_robot_actions
                        # redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        # redmax_u = redmax_actions.detach().cpu().numpy()
                        # redmax_u = redmax_u[: redmax_ndof_u]
                        # redmax_sim.set_u(redmax_u) # sim
                        
                        # tot_redmax_actions.append(redmax_actions)
                        
                        # ''' set and set the virtual forces ''' ## redmax_sim
                        # virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32)
                        # redmax_sim.set_forces(virtual_force) ### virtual forces ###
                        
                        # redmax_sim.forward(1, verbose = False)
                        # redmax_sim_q = redmax_sim.get_q()
                        
                        # # redmax_ndof_m = redmax_sim.ndof_m #
                        # self.ts_to_redmax_states[cur_ts] = redmax_sim_q.copy() 
                        
                        # redmax_sim_q_th = torch.from_numpy(redmax_sim_q).float().cuda()
                        # self.redmax_robot_states.weight.data[cur_ts, :redmax_sim_q_th.size(0)] = redmax_sim_q_th[:]
                        
                        # cur_sim_delta_states_th = self.redmax_robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts) - self.redmax_robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts - 1).detach() # tdelta exc
                        # cur_sim_delta_states_th = cur_sim_delta_states_th.squeeze(0)
                        # states = {}
                        # states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        # states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        # states['link_states'] = cur_sim_delta_states_th # set delta sates
                        # self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                        
                    cur_visual_pts, link_name_to_link_transformations, visual_pts_joint_idxes = self.robot_agent.get_init_state_visual_pts(True, True)

                # diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                ### transform the visual pts ###
                cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_visual_pts = cur_visual_pts * 2. - 1.
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                ## 
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) 
                
                
                
                diff_redmax_visual_pts_with_ori_visual_pts = torch.sum(
                    (cur_visual_pts[sampled_verts_idxes] - self.timestep_to_active_mesh_opt_ours_sim[cur_ts].detach()) ** 2, dim=-1
                )
                diff_redmax_visual_pts_with_ori_visual_pts = diff_redmax_visual_pts_with_ori_visual_pts.mean()
                
                
                self.timestep_to_active_mesh[cur_ts] = cur_visual_pts
                self.timestep_to_raw_active_meshes[cur_ts] = cur_visual_pts.detach().cpu().numpy()
                
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                # diff_robo_to_corr_mano_pts =  torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                
                contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=sampled_verts_idxes, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)

                
                if self.train_with_forces_to_active and (not self.use_mano_inputs):
                    # penetration_forces #
                    if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                        net_penetrating_forces = self.other_bending_network.penetrating_forces
                        net_penetrating_points = self.other_bending_network.penetrating_points
                        # cur_penetrating_forces = []
                        # for i_pts in range(net_penetrating_points.size(0)):
                        #     cur_penetration_point = net_penetrating_points[i_pts]
                        #     cur_penetration_point_forces = net_penetrating_forces[i_pts]
                        #     cur_penetrating_forces.append((cur_penetration_point, cur_penetration_point_forces))
                        # penetration_forces = cur_penetrating_forces
                        
                        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                        self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                        self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                        
                        
                        ### transform the visual pts ###
                        # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        # cur_visual_pts = cur_visual_pts * 2. - 1.
                        # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                        
                        sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[sampled_verts_idxes][self.other_bending_network.penetrating_indicator]
                        # sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[sampled_verts_idxes]
                        
                        
                        net_penetrating_forces = torch.matmul(
                            cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                        net_penetrating_forces = net_penetrating_forces / 2
                        net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                        
                        net_penetrating_points = torch.matmul(
                            cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                        net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                        net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                        
                        
                        penetration_forces = {
                            'penetration_forces': net_penetrating_forces, 'penetration_forces_points': net_penetrating_points
                        }
                        
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        # self.robot_agent.set_maximal_contact_forces(link_maximal_contact_forces)
                        self.robot_agent.set_penetration_forces(penetration_forces, sampled_visual_pts_joint_idxes, link_maximal_contact_forces)
                        
                    else:
                        penetration_forces = None
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        
                if contact_pairs_set is not None:
                    self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                
                self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                if self.other_bending_network.penalty_based_friction_forces is not None:
                    self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                
                # self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                tot_penetration_depth.append(self.other_bending_network.penetrating_depth_penalty.detach().item())
                
                ### optimize with intermediates ### 
                if self.optimize_with_intermediates:
                    tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1) # 
                else:
                    tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()

                tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                
                loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                # diff_redmax_visual_pts_with_ori_visual_pts.backward()
                penetraton_penalty = self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                
                # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
                robot_rotation_diff = torch.sum(
                    (ori_redmax_glb_rot - self.robot_glb_rotation.weight) ** 2, dim=-1
                )
                robot_rotation_diff = robot_rotation_diff.mean()
                
                robot_trans_diff = torch.sum(
                    (ori_redmax_glb_trans - self.robot_glb_trans.weight) ** 2, dim=-1
                )
                robot_trans_diff = robot_trans_diff.mean()
                
                
                kinematics_trans_diff = (robot_rotation_diff + robot_trans_diff) * self.robot_actions_diff_coef
                
                kinematics_proj_loss = kinematics_trans_diff + penetraton_penalty
                
                loss = loss # + (robot_rotation_diff + robot_trans_diff) * self.robot_actions_diff_coef
                
                
                self.kines_optimizer.zero_grad()
                
                kinematics_proj_loss.backward(retain_graph=True)
                
                
                
                # 
                # loss.backward()
                # repulsion to the obbject transformation related matrices #
                # delta rotation? # R \Delta R ^T
                # \Delta R \times R -> the updated rotation matrix #
                
                
                # cur_ts_redmax_delta_rotations, cur_ts_redmax_robot_trans # 
                # ## TODO: 
                proj_lr = 0.1
                proj_lr = 0.001
                proj_lr = 0.01
                proj_lr = 0.1
                proj_lr = 0.05
                proj_lr = 0.03
                proj_lr = 0.0
                # proj_lr = 0.0001
                # proj_lr = 0.00001
                # proj_lr = 0.000001
                # proj_lr = 0.00000
                redmax_robot_rotation_grad = self.robot_glb_rotation.weight.grad
                if redmax_robot_rotation_grad is not None:
                    cur_ts_redmax_delta_rotations = -1. * redmax_robot_rotation_grad[cur_ts] ### (4- dim vector # -> need to treat it as a rotation matrix)
                    if torch.norm(cur_ts_redmax_delta_rotations, p=2, dim=-1).item() < 1e-6:
                        # cur_ts_redmax_delta_rotations =  torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda() # 
                        cur_ts_redmax_delta_rotations =  torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda() # 
                    else:
                        # cur_ts_redmax_delta_rotations = cur_ts_redmax_delta_rotations / torch.clamp(torch.norm(cur_ts_redmax_delta_rotations, p=2, dim=-1, keepdim=True), min=1e-6) # another rotation component that should be added to the next rotation matrix #
                        cur_ts_redmax_delta_rotations = proj_lr * cur_ts_redmax_delta_rotations.clone()
                else:
                    # cur_ts_redmax_delta_rotations = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda() # 
                    cur_ts_redmax_delta_rotations = torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda() # 
                
                redmax_robot_trans_grad = self.robot_glb_trans.weight.grad
                if redmax_robot_trans_grad is not None:
                    # cur_ts_redmax_robot_trans = -1. * redmax_robot_trans_grad[cur_ts] # another translation vector that should be added to the next translation vector
                    cur_ts_redmax_robot_trans = -1. * proj_lr * redmax_robot_trans_grad[cur_ts].clone()
                    # cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda() ## 
                else:
                    cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda() ## 
                
                
                self.kines_optimizer.step()
                
                tracking_loss.backward(retain_graph=True)
                
                
                
                # redmax_sim.forward(1, verbose = False)
                # redmax_sim_q = redmax_sim.get_q() # 
                
                ''' Record actions grads ''' 
                # if cur_ts > 0:
                #     # tot_grad_qs
                #     redmax_robot_states_grad = self.redmax_robot_states.weight.grad[cur_ts]
                #     redmax_robot_states_grad_np = redmax_robot_states_grad.detach().cpu().numpy()
                #     tot_grad_qs.append(redmax_robot_states_grad_np[:redmax_ndof_r])
                    
                
                ''' Update actions in each timestep '''
                if cur_ts > 0:
                    # print(f"cur_ts: {cur_ts}")
                    
                    # redmax_robot_states_grad = self.redmax_robot_states.weight.grad[cur_ts]
                    
                    redmax_robot_states_grad = redmax_robot_states_th.grad.data
                    redmax_robot_states_grad_np = redmax_robot_states_grad.detach().cpu().numpy()
                    redmax_sim.backward_info.set_flags(False, False, False, True) 
                    redmax_robot_states_grad_np_tot = np.zeros((cur_ts * nn_substeps * redmax_ndof_r,), dtype=np.float32)
                    redmax_robot_states_grad_np_tot[-redmax_ndof_r:] = redmax_robot_states_grad_np[:redmax_ndof_r]
                    # redmax_robot_states_grad_np_tot = np.concatenate(
                    #     [np.zeros((redmax_ndof_r * (cur_ts - 1),), dtype=np.float32), redmax_robot_states_grad_np[:redmax_ndof_r]], axis=0
                    # )
                    # print(f"redmax_robot_states_grad_np_tot: {redmax_robot_states_grad_np_tot.shape}")
                    redmax_sim.backward_info.df_dq = redmax_robot_states_grad_np_tot
                    redmax_sim.backward_info.df_du = np.zeros(redmax_ndof_u * cur_ts * nn_substeps) 
                    # print(f"Start backarding")
                    redmax_sim.backward()
                    # print(f"After backawarding")
                    redmax_sim_robot_action_grad = np.copy(redmax_sim.backward_results.df_du)
                    # redmax_sim #
                    redmax_sim_robot_action_grad_th = torch.from_numpy(redmax_sim_robot_action_grad).float().cuda()[ -redmax_ndof_u * nn_substeps: ]
                    
                    tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) ## nn_forward_steps x nn_us_dim #
                    redmax_sim_robot_action_grad_th = redmax_sim_robot_action_grad_th.contiguous().view(nn_substeps, -1).contiguous()
                    surrogate_loss = torch.sum(
                        tot_redmax_actions[:, :redmax_sim_robot_action_grad_th.size(1)] * redmax_sim_robot_action_grad_th, dim=-1
                    )
                    
                    surrogate_loss = surrogate_loss.mean()
                    
                    ''' get states and actions differences '''
                    robot_actions_diff = torch.sum(
                        (ori_redmax_robot_actions - self.redmax_robot_actions.weight) ** 2, dim=-1
                    )
                    robot_actions_diff = robot_actions_diff.mean()
                    
                    
                    
                    robot_states_actions_diff_loss = robot_actions_diff # + robot_rotation_diff + robot_trans_diff
                    
                    
                    robo_actions_diff_loss.append(robot_states_actions_diff_loss.item())
                    
                    surrogate_loss = surrogate_loss + robot_actions_diff * self.robot_actions_diff_coef
                    
                    
                    ## surrogate loss ? ##
                    # surrogate_loss = torch.sum(redmax_sim_robot_action_grad_th[:redmax_sim_robot_action_grad_th.size(0)] * redmax_sim_q_th[:redmax_sim_robot_action_grad_th.size(0)])
                    
                    # loss = actions * actions_grad --> \partial loss / \partial actions = actions_grad -> per-element actions grad for the acjtions 
                    # surrogate_loss = torch.sum(redmax_actions[:redmax_sim_robot_action_grad_th.size(0)] * redmax_sim_robot_action_grad_th[:redmax_sim_robot_action_grad_th.size(0)])
                    
                    self.actions_optimizer.zero_grad()
                    surrogate_loss.backward()
                    self.actions_optimizer.step()
                    # 
                    
                
                tot_losses.append(loss.detach().item())
                # tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                # tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                self.update_learning_rate() ## update learning rate ##
                
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                
            
            ''' Get nn_forward_ts and backward through the actions for updating '''
            # redmax_sim.backward_info.set_flags(False, False, False, True) 
            # nn_forward_ts = len(tot_grad_qs)
            # tot_grad_qs = np.concatenate(tot_grad_qs, axis=0)
            # redmax_sim.backward_info.df_dq = tot_grad_qs
            # redmax_sim.backward_info.df_du = np.zeros(redmax_ndof_u * nn_forward_ts) 
            # redmax_sim.backward()
            # redmax_sim_robot_action_grad = np.copy(redmax_sim.backward_results.df_du)
            # redmax_sim_robot_action_grad_th = torch.from_numpy(redmax_sim_robot_action_grad).float().cuda()
            # redmax_sim_robot_action_grad_th = redmax_sim_robot_action_grad_th.contiguous().view(nn_forward_ts, -1).contiguous()
            
            # tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) ## nn_forward_steps x nn_us_dim # 
            # surrogate_loss = torch.sum(
            #     tot_redmax_actions[:, :redmax_ndof_u] * redmax_sim_robot_action_grad_th, dim=-1
            # )
            # surrogate_loss = surrogate_loss.mean()
            
            # self.optimizer.zero_grad()
            # surrogate_loss.backward()
            # self.optimizer.step()
            
            
            
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            # tot_penalty_dot_forces_normals = sum(tot_penalty_dot_forces_normals) / float(len(tot_penalty_dot_forces_normals))
            # tot_penalty_friction_constraint = sum(tot_penalty_friction_constraint) / float(len(tot_penalty_friction_constraint))
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            tot_penetration_depth = sum(tot_penetration_depth) / float(len(tot_penetration_depth))
            robo_actions_diff_loss = sum(robo_actions_diff_loss) / float(len(robo_actions_diff_loss))
            
            
            if i_iter % self.report_freq == 0:
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} tracking_loss = {} penetration_depth = {} actions_diff_loss = {} lr={}'.format(self.iter_step, tot_losses, tot_tracking_loss, tot_penetration_depth, robo_actions_diff_loss, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                ''' Dump to the file '''
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                
            robo_intermediates_states = np.stack(robo_intermediates_states, axis=0) ### robot intermediate states ###
            robo_intermediates_states = torch.from_numpy(robo_intermediates_states).float()
            self.robo_intermediates_states = robo_intermediates_states    
            
            self.validate_mesh_robo_a()
            if i_iter % self.val_mesh_freq == 0:
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                
            
            
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
    
    
    def train_real_robot_actions_from_mano_model_rules_v5_diffhand_fortest(self, ):
        
        # chagne #
        ### the real robot actions from mano model rules ### # logging #
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load 
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path']
        robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        ''' Load robot hand in DiffHand simulator '''
        redmax_sim = redmax.Simulation(model_path)
        redmax_sim.reset(backward_flag = True) # redmax_sim -- 
        # ### redmax_ndof_u, redmax_ndof_r ### #
        redmax_ndof_u = redmax_sim.ndof_u
        redmax_ndof_r = redmax_sim.ndof_r
        redmax_ndof_m = redmax_sim.ndof_m ### ndof_m ### # redma # x_sim 
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano)  # robot
        self.mano_agent = mano_agent
        # ''' Load the mano hand '''
        
        
        ''' Expnad the current visual points ''' 
        expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        self.expanded_visual_pts_nn = expanded_visual_pts.size(0)
        expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        # ''' Expnad the current visual points '''  # 
        
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        ''' Define MANO robot actions, delta_states, init_states, frictions, and others '''
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=778 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_robot_actuator_friction_forces.parameters())
        
        
        self.mano_expanded_actuator_delta_offset = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_delta_offset.weight)
        # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
        
        
        self.mano_expanded_actuator_delta_offset_nex = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_delta_offset_nex.weight)
        # params_to_train += list(self.mano_expanded_actuator_delta_offset.parameters())
        
        
        
        self.mano_expanded_actuator_friction_forces = nn.Embedding(
            num_embeddings=self.expanded_visual_pts_nn * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_expanded_actuator_friction_forces.weight)
        # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        # params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        
        self.free_deformation_time_latent = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        )
        params_to_train += list(self.free_deformation_time_latent.parameters())
        
        
        self.deformation_input_dim = 3 + self.bending_latent_size
        self.deformation_output_dim = 3
        ### free deformation network ### # free deformation # 
        self.free_deformation_network = self.construct_field_network(self.deformation_input_dim, self.bending_latent_size,  self.deformation_output_dim)
        params_to_train += list(self.free_deformation_network.parameters())
        
        
        ''' Load optimized MANO hand actions and states '''
        # ### laod optimized init actions ####
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # cur_optimized_init_actions = # optimized init states 
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                self.mano_robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            self.mano_robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
            if 'expanded_actuator_friction_forces' in optimized_init_actions_ckpt:
                try:
                    self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_friction_forces'])
                except:
                    pass
            self.mano_expanded_actuator_delta_offset.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_delta_offset'])
        # ### load init transformations ckpts ### #
        
        # robot actions #
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        ## ##
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        ## robot actuator friction #
        # robot actuator friction forces #
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        params_to_train = []
        
        ''' Only set the robot friction forces optimizable '''
        if not self.optimize_rules:
            params_to_train += list(self.robot_actions.parameters())
            params_to_train += list(self.robot_delta_states.parameters())
            params_to_train += list(self.robot_init_states.parameters())
            params_to_train += list(self.robot_glb_rotation.parameters())
            params_to_train += list(self.robot_glb_trans.parameters())
        
        ''' Add params for expanded points '''
        params_to_train += list(self.mano_expanded_actuator_delta_offset_nex.parameters())
        params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        if self.optimize_rules:
            params_to_train += list(self.other_bending_network.parameters())
        
        
        ''' Scaling constants '''
        ### affine ###
        # self.maxx_robo_pts = 25. ##
        # self.minn_robo_pts = -15. ##
        # self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        # self.mult_const_after_cent = 0.5437551664260203
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        # maxx robo pts ##
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        ''' Load optimized robot hands '''
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_transformations_fn = self.conf['model.load_optimized_init_transformations']
            ##### Load optimized initial transformations #####
            optimized_init_transformations_ckpt = torch.load(cur_optimized_init_transformations_fn, map_location=self.device, )
            self.robot_init_states.load_state_dict(optimized_init_transformations_ckpt['robot_init_states'])
            self.robot_glb_rotation.load_state_dict(optimized_init_transformations_ckpt['robot_glb_rotation'])
            # if 'robot_delta_states' in optimized_init_transformations_ckpt:
            #     self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            self.robot_actions.load_state_dict(optimized_init_transformations_ckpt['robot_actions'])
            self.robot_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['robot_actuator_friction_forces'])
            self.robot_glb_trans.load_state_dict(optimized_init_transformations_ckpt['robot_glb_trans'])
            
            if not self.load_only_glb:
                self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            
        if 'model.load_optimized_expanded' in self.conf and len(self.conf['model.load_optimized_expanded']) > 0:
            load_optimized_expanded_ckpt = self.conf['model.load_optimized_expanded']
            load_optimized_expanded_ckpt = torch.load(load_optimized_expanded_ckpt, map_location=self.device)
            self.mano_expanded_actuator_delta_offset.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset'])
            self.mano_expanded_actuator_friction_forces.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_friction_forces'])
            if 'mano_expanded_actuator_delta_offset_nex' in load_optimized_expanded_ckpt:
                print(f"Loading expanded actuator delta offset nex from the saved checkpoint...")
                self.mano_expanded_actuator_delta_offset_nex.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset_nex'])
        
        
        
        # robot_glb_rotation, robot_glb_trans #
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        
        
        # # train the robot or train the expanded points #
        # self.mano_fingers = [745, 279, 320, 444, 555, 672, 234, 121]
        # self.robot_fingers = [521, 624, 846, 973, 606, 459, 383, 265]
        
        # # optimize with intermediates #
        # self.mano_fingers = torch.tensor(self.mano_fingers, dtype=torch.long).cuda()
        # self.robot_fingers = torch.tensor(self.robot_fingers, dtype=torch.long).cuda()
        
        self.nn_ts = self.nn_timesteps - 1
        # self.optimize_with_intermediates = False
        
        # search for the nearest point so that it can manipulate the object #
        ## TODO: co-optimize the bending network weights? ## # optimize active object is true ##
        # if self.optimize_with_intermediates and (not self.optimize_active_object): # optimize with intermediates #
        #     # if not self.optimize_active_object: #
        #     # free_deformation_time_latent, free_deformation_network #
        #     params_to_train = []
        #     params_to_train += list(self.other_bending_network.parameters())
        #     params_to_train += list(self.robot_actuator_friction_forces.parameters())
        #     # params_to_train += list(self.expanded_actuator_friction_forces.parameters()) # expanded actuator 
        
        ''' Set actions for the redmax simulation and add parameters to params-to-train '''
        # robot actions #
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        
        self.redmax_robot_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        
        params_to_train = []
        params_to_train += list(self.redmax_robot_actions.parameters())


        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        # init_rot = R.random().as_quat()
        # init_rot = torch.from_numpy(init_rot).float().cuda()
        # self.robot_glb_rotation.weight.data[0, :] = init_rot[:] # init rot
        
        
        
        ### Constraint set ###
        self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        self.timestep_to_active_mesh = {}
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts #
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        
        # states -> get states -> only update the acitons #
        with torch.no_grad():
            for cur_ts in range(self.nn_ts):
                ''' Get rotations, translations, and actions of the current robot '''
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                else:
                    if self.conf['model.train_states']:
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    else:
                        actions = {}
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        actions['link_actions'] = actions_link_actions
                        self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        
                    if cur_ts == 10:
                        cur_visual_pts, link_name_to_transformations_and_transformed_pts = self.robot_agent.get_init_state_visual_pts(ret_link_name_to_tansformations=True)
                    else:
                        cur_visual_pts = self.robot_agent.get_init_state_visual_pts()

                ### transform the visual pts ###
                cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_visual_pts = cur_visual_pts * 2. -1.
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                if not self.use_mano_inputs:
                    self.timestep_to_active_mesh[cur_ts] = cur_visual_pts.detach()
                # timestep_to_active_mesh_opt_ours_sim #
                self.timestep_to_active_mesh_opt_ours_sim[cur_ts] = cur_visual_pts[sampled_verts_idxes].detach()
                
                
        if self.train_actions_with_states: # 
            for cur_ts in range(self.nn_ts):
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                else:
                    # if self.conf['model.train_states']: # 
                    cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    states = {}
                    states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                    states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                    states['link_states'] = cur_state
                    self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)      
            tmp_joint_nm_to_ts_to_states = self.robot_agent.get_timestep_to_states()
            joint_nm_to_ts_to_states = {}
            for cur_joint_nm in tmp_joint_nm_to_ts_to_states:
                cur_joint_ts_to_states = {}
                for cur_ts in tmp_joint_nm_to_ts_to_states[cur_joint_nm]:
                    cur_joint_ts_to_states[cur_ts] = tmp_joint_nm_to_ts_to_states[cur_joint_nm][cur_ts].detach().clone()
                joint_nm_to_ts_to_states[cur_joint_nm] = cur_joint_ts_to_states
    

        
        ''' optimiz the delta states '''
        # nn_iters_opt_delta_states = 100
        # params_to_train_delta_states = []
        # params_to_train_delta_states += list(self.robot_delta_states.parameters())
        # self.delta_states_optimizer = torch.optim.Adam(params_to_train_delta_states, lr=self.learning_rate)
        
        # for i_iter in tqdm(range(nn_iters_opt_delta_states)):
        #     tot_diff_visual_pts = []
        #     cur_ts_to_optimized_visual_pts = {}
        #     for cur_ts in range(self.nn_ts):
        #         cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
        #         cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
        #         cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
        #         cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
        #         if cur_ts == 0:
        #             links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
        #             self.robot_agent.set_init_states_target_value(links_init_states)
        #             cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
        #         else:
        #             # if self.conf['model.train_states']: # 
        #             cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
        #             states = {}
        #             states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
        #             states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
        #             states['link_states'] = cur_state
        #             self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)    
        #             cur_visual_pts = self.robot_agent.get_init_state_visual_pts()

        #         ### transform the visual pts ###
        #         cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
        #         cur_visual_pts = cur_visual_pts * 2. -1.
        #         cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                
        #         cur_rot = cur_glb_rot
        #         cur_trans = cur_glb_trans
                
        #         ### transform by the glboal transformation and the translation ###
        #         cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) 
        #         diff_visual_pts = torch.sum(
        #             (cur_visual_pts[sampled_verts_idxes] - self.timestep_to_active_mesh_opt_ours_sim[cur_ts].detach()) ** 2, dim=-1
        #         )
        #         diff_visual_pts = diff_visual_pts.mean()
            
        #         self.delta_states_optimizer.zero_grad()
        #         diff_visual_pts.backward()
        #         self.delta_states_optimizer.step()
        #         tot_diff_visual_pts.append(diff_visual_pts.detach().item())
                
                
        #         cur_ts_to_optimized_visual_pts[cur_ts] = cur_visual_pts[sampled_verts_idxes].detach().cpu().numpy()
            
        #     self.cur_ts_to_optimized_visual_pts = cur_ts_to_optimized_visual_pts
                
        #     tot_diff_visual_pts = sum(tot_diff_visual_pts) / float(len(tot_diff_visual_pts))
        #     cur_log_sv_str = 'iter:{:8>d} loss = {} lr={}'.format(i_iter, tot_diff_visual_pts, self.delta_states_optimizer.param_groups[0]['lr'])
        #     # get states and then use the obtianed states for actions optim?
        #     print(cur_log_sv_str)
            
        #     self.iter_step = i_iter
            
        #     if i_iter % 10 == 0:
        #         self.validate_mesh_robo_b()
        #         self.save_checkpoint_delta_states()
        
        
        self.iter_step = 0
        
        # load_delta_states_fn = "/data3/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_/checkpoints/robo_delta_states_ckpt_000090.pth"
        # if len(load_delta_states_fn) > 0:
        #     delta_states_ckpt = torch.load(load_delta_states_fn, map_location=self.device, )
        #     self.robot_delta_states.load_state_dict(delta_states_ckpt['robot_delta_states'])
        # self.

        robo_states = []
        for cur_ts in range(self.nn_ts):
            if cur_ts == 0:
                cur_state = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()[:redmax_ndof_r]
            else:
                cur_delta_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0).detach().cpu().numpy()[:redmax_ndof_r]
                cur_state = robo_states[-1] + cur_delta_state
            # if cur_ts > 0:
            robo_states.append(cur_state)
        robo_states = np.stack(robo_states[1:], axis=0) # nn_states_to_optimize x ndof_r
        
        
        ''' Set redmax robot actions '''
        # nn_iters_opt_redmax_actions = 10000
        # nn_iters_opt_redmax_actions = 100
        # nn_iters_opt_redmax_actions = 17
        nn_substeps = 10
        params_to_train_actions = []
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps * nn_substeps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        params_to_train_actions += list(self.redmax_robot_actions.parameters())
        
        self.actions_optimizer = torch.optim.Adam(params_to_train_actions, lr=self.learning_rate)
        
        params_to_train_kines = []
        # params_to_train_kines += list(self.robot_glb_rotation.parameters())
        # params_to_train_kines += list(self.robot_glb_trans.parameters())
        params_to_train_kines += list(self.robot_delta_states.parameters())
        
        if self.use_LBFGS:
            self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
        else:
            self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        
        
        load_redmax_robot_actions_fn = "/data3/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_diffh_subs_10_/checkpoints/redmax_robot_actions_ckpt_001900.pth"
        if len(load_redmax_robot_actions_fn) > 0:
            redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
            self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        
        ''' Optimize redmax robot actions '''
        # for i_iter in tqdm(range(nn_iters_opt_redmax_actions)):
        #     redmax_sim.reset(backward_flag = True) 
        #     robo_intermediates_states = []
        #     tot_redmax_actions = []
        #     tot_visual_pts = []
        #     df_dqs = []
        #     tot_diff_qs = []
        #     for cur_ts in range(self.nn_ts):
        #         cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
        #         cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
        #         cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
        #         cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
        #         if cur_ts == 0:
        #             robot_init_state = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
        #             redmax_q_init = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
        #             redmax_q_init = redmax_q_init[:redmax_ndof_r]
        #             redmax_sim.set_q_init(redmax_q_init)
                    
        #             if i_iter % 100 == 0 or (i_iter == nn_iters_opt_redmax_actions - 1):
        #                 links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
        #                 self.robot_agent.set_init_states_target_value(links_init_states)
        #                 cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
        #         else:
                    
        #             for i_sub_step in range(nn_substeps):
        #                 cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
        #                 redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
        #                 redmax_u = redmax_actions.detach().cpu().numpy()
        #                 redmax_u = redmax_u[: redmax_ndof_u]
        #                 redmax_sim.set_u(redmax_u) # sim
        #                 tot_redmax_actions.append(redmax_actions)
        #                 ''' set and set the virtual forces ''' ## redmax_sim
        #                 virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
        #                 redmax_sim.set_forces(virtual_force) ### virtual forces ###

        #                 redmax_sim.forward(1, verbose = False)
        #                 redmax_sim_q = redmax_sim.get_q()
        #                 if i_sub_step == nn_substeps - 1:
        #                     robo_intermediates_states.append(redmax_sim_q)
        #                     df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts - 1]))
        #                     cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts - 1]) ** 2)
        #                     tot_diff_qs.append(cur_diff_qs.item())
        #                 else:
        #                     df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                    
        #             if i_iter % 100 == 0 or (i_iter == nn_iters_opt_redmax_actions - 1):
        #                 cur_delta_state = redmax_sim_q - (robo_intermediates_states[-1 - 1] if cur_ts > 1 else redmax_q_init)
        #                 cur_delta_state = torch.from_numpy(cur_delta_state).float().cuda()
        #                 states = {}
        #                 states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
        #                 states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
        #                 states['link_states'] = cur_delta_state
                        
        #                 self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)    
        #                 cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
        #         if i_iter % 100 == 0 or (i_iter == nn_iters_opt_redmax_actions - 1):
        #             ### transform the visual pts ###
        #             cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
        #             cur_visual_pts = cur_visual_pts * 2. -1.
        #             cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                     
        #             cur_rot = cur_glb_rot
        #             cur_trans = cur_glb_trans
                    
        #             ### transform by the glboal transformation and the translation ###
        #             cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) 
                    
        #             tot_visual_pts.append(cur_visual_pts[sampled_verts_idxes].detach().cpu().numpy())
                    
        #     # robo_intermediates_states = np.stack(robo_intermediates_states, axis=0) # nn_timesteps x ndof_r #
        #     # diff_qs = np.sum(
        #     #      (robo_intermediates_states - robo_states) ** 2, axis=-1
        #     # )
        #     # diff_qs = np.mean(diff_qs)
        #     # df_dq = 2 * (robo_intermediates_states - robo_states)
            
        #     df_dq = np.stack(df_dqs, axis=0)
        #     nn_forward_ts = df_dq.shape[0]
        #     df_dq = np.reshape(df_dq, (-1,))
        #     df_du = np.zeros((nn_forward_ts * redmax_ndof_u,))
            
            
        #     redmax_sim.backward_info.set_flags(False, False, False, True) 
        #     # nn_forward_ts = len(tot_grad_qs)
        #     # tot_grad_qs = np.concatenate(tot_grad_qs, axis=0)
        #     redmax_sim.backward_info.df_dq = df_dq
        #     redmax_sim.backward_info.df_du = df_du
        #     redmax_sim.backward()
        #     # redmax
        #     redmax_sim_robot_action_grad = np.copy(redmax_sim.backward_results.df_du)
        #     redmax_sim_robot_action_grad_th = torch.from_numpy(redmax_sim_robot_action_grad).float().cuda()
        #     redmax_sim_robot_action_grad_th = redmax_sim_robot_action_grad_th.contiguous().view(nn_forward_ts, -1).contiguous()
            
        #     tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) ## nn_forward_steps x nn_us_dim # 
        #     surrogate_loss = torch.sum(
        #         tot_redmax_actions[:, :redmax_ndof_u] * redmax_sim_robot_action_grad_th, dim=-1
        #     )
        #     surrogate_loss = surrogate_loss.mean()
            
        #     self.actions_optimizer.zero_grad()
        #     surrogate_loss.backward()
        #     self.actions_optimizer.step()
            
        #     diff_qs = sum(tot_diff_qs) / float(len(tot_diff_qs))
        #     cur_log_sv_str = 'iter:{:8>d} diff_qs = {} surrogate_loss = {} lr={}'.format(i_iter, diff_qs, surrogate_loss.item(), self.actions_optimizer.param_groups[0]['lr'])
        #     # get states and then use the obtianed states for actions optim?
        #     print(cur_log_sv_str)
            
            
            
        #     self.iter_step = i_iter
        #     if i_iter % 100 == 0 or (i_iter == nn_iters_opt_redmax_actions - 1):
        #         tot_visual_pts = np.stack(tot_visual_pts, axis=0)
        #         self.tot_visual_pts = tot_visual_pts
        #         self.validate_mesh_robo_c()
        #         self.save_checkpoint_redmax_robot_actions()
                    

        ''' Load optimized redmax robot actions '''
        # load_redmax_robot_actions_fn = "/data3/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_diffh_subs_10_/checkpoints/redmax_robot_actions_ckpt_001900.pth"
        # if len(load_redmax_robot_actions_fn) > 0:
        #     redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
        #     self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        # # self.


        # model_path = self.conf['model.sim_model_path']
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        # self.robot_agent = robot_agent
        
        # delta states -> 
        
        # the simulator env?  #
        ### load optimized actions ###
        if 'model.load_redmax_robot_actions' in self.conf and len(self.conf['model.load_redmax_robot_actions']) > 0: 
            load_redmax_robot_actions_fn = self.conf['model.load_redmax_robot_actions']
            redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
            self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        
        # robot_glb_rotation, robot_glb_trans #
        # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
        ori_redmax_robot_actions = self.redmax_robot_actions.weight.data.clone()    
        ori_redmax_glb_rot = self.robot_glb_rotation.weight.data.clone()
        ori_redmax_glb_trans = self.robot_glb_trans.weight.data.clone()
        ori_robot_delta_states = self.robot_delta_states.weight.data.clone()
        
        
        self.iter_step = 0
        # penetration_forces = None
        self.validate_mesh_robo_a()
        
        finger_sampled_idxes = None
        
        for i_iter in tqdm(range(100000)):
            tot_losses = []
            # tot_penalty_dot_forces_normals = []
            # tot_penalty_friction_constraint = []
            # tot_loss_figner_tracking = []
            # tot_loss_corr_tracking = []
            tot_tracking_loss = []
            
            # timestep 
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # self.timestep_to_
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            correspondence_pts_idxes = None
            
            # tot_penetrating_depth_penalty = []
            # tot_ragged_dist = []
            # tot_delta_offset_reg_motion = []
            # tot_dist_mano_visual_ori_to_cur = []
            # tot_reg_loss = []
            # tot_diff_cur_states_to_ref_states = []
            # tot_diff_tangential_forces = []
            # penetration_forces = None
            # sampled_visual_pts_joint_idxes = None
            
            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_force_d = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            self.ts_to_redmax_states = {}
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16]
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            
            redmax_sim.reset(backward_flag = True)
            
            tot_grad_qs = []
            
            robo_intermediates_states = []
            
            tot_penetration_depth = []
            
            robo_actions_diff_loss = []
            
            # init global transformations ##
            # cur_ts_redmax_delta_rotations = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_delta_rotations = torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda()
            
            for cur_ts in range(self.nn_ts):
                tot_redmax_actions = []
                
                # redmax_robot_states # # # #
                self.redmax_robot_states.weight.data[:, :] = 0.
                if self.redmax_robot_states.weight.grad is not None:
                    self.redmax_robot_states.weight.grad.data[:, :] =  self.redmax_robot_states.weight.grad.data[:, :] * 0.
                    
                actions = {}
                
                self.free_def_bending_weight = 0.0
           
                
                cur_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                cur_glb_rot = cur_glb_rot + cur_ts_redmax_delta_rotations
                
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                # cur_glb_rot_quat = cur_glb_rot.clone()
                
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                # cur_ts_redmax_delta_rotations, cur_ts_redmax_robot_trans # 
                # cur_ts_delta_rot, cur_ts_redmax_robot_trans #
                # # print(f"ts: {cur_ts}, cur_ts_redmax_delta_rotations: {cur_ts_redmax_delta_rotations}, cur_glb_rot: {cur_glb_rot_quat}")
                # cur_ts_delta_rot = dyn_model_act.quaternion_to_matrix(cur_ts_redmax_delta_rotations) # mano glboa
                # # cur_ts_delta_trans = 
                
                # # # cur_ts_delta_rot, cur_ts_redmax_robot_trans # # #
                # cur_glb_rot = torch.matmul(cur_ts_delta_rot, cur_glb_rot)
                cur_glb_trans = cur_glb_trans + cur_ts_redmax_robot_trans # 
                
                if cur_ts == 0:
                    links_init_states = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.robot_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts, link_name_to_link_transformations, visual_pts_joint_idxes = self.robot_agent.get_init_state_visual_pts(True, True)
                    
                    ''' Set init q of the redmax '''
                    # ### redmax_ndof_u, redmax_ndof_r ### #
                    redmax_q_init = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
                    redmax_q_init = redmax_q_init[:redmax_ndof_r]
                    
                    # 
                    redmax_sim.set_q_init(redmax_q_init)
                    
                    redmax_q_init_th = torch.from_numpy(redmax_q_init).float().cuda()
                    self.redmax_robot_states.weight.data[cur_ts, :redmax_q_init_th.size(0)] = redmax_q_init_th[:]
                    
                    self.ts_to_redmax_states[cur_ts] = redmax_q_init.copy()
                    
                else:
                    if self.conf['model.train_states']: # 
                        if self.add_delta_state_constraints:
                            self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg] = torch.clamp(self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg], max=0.)
                        
                        #  add delta states # 
                        cur_state = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        # self.redmax_robot_states.weight.data[0, :cur_state.]
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    else:
                        for i_sub_step in range(nn_substeps):
                            cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
                            redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
                            redmax_u = redmax_actions.detach().cpu().numpy()
                            redmax_u = redmax_u[: redmax_ndof_u]
                            redmax_sim.set_u(redmax_u) # sim
                            tot_redmax_actions.append(redmax_actions)
                            ''' set and set the virtual forces ''' ## redmax_sim
                            ###  TODO: get and set the virtual forces ###
                            # link_maximal_contact_forces # 
                            ### TODO: add the virtual force objects and set the virtual forces ###
                            # virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                            
                            ''' Set virtual forces ''' ### set the 
                            virtual_force = link_maximal_contact_forces.detach().cpu().numpy()
                            virtual_force = np.reshape(virtual_force, (virtual_force.shape[0] * virtual_force.shape[1]))
                            # # virtual forces # #
                            redmax_sim.set_forces(virtual_force) ### virtual forces ###

                            redmax_sim.forward(1, verbose = False)
                            # 
                            
                            # redmax_sim_q = redmax_sim.get_q()
                            # if i_sub_step == nn_substeps - 1:
                            #     robo_intermediates_states.append(redmax_sim_q)
                            #     df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts - 1]))
                            #     cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts - 1]) ** 2)
                            #     tot_diff_qs.append(cur_diff_qs.item())
                            # else:
                            #     df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                        redmax_sim_q = redmax_sim.get_q()
                        robo_intermediates_states.append(redmax_sim_q)
                        
                        # print(f"self.redmax_robot_states.weight: {self.redmax_robot_states.weight.data.size()}")
                        # self.redmax_robot_states.weight[cur_ts, :redmax_sim_q.shape[0]] = torch.from_numpy(redmax_sim_q).float().cuda()[:]
                        
                        
                        # self.redmax_robot_states.weight.data[cur_ts, :redmax_sim_q.shape[0]] = self.redmax_robot_states.weight.data[cur_ts, :redmax_sim_q.shape[0]] * 0. + torch.from_numpy(redmax_sim_q).float().cuda()[:]
                        
                        # cur_delta_state = self.redmax_robot_states(torch.zeros((1,)).long().cuda() + cur_ts) - self.redmax_robot_states(torch.zeros((1,)).long().cuda() + cur_ts - 1).detach()
                        
                        redmax_robot_states_th = torch.from_numpy(redmax_sim_q).float().cuda()
                        redmax_robot_states_th.requires_grad = True
                        redmax_robot_states_th.requires_grad_ = True
                        cur_delta_state = redmax_robot_states_th - (torch.from_numpy(robo_intermediates_states[-1 - 1]).float().cuda() if cur_ts > 1 else torch.from_numpy(redmax_q_init).float().cuda())
                        
                        
                        
                        
                        # self.redmax_robot_states 
                        # self.redmax_robot_states
                        # self.redma
                        # self
                        
                        
                        # cur_delta_state = redmax_sim_q - (robo_intermediates_states[-1 - 1] if cur_ts > 1 else redmax_q_init)
                        # cur_delta_state = torch.from_numpy(cur_delta_state).float().cuda()
                        
                        
                        cur_delta_state = cur_delta_state.squeeze(0)[:redmax_ndof_r]
                        # cur_delta_state = redmax_sim_q - (robo_intermediates_states[-1 - 1] if cur_ts > 1 else redmax_q_init)
                        # cur_delta_state = torch.from_numpy(cur_delta_state).float().cuda()
                        
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_delta_state
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)    
                        # cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                        
                        # # if i_iter % 100 == 0:
                        # #     cur_delta_state = redmax_sim_q - (robo_intermediates_states[-1 - 1] if cur_ts > 1 else redmax_q_init)
                        # #     cur_delta_state = torch.from_numpy(cur_delta_state).float().cuda()
                        # #     states = {}
                        # #     states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        # #     states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        # #     states['link_states'] = cur_delta_state
                            
                        # #     self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)    
                        # #     cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                        
                        
                        # # print(f"Draing via actions")
                        # actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        # actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        # actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        # actions['link_actions'] = actions_link_actions
                        
                        
                        # ''' set action and get states from the redmax simulator '''
                        # redmax_u = actions_link_actions.detach().cpu().numpy()
                        
                        # # redmax_robot_actions
                        # redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        # redmax_u = redmax_actions.detach().cpu().numpy()
                        # redmax_u = redmax_u[: redmax_ndof_u]
                        # redmax_sim.set_u(redmax_u) # sim
                        
                        # tot_redmax_actions.append(redmax_actions)
                        
                        # ''' set and set the virtual forces ''' ## redmax_sim
                        # virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32)
                        # redmax_sim.set_forces(virtual_force) ### virtual forces ###
                        
                        # redmax_sim.forward(1, verbose = False)
                        # redmax_sim_q = redmax_sim.get_q()
                        
                        # # redmax_ndof_m = redmax_sim.ndof_m #
                        # self.ts_to_redmax_states[cur_ts] = redmax_sim_q.copy() 
                        
                        # redmax_sim_q_th = torch.from_numpy(redmax_sim_q).float().cuda()
                        # self.redmax_robot_states.weight.data[cur_ts, :redmax_sim_q_th.size(0)] = redmax_sim_q_th[:]
                        
                        # cur_sim_delta_states_th = self.redmax_robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts) - self.redmax_robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts - 1).detach() # tdelta exc
                        # cur_sim_delta_states_th = cur_sim_delta_states_th.squeeze(0)
                        # states = {}
                        # states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        # states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        # states['link_states'] = cur_sim_delta_states_th # set delta sates
                        # self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                    
                    # cur visual pts #
                    cur_visual_pts, link_name_to_link_transformations, visual_pts_joint_idxes = self.robot_agent.get_init_state_visual_pts(True, True)




                # diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                ### transform the visual pts ###
                cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                cur_visual_pts = cur_visual_pts * 2. - 1.
                cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                ## 
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) 
                
                
                
                diff_redmax_visual_pts_with_ori_visual_pts = torch.sum(
                    (cur_visual_pts[sampled_verts_idxes] - self.timestep_to_active_mesh_opt_ours_sim[cur_ts].detach()) ** 2, dim=-1
                )
                diff_redmax_visual_pts_with_ori_visual_pts = diff_redmax_visual_pts_with_ori_visual_pts.mean()
                
                
                self.timestep_to_active_mesh[cur_ts] = cur_visual_pts
                self.timestep_to_raw_active_meshes[cur_ts] = cur_visual_pts.detach().cpu().numpy()
                
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                # diff_robo_to_corr_mano_pts =  torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                if finger_sampled_idxes is None:
                    finger_sampled_idxes = []
                    minn_visual_pts_joint_idxes = torch.min(visual_pts_joint_idxes).item()
                    for i_sampled_v in range(sampled_verts_idxes.size(0)):
                        cur_sampled_v_idx = sampled_verts_idxes[i_sampled_v].item()
                        if visual_pts_joint_idxes[cur_sampled_v_idx] != minn_visual_pts_joint_idxes:
                            finger_sampled_idxes.append(cur_sampled_v_idx)
                    finger_sampled_idxes = torch.tensor(finger_sampled_idxes, dtype=torch.long).cuda()
                    print(f"minn_visual_pts_joint_idxes: {minn_visual_pts_joint_idxes}, finger_sampled_idxes: {finger_sampled_idxes.size()}, sampled_verts_idxes: {sampled_verts_idxes.size()}")
                    self.sampled_verts_idxes = finger_sampled_idxes
                        
                def evaluate_tracking_loss():
                    self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=finger_sampled_idxes, reference_mano_pts=None, fix_obj=False, contact_pairs_set=self.contact_pairs_set)
                    #### compute loss optmimized ransformations ##
                    in_func_tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_ts + 1)
                    # # init states  
                    # # cur_ts % mano_nn_substeps == 0:
                    # if (cur_ts + 1) % mano_nn_substeps == 0:
                    #     cur_passive_big_ts = cur_ts // mano_nn_substeps
                    #     # tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                    # else:
                    #     in_func_tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    return in_func_tracking_loss

                    
                if contact_pairs_set is None:
                    self.contact_pairs_set = None
                else:
                    self.contact_pairs_set = contact_pairs_set.copy()
                
                contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=finger_sampled_idxes, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)

                
                if self.train_with_forces_to_active and (not self.use_mano_inputs): # use mano inputs #
                    # penetration_forces #
                    if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                        net_penetrating_forces = self.other_bending_network.penetrating_forces
                        net_penetrating_points = self.other_bending_network.penetrating_points
                        # cur_penetrating_forces = []
                        # for i_pts in range(net_penetrating_points.size(0)):
                        #     cur_penetration_point = net_penetrating_points[i_pts]
                        #     cur_penetration_point_forces = net_penetrating_forces[i_pts]
                        #     cur_penetrating_forces.append((cur_penetration_point, cur_penetration_point_forces))
                        # penetration_forces = cur_penetrating_forces
                        
                        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                        self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                        self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                        
                        
                        ### transform the visual pts ###
                        # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        # cur_visual_pts = cur_visual_pts * 2. - 1.
                        # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                        
                        sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[finger_sampled_idxes][self.other_bending_network.penetrating_indicator]
                        # sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[sampled_verts_idxes]
                        
                        
                        net_penetrating_forces = torch.matmul(
                            cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                        net_penetrating_forces = net_penetrating_forces / 2
                        net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                        
                        net_penetrating_points = torch.matmul(
                            cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                        net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                        net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                        
                        
                        penetration_forces = {
                            'penetration_forces': net_penetrating_forces, 'penetration_forces_points': net_penetrating_points
                        }
                        
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        # self.robot_agent.set_maximal_contact_forces(link_maximal_contact_forces)
                        self.robot_agent.set_penetration_forces(penetration_forces, sampled_visual_pts_joint_idxes, link_maximal_contact_forces)
                        
                    else:
                        penetration_forces = None
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        
                if contact_pairs_set is not None:
                    self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                
                self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                if self.other_bending_network.penalty_based_friction_forces is not None:
                    self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                
                # self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                tot_penetration_depth.append(self.other_bending_network.penetrating_depth_penalty.detach().item())
                
                ### optimize with intermediates ### 
                if self.optimize_with_intermediates:
                    tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1) # 
                else:
                    tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()

                tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                
                loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                # diff_redmax_visual_pts_with_ori_visual_pts.backward()
                penetraton_penalty = self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                
                # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
                robot_rotation_diff = torch.sum(
                    (ori_redmax_glb_rot - self.robot_glb_rotation.weight) ** 2, dim=-1
                )
                robot_rotation_diff = robot_rotation_diff.mean()
                
                robot_trans_diff = torch.sum(
                    (ori_redmax_glb_trans - self.robot_glb_trans.weight) ** 2, dim=-1
                )
                robot_trans_diff = robot_trans_diff.mean()
                
                robot_delta_states_diff = torch.sum(
                    (ori_robot_delta_states - self.robot_delta_states.weight) ** 2, dim=-1
                )
                robot_delta_states_diff = robot_delta_states_diff.mean()
                
                
                
                kinematics_trans_diff = (robot_rotation_diff + robot_trans_diff + robot_delta_states_diff) * self.robot_actions_diff_coef
                
                kinematics_proj_loss = kinematics_trans_diff + penetraton_penalty + tracking_loss # tracking loss only ?# #
                
                loss = loss # + (robot_rotation_diff + robot_trans_diff) * self.robot_actions_diff_coef
                
                loss = kinematics_proj_loss
                
                # self.kines_optimizer.zero_grad()
                
                # kinematics_proj_loss.backward(retain_graph=True)
                
                # self.kines_optimizer.step()
                
                
                if contact_pairs_set is not None: # 
                    self.kines_optimizer.zero_grad()
                    
                    kinematics_proj_loss.backward(retain_graph=True)
                    
                    if self.use_LBFGS:
                        self.kines_optimizer.step(evaluate_tracking_loss) # 
                    else:
                        self.kines_optimizer.step()

                # tracking_loss.backward(retain_graph=True) # tracking_loss #
                if self.use_LBFGS:
                    self.other_bending_network.reset_timestep_to_quantities(cur_ts)
                
                
                
                # tracking_loss.backward(retain_graph=True)
                
                
                
                # redmax_sim.forward(1, verbose = False)
                # redmax_sim_q = redmax_sim.get_q() # 
                
                ''' Record actions grads ''' 
                # if cur_ts > 0:
                #     # tot_grad_qs
                #     redmax_robot_states_grad = self.redmax_robot_states.weight.grad[cur_ts]
                #     redmax_robot_states_grad_np = redmax_robot_states_grad.detach().cpu().numpy()
                #     tot_grad_qs.append(redmax_robot_states_grad_np[:redmax_ndof_r])
                    
                
                ''' Update actions in each timestep '''
                # if cur_ts > 0:
                #     # print(f"cur_ts: {cur_ts}")
                    
                #     # redmax_robot_states_grad = self.redmax_robot_states.weight.grad[cur_ts]
                    
                #     redmax_robot_states_grad = redmax_robot_states_th.grad.data
                #     redmax_robot_states_grad_np = redmax_robot_states_grad.detach().cpu().numpy()
                #     redmax_sim.backward_info.set_flags(False, False, False, True) 
                #     redmax_robot_states_grad_np_tot = np.zeros((cur_ts * nn_substeps * redmax_ndof_r,), dtype=np.float32)
                #     redmax_robot_states_grad_np_tot[-redmax_ndof_r:] = redmax_robot_states_grad_np[:redmax_ndof_r]
                #     # redmax_robot_states_grad_np_tot = np.concatenate(
                #     #     [np.zeros((redmax_ndof_r * (cur_ts - 1),), dtype=np.float32), redmax_robot_states_grad_np[:redmax_ndof_r]], axis=0
                #     # )
                #     # print(f"redmax_robot_states_grad_np_tot: {redmax_robot_states_grad_np_tot.shape}")
                #     redmax_sim.backward_info.df_dq = redmax_robot_states_grad_np_tot
                #     redmax_sim.backward_info.df_du = np.zeros(redmax_ndof_u * cur_ts * nn_substeps) 
                #     # print(f"Start backarding")
                #     redmax_sim.backward()
                #     # print(f"After backawarding")
                #     redmax_sim_robot_action_grad = np.copy(redmax_sim.backward_results.df_du)
                #     # redmax_sim #
                #     redmax_sim_robot_action_grad_th = torch.from_numpy(redmax_sim_robot_action_grad).float().cuda()[ -redmax_ndof_u * nn_substeps: ]
                    
                #     tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) ## nn_forward_steps x nn_us_dim #
                #     redmax_sim_robot_action_grad_th = redmax_sim_robot_action_grad_th.contiguous().view(nn_substeps, -1).contiguous()
                #     surrogate_loss = torch.sum(
                #         tot_redmax_actions[:, :redmax_sim_robot_action_grad_th.size(1)] * redmax_sim_robot_action_grad_th, dim=-1
                #     )
                    
                #     surrogate_loss = surrogate_loss.mean()
                    
                #     ''' get states and actions differences '''
                #     robot_actions_diff = torch.sum(
                #         (ori_redmax_robot_actions - self.redmax_robot_actions.weight) ** 2, dim=-1
                #     )
                #     robot_actions_diff = robot_actions_diff.mean()
                    
                    
                    
                #     robot_states_actions_diff_loss = robot_actions_diff # + robot_rotation_diff + robot_trans_diff
                    
                    
                #     robo_actions_diff_loss.append(robot_states_actions_diff_loss.item())
                    
                #     surrogate_loss = surrogate_loss + robot_actions_diff * self.robot_actions_diff_coef
                    
                    
                #     ## surrogate loss ? ##
                #     # surrogate_loss = torch.sum(redmax_sim_robot_action_grad_th[:redmax_sim_robot_action_grad_th.size(0)] * redmax_sim_q_th[:redmax_sim_robot_action_grad_th.size(0)])
                    
                #     # loss = actions * actions_grad --> \partial loss / \partial actions = actions_grad -> per-element actions grad for the acjtions 
                #     # surrogate_loss = torch.sum(redmax_actions[:redmax_sim_robot_action_grad_th.size(0)] * redmax_sim_robot_action_grad_th[:redmax_sim_robot_action_grad_th.size(0)])
                
                robot_states_actions_diff_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                robo_actions_diff_loss.append(robot_states_actions_diff_loss)
                
                #     self.actions_optimizer.zero_grad()
                #     surrogate_loss.backward()
                #     self.actions_optimizer.step()
                #     # 
                
                
                tot_losses.append(loss.detach().item())
                # tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                # tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                self.update_learning_rate() ## update learning rate ##
                
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                
            
            ''' Get nn_forward_ts and backward through the actions for updating '''
            # redmax_sim.backward_info.set_flags(False, False, False, True) 
            # nn_forward_ts = len(tot_grad_qs)
            # tot_grad_qs = np.concatenate(tot_grad_qs, axis=0)
            # redmax_sim.backward_info.df_dq = tot_grad_qs
            # redmax_sim.backward_info.df_du = np.zeros(redmax_ndof_u * nn_forward_ts) 
            # redmax_sim.backward()
            # redmax_sim_robot_action_grad = np.copy(redmax_sim.backward_results.df_du)
            # redmax_sim_robot_action_grad_th = torch.from_numpy(redmax_sim_robot_action_grad).float().cuda()
            # redmax_sim_robot_action_grad_th = redmax_sim_robot_action_grad_th.contiguous().view(nn_forward_ts, -1).contiguous()
            
            # tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) ## nn_forward_steps x nn_us_dim # 
            # surrogate_loss = torch.sum(
            #     tot_redmax_actions[:, :redmax_ndof_u] * redmax_sim_robot_action_grad_th, dim=-1
            # )
            # surrogate_loss = surrogate_loss.mean()
            
            # self.optimizer.zero_grad()
            # surrogate_loss.backward()
            # self.optimizer.step()
            
            
            
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            # tot_penalty_dot_forces_normals = sum(tot_penalty_dot_forces_normals) / float(len(tot_penalty_dot_forces_normals))
            # tot_penalty_friction_constraint = sum(tot_penalty_friction_constraint) / float(len(tot_penalty_friction_constraint))
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            tot_penetration_depth = sum(tot_penetration_depth) / float(len(tot_penetration_depth))
            robo_actions_diff_loss = sum(robo_actions_diff_loss) / float(len(robo_actions_diff_loss))
            
            if i_iter % self.report_freq == 0:
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} tracking_loss = {} penetration_depth = {} actions_diff_loss = {} lr={}'.format(self.iter_step, tot_losses, tot_tracking_loss, tot_penetration_depth, robo_actions_diff_loss, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                ''' Dump to the file '''
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                
            # robo_intermediates_states = np.stack(robo_intermediates_states, axis=0) ### robot intermediate states ###
            # robo_intermediates_states = torch.from_numpy(robo_intermediates_states).float()
            # self.robo_intermediates_states = robo_intermediates_states    
            
            self.validate_mesh_robo_a()
            if i_iter % self.val_mesh_freq == 0:
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                
            
            
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()

    def train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest(self, ):
        
        # chagne # # mano notjmano but the mano ---> optimize the mano delta states? #
        ### the real robot actions from mano model rules ### # logging #
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate() # update learning rrate # 
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load 
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path'] # 
        robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        
        
        
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        # model_path = 
        
        
        ''' Load robot hand in DiffHand simulator '''
        redmax_sim = redmax.Simulation(model_path)
        redmax_sim.reset(backward_flag = True) # redmax_sim -- 
        # ### redmax_ndof_u, redmax_ndof_r ### #
        redmax_ndof_u = redmax_sim.ndof_u
        redmax_ndof_r = redmax_sim.ndof_r
        redmax_ndof_m = redmax_sim.ndof_m ### ndof_m ### # redma # x_sim 
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        # mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano) # robot #
        mano_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path_mano) ## model path mano ## # 
        self.mano_agent = mano_agent
        # ''' Load the mano hand '''
        
        self.robo_hand_faces = self.mano_agent.robot_faces
        
        if self.use_mano_hand_for_test:
            self.robo_hand_faces = self.hand_faces
        # self.
        
        # mano_agent #
        
        nn_substeps = 10
        
        mano_nn_substeps = 1
        # mano_nn_substeps = 10 # 
        self.mano_nn_substeps = mano_nn_substeps
        
        # self.hand_faces # 
        
        
        
        
        ''' Expnad the current visual points ''' 
        # expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        # self.expanded_visual_pts_nn = expanded_visual_pts.size(0)
        # expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        # expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        # np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        # # ''' Expnad the current visual points '''  #  # differentiate through the simulator? # # 
        
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        ''' Define MANO robot actions, delta_states, init_states, frictions, and others '''
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        
        # self.free_deformation_time_latent = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        # )
        # params_to_train += list(self.free_deformation_time_latent.parameters())
        
        
        # self.deformation_input_dim = 3 + self.bending_latent_size
        # self.deformation_output_dim = 3
        # ### free deformation network ### # free deformation # 
        # self.free_deformation_network = self.construct_field_network(self.deformation_input_dim, self.bending_latent_size,  self.deformation_output_dim)
        # params_to_train += list(self.free_deformation_network.parameters())
        
        
        ''' Load optimized MANO hand actions and states '''
        # ### laod optimized init actions ####
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # cur_optimized_init_actions = # optimized init states 
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            
            
            
            self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_init_states'])
            
            
            if optimized_init_actions_ckpt['mano_robot_glb_rotation']['weight'].size(0) == num_steps:
                # weight nn_steps x xxx # 
                for i_cur_step in range(num_steps):
                    self.mano_robot_glb_rotation.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 1) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_glb_rotation']['weight'][i_cur_step: i_cur_step + 1, :]
                    
                    #### set mano glb rotations ###
                    if i_cur_step < num_steps - 1:
                        for i_mano_substep in range(mano_nn_substeps):
                            
                            cur_substep_trans = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step, :] + (optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step + 1, :] - optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step, :]) / float(mano_nn_substeps) * float(i_mano_substep)
                            self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps + i_mano_substep, :] = cur_substep_trans
                    else:
                        self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 2) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step: i_cur_step + 1, :]
                    # self.mano_robot_glb_trans.weight.data[(i_cur_step + ) * mano_nn_substeps + i_mano_substep, :] = cur_substep_trans
                    
                    
                    ### 
                    # self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps: i_cur_step * mano_nn_substeps + 1] = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step: i_cur_step + 1, :]
                    
                    
                    # self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 1) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step: i_cur_step + 1, :]
                    
                    ### delta states trans ###
                    #### delta states --- the current timestteps  ###
                    # for i_substep in range(mano_nn_substeps):
                    
                    self.mano_robot_delta_states.weight.data[i_cur_step * mano_nn_substeps: i_cur_step * mano_nn_substeps + 1] = optimized_init_actions_ckpt['mano_robot_delta_states']['weight'][i_cur_step: i_cur_step + 1, :]
                    #### delta states --- set to zero ###
                    self.mano_robot_delta_states.weight.data[i_cur_step * mano_nn_substeps + 1: (i_cur_step + 1) * mano_nn_substeps] = 0. # optimized_init_actions_ckpt['mano_robot_delta_states']['weight'][i_cur_step: i_cur_step + 1, :]
                    
                    # if self.use_mano_hand_for_test:
                        
                    
                    
                    # self.mano_robot_delta_states.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 1) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_delta_states']['weight'][i_cur_step: i_cur_step + 1, :]
            else:
                self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_rotation'])
                if 'robot_delta_states' in optimized_init_actions_ckpt:
                    self.mano_robot_delta_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_delta_states'])
                if 'mano_robot_actions' in optimized_init_actions_ckpt:
                    self.mano_robot_actions.load_state_dict(optimized_init_actions_ckpt['mano_robot_actions'])
                # self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
                self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_trans'])
            # if 'expanded_actuator_friction_forces' in optimized_init_actions_ckpt:
            #     try:
            #         self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_friction_forces'])
            #     except:
            #         pass
        #     self.mano_expanded_actuator_delta_offset.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_delta_offset'])
        # # ### load init transformations ckpts ### #
        
        # robot actions #
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        ## ##
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        ## robot actuator friction #
        # robot actuator friction forces #
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        params_to_train = []
        
        ''' Only set the robot friction forces optimizable '''
        if not self.optimize_rules:
            params_to_train += list(self.robot_actions.parameters())
            params_to_train += list(self.robot_delta_states.parameters())
            params_to_train += list(self.robot_init_states.parameters())
            params_to_train += list(self.robot_glb_rotation.parameters())
            params_to_train += list(self.robot_glb_trans.parameters())
        
        ''' Add params for expanded points '''
        # params_to_train += list(self.mano_expanded_actuator_delta_offset_nex.parameters())
        # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        if self.optimize_rules:
            params_to_train += list(self.other_bending_network.parameters())
        
        
        ''' Scaling constants '''
        ### affine ###
        # self.maxx_robo_pts = 25. ##
        # self.minn_robo_pts = -15. ##
        # self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        # self.mult_const_after_cent = 0.5437551664260203
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        print(f"mano_to_dyn_corr_pts_idxes: {self.mano_to_dyn_corr_pts_idxes.size()}")
        
        
        # maxx robo pts ##
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        self.mano_mult_const_after_cent = 3.
        
        ''' Load optimized robot hands '''
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_transformations_fn = self.conf['model.load_optimized_init_transformations']
            ##### Load optimized initial transformations #####
            optimized_init_transformations_ckpt = torch.load(cur_optimized_init_transformations_fn, map_location=self.device, )
            self.robot_init_states.load_state_dict(optimized_init_transformations_ckpt['robot_init_states'])
            self.robot_glb_rotation.load_state_dict(optimized_init_transformations_ckpt['robot_glb_rotation'])
            # if 'robot_delta_states' in optimized_init_transformations_ckpt:
            #     self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            self.robot_actions.load_state_dict(optimized_init_transformations_ckpt['robot_actions'])
            self.robot_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['robot_actuator_friction_forces'])
            self.robot_glb_trans.load_state_dict(optimized_init_transformations_ckpt['robot_glb_trans'])
            
            if not self.load_only_glb:
                self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            
        # if 'model.load_optimized_expanded' in self.conf and len(self.conf['model.load_optimized_expanded']) > 0:
        #     load_optimized_expanded_ckpt = self.conf['model.load_optimized_expanded']
        #     load_optimized_expanded_ckpt = torch.load(load_optimized_expanded_ckpt, map_location=self.device)
        #     self.mano_expanded_actuator_delta_offset.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset'])
        #     self.mano_expanded_actuator_friction_forces.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_friction_forces'])
        #     if 'mano_expanded_actuator_delta_offset_nex' in load_optimized_expanded_ckpt:
        #         print(f"Loading expanded actuator delta offset nex from the saved checkpoint...")
        #         self.mano_expanded_actuator_delta_offset_nex.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset_nex'])
        
        
        
        # robot_glb_rotation, robot_glb_trans #
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        
        
        # # train the robot or train the expanded points #
        # self.mano_fingers = [745, 279, 320, 444, 555, 672, 234, 121]
        # self.robot_fingers = [521, 624, 846, 973, 606, 459, 383, 265]
        
        # # optimize with intermediates #
        # self.mano_fingers = torch.tensor(self.mano_fingers, dtype=torch.long).cuda()
        # self.robot_fingers = torch.tensor(self.robot_fingers, dtype=torch.long).cuda()
        
        self.nn_ts = self.nn_timesteps - 1
        # self.optimize_with_intermediates = False
        
        # search for the nearest point so that it can manipulate the object #
        ## TODO: co-optimize the bending network weights? ## # optimize active object is true ##
        # if self.optimize_with_intermediates and (not self.optimize_active_object): # optimize with intermediates #
        #     # if not self.optimize_active_object: #
        #     # free_deformation_time_latent, free_deformation_network #
        #     params_to_train = []
        #     params_to_train += list(self.other_bending_network.parameters())
        #     params_to_train += list(self.robot_actuator_friction_forces.parameters())
        #     # params_to_train += list(self.expanded_actuator_friction_forces.parameters()) # expanded actuator 
        
        ''' Set actions for the redmax simulation and add parameters to params-to-train '''
        # robot actions #
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        
        self.redmax_robot_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        
        params_to_train = []
        params_to_train += list(self.redmax_robot_actions.parameters())


        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        # init_rot = R.random().as_quat()
        # init_rot = torch.from_numpy(init_rot).float().cuda()
        # self.robot_glb_rotation.weight.data[0, :] = init_rot[:] # init rot
        
        
        
        ### Constraint set ###
        self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        self.timestep_to_active_mesh = {}
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts #
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        
        # states -> get states -> only update the acitons #
        with torch.no_grad():
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                ''' Get rotations, translations, and actions of the current robot '''
                cur_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7) # mano glb rot
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.mano_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.mano_agent.get_init_state_visual_pts()
                else:
                    if self.conf['model.train_states']:
                        cur_state = self.mano_robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        # self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                        self.mano_agent.active_robot.set_delta_state_and_update(cur_state, cur_ts)
                    else:
                        actions = {}
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        actions['link_actions'] = actions_link_actions
                        self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                    
                    cur_visual_pts = self.mano_agent.get_init_state_visual_pts()
                    # if cur_ts == 10:
                    #     cur_visual_pts, link_name_to_transformations_and_transformed_pts = self.robot_agent.get_init_state_visual_pts(ret_link_name_to_tansformations=True)
                    # else:
                    #     cur_visual_pts = self.robot_agent.get_init_state_visual_pts()

                ### transform the visual pts ###
                # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                # cur_visual_pts = cur_visual_pts * 2. -1.
                # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                
                cur_visual_pts = cur_visual_pts * self.mano_mult_const_after_cent
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                # if not self.use_mano_inputs:
                if self.use_mano_hand_for_test:
                    self.timestep_to_active_mesh[cur_ts] = self.rhand_verts[cur_ts].detach()
                else:
                    self.timestep_to_active_mesh[cur_ts] = cur_visual_pts.detach()
                # timestep_to_active_mesh_opt_ours_sim #
                # self.timestep_to_active_mesh_opt_ours_sim[cur_ts] = cur_visual_pts[sampled_verts_idxes].detach()
                self.timestep_to_active_mesh_opt_ours_sim[cur_ts] = cur_visual_pts.detach()
                
        self.iter_step = 0
        
        ''' Set redmax robot actions '''
        # nn_iters_opt_redmax_actions = 10000
        # nn_iters_opt_redmax_actions = 100
        # nn_iters_opt_redmax_actions = 17
        
        params_to_train_actions = []
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps * nn_substeps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        params_to_train_actions += list(self.redmax_robot_actions.parameters())
        
        self.actions_optimizer = torch.optim.Adam(params_to_train_actions, lr=self.learning_rate)
        
        params_to_train_kines = []
        params_to_train_kines += list(self.mano_robot_glb_rotation.parameters())
        params_to_train_kines += list(self.mano_robot_glb_trans.parameters())
        params_to_train_kines += list(self.mano_robot_delta_states.parameters())
        
        # can tray the optimizer ##
        if self.use_LBFGS:
            self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
        else:
            self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        
        if self.optimize_rules:
            params_to_train_kines = []
            params_to_train_kines += list(self.other_bending_network.parameters())
            # self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            if self.use_LBFGS:
                self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
            else:
                self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            # self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=1e-2)
        
        # spring_ks_values, inertia_div_factor
        # self.other_bending_network.spring_ks_values.weight.data[:, :] = 0.1395
        # self.other_bending_network.spring_ks_values.weight.data[0, :] = 0.1395
        # self.other_bending_network.spring_ks_values.weight.data[1, :] = 0.00
        # self.other_bending_network.inertia_div_factor.weight.data[:, :] = 10.0
        # self.other_bending_network.inertia_div_factor.weight.data[:, :] = 1000.0
        # self.other_bending_network.inertia_div_factor.weight.data[:, :] = 100.0
        
        # load redma robot actions #
        load_redmax_robot_actions_fn = "/data3/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_diffh_subs_10_/checkpoints/redmax_robot_actions_ckpt_001900.pth"
        if len(load_redmax_robot_actions_fn) > 0:
            redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
            self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        

        ''' Load optimized redmax robot actions '''
        # load_redmax_robot_actions_fn = "/data3/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_diffh_subs_10_/checkpoints/redmax_robot_actions_ckpt_001900.pth"
        # if len(load_redmax_robot_actions_fn) > 0:
        #     redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
        #     self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        # # self. # # 
        
        # use


        # model_path = self.conf['model.sim_model_path']
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        # self.robot_agent = robot_agent
        
        # delta states -> 
        
        # the simulator env?  #
        ### load optimized actions ###
        # if 'model.load_redmax_robot_actions' in self.conf and len(self.conf['model.load_redmax_robot_actions']) > 0: 
        #     load_redmax_robot_actions_fn = self.conf['model.load_redmax_robot_actions']
        #     redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
        #     self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        
        # robot_glb_rotation, robot_glb_trans #
        # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
        # ori_redmax_robot_actions = self.redmax_robot_actions.weight.data.clone()    
        # ori_redmax_glb_rot = self.robot_glb_rotation.weight.data.clone()
        # ori_redmax_glb_trans = self.robot_glb_trans.weight.data.clone()
        # ori_robot_delta_states = self.robot_delta_states.weight.data.clone()
        
        
        # # ori_mano_robot_glb_rot, ori_mano_robot_glb_trans, ori_mano_robot_delta_states #
        ori_mano_robot_glb_rot = self.mano_robot_glb_rotation.weight.data.clone()
        ori_mano_robot_glb_trans = self.mano_robot_glb_trans.weight.data.clone()
        ori_mano_robot_delta_states = self.mano_robot_delta_states.weight.data.clone()
        
        
        self.iter_step = 0
        # print()
        # penetration_forces = None
        # self.validate_mesh_robo_a() # valida
        
        finger_sampled_idxes = None
        
        for i_iter in tqdm(range(100000)):
            tot_losses = []
            tot_tracking_loss = []
            
            # timestep 
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # self.timestep_to_
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            correspondence_pts_idxes = None
            
            # tot_penetrating_depth_penalty = []
            # tot_ragged_dist = []
            # tot_delta_offset_reg_motion = []
            # tot_dist_mano_visual_ori_to_cur = []
            # tot_reg_loss = []
            # tot_diff_cur_states_to_ref_states = []
            # tot_diff_tangential_forces = []
            # penetration_forces = None
            # sampled_visual_pts_joint_idxes = None
            
            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_force_d = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            self.ts_to_redmax_states = {}
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16]
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            
            redmax_sim.reset(backward_flag = True)
            
            # tot_grad_qs = []
            
            robo_intermediates_states = []
            
            tot_penetration_depth = []
            
            robo_actions_diff_loss = []
            mano_tracking_loss = []
            
            # init global transformations ##
            # cur_ts_redmax_delta_rotations = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_delta_rotations = torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda()
            
            # for cur_ts in range(self.nn_ts):
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                tot_redmax_actions = []
                
                # # redmax_robot_states # # # #
                # self.redmax_robot_states.weight.data[:, :] = 0.
                # if self.redmax_robot_states.weight.grad is not None:
                #     self.redmax_robot_states.weight.grad.data[:, :] =  self.redmax_robot_states.weight.grad.data[:, :] * 0.
                # mano_robot_glb_rotation, mano_robot_glb_trans, mano_robot_init_states, mano_robot_delta_states #
                    
                actions = {}
                
                self.free_def_bending_weight = 0.0

                # mano_robot_glb_rotation, mano_robot_glb_trans, mano_robot_delta_states #
                cur_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                cur_glb_rot = cur_glb_rot + cur_ts_redmax_delta_rotations
                
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                # cur_glb_rot_quat = cur_glb_rot.clone()
                
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                # cur_ts_redmax_delta_rotations, cur_ts_redmax_robot_trans # 
                # cur_ts_delta_rot, cur_ts_redmax_robot_trans #
                # # print(f"ts: {cur_ts}, cur_ts_redmax_delta_rotations: {cur_ts_redmax_delta_rotations}, cur_glb_rot: {cur_glb_rot_quat}")
                # cur_ts_delta_rot = dyn_model_act.quaternion_to_matrix(cur_ts_redmax_delta_rotations) # mano glboa
                # # cur_ts_delta_trans = 
                
                # # # cur_ts_delta_rot, cur_ts_redmax_robot_trans # # #
                # cur_glb_rot = torch.matmul(cur_ts_delta_rot, cur_glb_rot)
                cur_glb_trans = cur_glb_trans + cur_ts_redmax_robot_trans # 
                
                if cur_ts == 0:
                    links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    
                    self.mano_agent.set_init_states_target_value(links_init_states)
                    
                    # print(f"cur_ts: {cur_ts}, calculating mano agent points...")
                    cur_visual_pts = self.mano_agent.get_init_state_visual_pts() # get the visual points # 
                    
                    #### robot agent ####
                    # self.robot_agent.set_init_states_target_value(links_init_states)
                    # cur_visual_pts, link_name_to_link_transformations, visual_pts_joint_idxes = self.robot_agent.get_init_state_visual_pts(True, True)
                    
                    ''' Set init q of the redmax ''' # set init q of the redmax # 
                    # # ### redmax_ndof_u, redmax_ndof_r ### #
                    # redmax_q_init = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
                    # redmax_q_init = redmax_q_init[:redmax_ndof_r]
                    
                    # # # 
                    # redmax_sim.set_q_init(redmax_q_init)
                    
                    # redmax_q_init_th = torch.from_numpy(redmax_q_init).float().cuda()
                    # self.redmax_robot_states.weight.data[cur_ts, :redmax_q_init_th.size(0)] = redmax_q_init_th[:]
                    
                    # self.ts_to_redmax_states[cur_ts] = redmax_q_init.copy()
                    
                else:
                    if self.conf['model.train_states']: # 
                        # if self.add_delta_state_constraints:
                        #     self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg] = torch.clamp(self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg], max=0.)
                        
                        #  add delta states # 
                        cur_state = self.mano_robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        # self.redmax_robot_states.weight.data[0, :cur_state.]
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        # self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                        
                        self.mano_agent.active_robot.set_delta_state_and_update(cur_state, cur_ts)
                    else:
                        for i_sub_step in range(nn_substeps):
                            cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
                            redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
                            redmax_u = redmax_actions.detach().cpu().numpy()
                            redmax_u = redmax_u[: redmax_ndof_u]
                            redmax_sim.set_u(redmax_u) # sim
                            tot_redmax_actions.append(redmax_actions)
                            ''' set and set the virtual forces ''' ## redmax_sim
                            ###  TODO: get and set the virtual forces ###
                            # link_maximal_contact_forces # 
                            ### TODO: add the virtual force objects and set the virtual forces ###
                            # virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                            
                            ''' Set virtual forces ''' ### set the 
                            virtual_force = link_maximal_contact_forces.detach().cpu().numpy()
                            virtual_force = np.reshape(virtual_force, (virtual_force.shape[0] * virtual_force.shape[1]))
                            # # virtual forces # #
                            redmax_sim.set_forces(virtual_force) ### virtual forces ###

                            redmax_sim.forward(1, verbose = False)
                            # 
                            
                            # redmax_sim_q = redmax_sim.get_q()
                            # if i_sub_step == nn_substeps - 1:
                            #     robo_intermediates_states.append(redmax_sim_q)
                            #     df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts - 1]))
                            #     cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts - 1]) ** 2)
                            #     tot_diff_qs.append(cur_diff_qs.item())
                            # else:
                            #     df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                        redmax_sim_q = redmax_sim.get_q()
                        robo_intermediates_states.append(redmax_sim_q)
                        
                        # print(f"self.redmax_robot_states.weight: {self.redmax_robot_states.weight.data.size()}")
                        # self.redmax_robot_states.weight[cur_ts, :redmax_sim_q.shape[0]] = torch.from_numpy(redmax_sim_q).float().cuda()[:]
                        
                        
                        redmax_robot_states_th = torch.from_numpy(redmax_sim_q).float().cuda()
                        redmax_robot_states_th.requires_grad = True
                        redmax_robot_states_th.requires_grad_ = True
                        cur_delta_state = redmax_robot_states_th - (torch.from_numpy(robo_intermediates_states[-1 - 1]).float().cuda() if cur_ts > 1 else torch.from_numpy(redmax_q_init).float().cuda())
                        
                        
                        cur_delta_state = cur_delta_state.squeeze(0)[:redmax_ndof_r]
                        # cur_delta_state = redmax_sim_q - (robo_intermediates_states[-1 - 1] if cur_ts > 1 else redmax_q_init)
                        # cur_delta_state = torch.from_numpy(cur_delta_state).float().cuda()
                        
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_delta_state
                        self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)    
                        # cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                        
                    # cur visual pts #
                    # cur_visual_pts, link_name_to_link_transformations, visual_pts_joint_idxes = self.robot_agent.get_init_state_visual_pts(True, True)
                    cur_visual_pts = self.mano_agent.get_init_state_visual_pts() 



                ### transform the visual pts ###
                # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                # cur_visual_pts = cur_visual_pts * 2. - 1.  # cur visual pts # 
                # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                
                cur_visual_pts = cur_visual_pts * self.mano_mult_const_after_cent
                
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                ## 
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) 
                
                
                
                # diff_redmax_visual_pts_with_ori_visual_pts = torch.sum(
                #     (cur_visual_pts[sampled_verts_idxes] - self.timestep_to_active_mesh_opt_ours_sim[cur_ts].detach()) ** 2, dim=-1
                # )
                # diff_redmax_visual_pts_with_ori_visual_pts = diff_redmax_visual_pts_with_ori_visual_pts.mean()
                
                if self.use_mano_hand_for_test:
                    self.timestep_to_active_mesh[cur_ts] = self.rhand_verts[cur_ts] # .detach()
                else:
                    self.timestep_to_active_mesh[cur_ts] = cur_visual_pts
                self.timestep_to_raw_active_meshes[cur_ts] = cur_visual_pts.detach().cpu().numpy()
                
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                
                def evaluate_tracking_loss():
                    self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=self.contact_pairs_set)
                    
                    # init states  
                    # cur_ts % mano_nn_substeps == 0:
                    if (cur_ts + 1) % mano_nn_substeps == 0:
                        cur_passive_big_ts = cur_ts // mano_nn_substeps
                        in_func_tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_passive_big_ts + 1)
                        # tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                    else:
                        in_func_tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    return in_func_tracking_loss

                    
                if contact_pairs_set is None:
                    self.contact_pairs_set = None
                else:
                    self.contact_pairs_set = contact_pairs_set.copy()
                
                # print(f"start fowarding....")
                ### use only the mano hand points ###
                contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)

                # print(f"after fowarding....") # maintain the contacts ####
                # 
                if self.train_with_forces_to_active and (not self.use_mano_inputs): # use mano inputs #
                    # penetration_forces #
                    if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                        net_penetrating_forces = self.other_bending_network.penetrating_forces
                        net_penetrating_points = self.other_bending_network.penetrating_points
                        # cur_penetrating_forces = []
                        # for i_pts in range(net_penetrating_points.size(0)):
                        #     cur_penetration_point = net_penetrating_points[i_pts]
                        #     cur_penetration_point_forces = net_penetrating_forces[i_pts]
                        #     cur_penetrating_forces.append((cur_penetration_point, cur_penetration_point_forces))
                        # penetration_forces = cur_penetrating_forces
                        
                        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                        self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                        self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                        
                        
                        ### transform the visual pts ###
                        # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        # cur_visual_pts = cur_visual_pts * 2. - 1.
                        # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                        
                        # sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[finger_sampled_idxes][self.other_bending_network.penetrating_indicator]
                        
                        # sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[self.other_bending_network.penetrating_indicator]
                        
                        
                        
                        net_penetrating_forces = torch.matmul(
                            cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                        net_penetrating_forces = net_penetrating_forces / 2
                        net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                        
                        net_penetrating_points = torch.matmul(
                            cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                        net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                        net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                        
                        
                        # penetration_forces = {
                        #     'penetration_forces': net_penetrating_forces, 'penetration_forces_points': net_penetrating_points
                        # }
                        
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        # self.robot_agent.set_maximal_contact_forces(link_maximal_contact_forces)
                        # self.robot_agent.set_penetration_forces(penetration_forces, sampled_visual_pts_joint_idxes, link_maximal_contact_forces)
                        
                    else:
                        # penetration_forces = None
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        
                if contact_pairs_set is not None:
                    self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                
                # ts to 
                # contact force d 
                self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                if self.other_bending_network.penalty_based_friction_forces is not None:
                    self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                
                # contact 
                # changing cotnact points # 
                # easy to differentiate through cotnacts # 
                # maintain thee contact points # 
                # approaching and contacting ? # --- #
                # approaching and contacting ? # --- # contacting and approaching #  
                # active points; points; # --- in contact points # 
                # and calculate forces # 
                # if they are no longer in contacts # --- 
                # self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                # tot penetration depth
                tot_penetration_depth.append(self.other_bending_network.penetrating_depth_penalty.detach().item())
                
                ### optimize with intermediates ### # optimize with intermediates # 
                # if self.optimize_with_intermediates:
                #     tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1) # 
                # else:
                #     tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    
                
                # init states  
                # cur_ts % mano_nn_substeps == 0:
                if (cur_ts + 1) % mano_nn_substeps == 0:
                    cur_passive_big_ts = cur_ts // mano_nn_substeps
                    tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_passive_big_ts + 1)
                    tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                else:
                    tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()


                
                
                loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                # diff_redmax_visual_pts_with_ori_visual_pts.backward()
                penetraton_penalty = self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                
                
                # print(f"after calculating losses....")
                
                # # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
                # robot_rotation_diff = torch.sum(
                #     (ori_redmax_glb_rot - self.robot_glb_rotation.weight) ** 2, dim=-1
                # )
                # robot_rotation_diff = robot_rotation_diff.mean()
                
                # robot_trans_diff = torch.sum(
                #     (ori_redmax_glb_trans - self.robot_glb_trans.weight) ** 2, dim=-1
                # )
                # robot_trans_diff = robot_trans_diff.mean()
                
                # robot_delta_states_diff = torch.sum(
                #     (ori_robot_delta_states - self.robot_delta_states.weight) ** 2, dim=-1
                # )
                # robot_delta_states_diff = robot_delta_states_diff.mean()
                
                
                # mano_robot_glb_rotation, mano_robot_glb_trans, mano_robot_delta_states #
                # ori_mano_robot_glb_rot, ori_mano_robot_glb_trans, ori_mano_robot_delta_states #
                mano_robot_rotation_diff = torch.sum(
                    (ori_mano_robot_glb_rot - self.mano_robot_glb_rotation.weight) ** 2, dim=-1
                )
                mano_robot_rotation_diff = mano_robot_rotation_diff.mean()
                
                mano_robot_trans_diff = torch.sum(
                    (ori_mano_robot_glb_trans - self.mano_robot_glb_trans.weight) ** 2, dim=-1
                )
                mano_robot_trans_diff = mano_robot_trans_diff.mean()
                
                mano_robot_delta_states_diff = torch.sum(
                    (ori_mano_robot_delta_states - self.mano_robot_delta_states.weight) ** 2, dim=-1
                )
                mano_robot_delta_states_diff = mano_robot_delta_states_diff.mean()
                
                
                kinematics_trans_diff = (mano_robot_rotation_diff + mano_robot_trans_diff + mano_robot_delta_states_diff) * self.robot_actions_diff_coef
                
                # kinematics_trans_diff = (robot_rotation_diff + robot_trans_diff + robot_delta_states_diff) * self.robot_actions_diff_coef
                
                # delta # rhand vertices #
                if cur_ts % mano_nn_substeps == 0:
                    cur_kine_rhand_verts = self.rhand_verts[cur_ts // mano_nn_substeps]
                    cur_dyn_visual_pts_to_mano_verts = cur_visual_pts[self.mano_to_dyn_corr_pts_idxes]
                    diff_hand_tracking = torch.mean(
                        torch.sum((cur_kine_rhand_verts - cur_dyn_visual_pts_to_mano_verts) ** 2, dim=-1)
                    )
                else:
                    diff_hand_tracking = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                    
                
                
                # kinematics_proj_loss = kinematics_trans_diff + penetraton_penalty + tracking_loss + diff_hand_tracking
                
                # kinematics_proj_loss = kinematics_trans_diff + penetraton_penalty + diff_hand_tracking * 1e2 + tracking_loss
                
                # kinematics_proj_loss = kinematics_trans_diff + penetraton_penalty + diff_hand_tracking * 1e2 + tracking_loss
                
                # diff_hand_tracking_coef
                kinematics_proj_loss = kinematics_trans_diff + penetraton_penalty + diff_hand_tracking * self.diff_hand_tracking_coef + tracking_loss
                
                if self.use_mano_hand_for_test:
                    kinematics_proj_loss = tracking_loss
                
                loss = loss # + (robot_rotation_diff + robot_trans_diff) * self.robot_actions_diff_coef
                
                loss = kinematics_proj_loss
                
                
                # diff_hand_tracking
                mano_tracking_loss.append(diff_hand_tracking.detach().cpu().item())
                
                
                if contact_pairs_set is not None:
                    self.kines_optimizer.zero_grad()
                    
                    kinematics_proj_loss.backward(retain_graph=True)
                    
                    if self.use_LBFGS:
                        self.kines_optimizer.step(evaluate_tracking_loss) # 
                    else:
                        self.kines_optimizer.step()

                # tracking_loss.backward(retain_graph=True)
                if self.use_LBFGS:
                    self.other_bending_network.reset_timestep_to_quantities(cur_ts)
                
                
                # redmax_sim.forward(1, verbose = False)
                # redmax_sim_q = redmax_sim.get_q() # 
                
                ''' Record actions grads ''' 
                # if cur_ts > 0: # cur_ts # cur_ts > 0 #
                #     # tot_grad_qs
                #     redmax_robot_states_grad = self.redmax_robot_states.weight.grad[cur_ts]
                #     redmax_robot_states_grad_np = redmax_robot_states_grad.detach().cpu().numpy()
                #     tot_grad_qs.append(redmax_robot_states_grad_np[:redmax_ndof_r])
                    
                # tot_grads_qs #
                
                    
                #     ## surrogate loss ? ##
                #     # surrogate_loss = torch.sum(redmax_sim_robot_action_grad_th[:redmax_sim_robot_action_grad_th.size(0)] * redmax_sim_q_th[:redmax_sim_robot_action_grad_th.size(0)])
                    
                #     # loss = actions * actions_grad --> \partial loss / \partial actions = actions_grad -> per-element actions grad for the acjtions 
                #     # surrogate_loss = torch.sum(redmax_actions[:redmax_sim_robot_action_grad_th.size(0)] * redmax_sim_robot_action_grad_th[:redmax_sim_robot_action_grad_th.size(0)])
                
                robot_states_actions_diff_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                robo_actions_diff_loss.append(robot_states_actions_diff_loss)
                
                #     self.actions_optimizer.zero_grad()
                #     surrogate_loss.backward()
                #     self.actions_optimizer.step()
                #     # 
                
                
                tot_losses.append(loss.detach().item())
                # tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                # tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                self.update_learning_rate() ## update learning rate ##
                
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                
            
            ''' Get nn_forward_ts and backward through the actions for updating '''
            
            
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            # tot_penalty_dot_forces_normals = sum(tot_penalty_dot_forces_normals) / float(len(tot_penalty_dot_forces_normals))
            # tot_penalty_friction_constraint = sum(tot_penalty_friction_constraint) / float(len(tot_penalty_friction_constraint))
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            tot_penetration_depth = sum(tot_penetration_depth) / float(len(tot_penetration_depth))
            robo_actions_diff_loss = sum(robo_actions_diff_loss) / float(len(robo_actions_diff_loss))
            mano_tracking_loss = sum(mano_tracking_loss) / float(len(mano_tracking_loss))
            
            if i_iter % self.report_freq == 0:
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} tracking_loss = {} mano_tracking_loss = {} penetration_depth = {} actions_diff_loss = {} lr={}'.format(self.iter_step, tot_losses, tot_tracking_loss, mano_tracking_loss, tot_penetration_depth, robo_actions_diff_loss, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                ''' Dump to the file '''
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                # spring_ks_values, inertia_div_factor
                print(f"spring_ks_values: {self.other_bending_network.spring_ks_values.weight.data.detach().cpu().numpy()}, inertia_div_factor: {self.other_bending_network.inertia_div_factor.weight.data.detach().cpu().numpy()}")
            
            ## mano tracking loss = {} ## # 
            # mano tracking loss = {} ? # # mano tracking loss # 
            # get the robo intermediates states # # robot #
            # robo_intermediates_states = np.stack(robo_intermediates_states, axis=0) ### robot intermediate states ###
            # robo_intermediates_states = torch.from_numpy(robo_intermediates_states).float() # ## intermediate states #
            # self.robo_intermediates_states = robo_intermediates_states
            
            # self.validate_mesh_robo_a()
            if i_iter % self.val_mesh_freq == 0:
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                
            
            ## test cases ## ## test cases ##
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
    
    
    ''' ARCTIC clips '''
    def train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states(self, ):
        
        # chagne # # mano notjmano but the mano ---> optimize the mano delta states? # 
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate() # update learning rrate # 
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load 
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path'] # 
        robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        
        
        
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        ''' Load robot hand in DiffHand simulator '''
        redmax_sim = redmax.Simulation(model_path)
        redmax_sim.reset(backward_flag = True) # redmax_sim -- 
        # ### redmax_ndof_u, redmax_ndof_r ### #
        redmax_ndof_u = redmax_sim.ndof_u
        redmax_ndof_r = redmax_sim.ndof_r
        redmax_ndof_m = redmax_sim.ndof_m ### ndof_m ### # redma # x_sim 
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        # mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano) # robot #
        mano_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path_mano) ## model path mano ## # 
        self.mano_agent = mano_agent
        # ''' Load the mano hand '''
        
        self.robo_hand_faces = self.mano_agent.robot_faces
        
        if self.use_mano_hand_for_test:
            self.robo_hand_faces = self.hand_faces
        # self.
        
        # mano_agent #
        
        nn_substeps = 10
        
        mano_nn_substeps = 1
        # mano_nn_substeps = 10 # 
        self.mano_nn_substeps = mano_nn_substeps
        
        # self.hand_faces # 
        
        
        
        # params to train ##
        
        
        
        
        ''' Expnad the current visual points ''' 
        # expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        # self.expanded_visual_pts_nn = expanded_visual_pts.size(0)
        # expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        # expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        # np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        # # ''' Expnad the current visual points '''  #  # differentiate through the simulator? # # 
        
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        ''' Define MANO robot actions, delta_states, init_states, frictions, and others '''
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        self.mano_robot_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_states.weight)
        self.mano_robot_states.weight.data[0, :] = self.mano_robot_init_states.weight.data[0, :].clone()
        
        
        # self.free_deformation_time_latent = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        # )
        # params_to_train += list(self.free_deformation_time_latent.parameters())
        
        
        # self.deformation_input_dim = 3 + self.bending_latent_size
        # self.deformation_output_dim = 3
        # ### free deformation network ### # free deformation # 
        # self.free_deformation_network = self.construct_field_network(self.deformation_input_dim, self.bending_latent_size,  self.deformation_output_dim)
        # params_to_train += list(self.free_deformation_network.parameters())
        
        
        ''' Load optimized MANO hand actions and states '''
        # ### laod optimized init actions ####
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # cur_optimized_init_actions = # optimized init states 
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            
            if 'mano_robot_states' in optimized_init_actions_ckpt:
                self.mano_robot_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_states'])
            
            self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_init_states'])
            
            
            if optimized_init_actions_ckpt['mano_robot_glb_rotation']['weight'].size(0) == num_steps:
                # weight nn_steps x xxx # 
                for i_cur_step in range(num_steps):
                    self.mano_robot_glb_rotation.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 1) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_glb_rotation']['weight'][i_cur_step: i_cur_step + 1, :]
                    
                    #### set mano glb rotations ###
                    if i_cur_step < num_steps - 1:
                        for i_mano_substep in range(mano_nn_substeps):
                            
                            cur_substep_trans = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step, :] + (optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step + 1, :] - optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step, :]) / float(mano_nn_substeps) * float(i_mano_substep)
                            self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps + i_mano_substep, :] = cur_substep_trans
                    else:
                        self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 2) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step: i_cur_step + 1, :]
                    # self.mano_robot_glb_trans.weight.data[(i_cur_step + ) * mano_nn_substeps + i_mano_substep, :] = cur_substep_trans
                    
                    
                    ### 
                    # self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps: i_cur_step * mano_nn_substeps + 1] = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step: i_cur_step + 1, :]
                    
                    
                    # self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 1) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step: i_cur_step + 1, :]
                    
                    ### delta states trans ###
                    #### delta states --- the current timestteps  ###
                    # for i_substep in range(mano_nn_substeps):
                    
                    self.mano_robot_delta_states.weight.data[i_cur_step * mano_nn_substeps: i_cur_step * mano_nn_substeps + 1] = optimized_init_actions_ckpt['mano_robot_delta_states']['weight'][i_cur_step: i_cur_step + 1, :]
                    #### delta states --- set to zero ###
                    self.mano_robot_delta_states.weight.data[i_cur_step * mano_nn_substeps + 1: (i_cur_step + 1) * mano_nn_substeps] = 0. # optimized_init_actions_ckpt['mano_robot_delta_states']['weight'][i_cur_step: i_cur_step + 1, :]
                    
                    # if self.use_mano_hand_for_test:
                        
                    
                    
                    # self.mano_robot_delta_states.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 1) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_delta_states']['weight'][i_cur_step: i_cur_step + 1, :]
            else:
                self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_rotation'])
                if 'robot_delta_states' in optimized_init_actions_ckpt:
                    self.mano_robot_delta_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_delta_states'])
                if 'mano_robot_actions' in optimized_init_actions_ckpt:
                    self.mano_robot_actions.load_state_dict(optimized_init_actions_ckpt['mano_robot_actions'])
                # self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
                self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_trans'])
            # if 'expanded_actuator_friction_forces' in optimized_init_actions_ckpt:
            #     try:
            #         self.mano_expanded_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_friction_forces'])
            #     except:
            #         pass
        #     self.mano_expanded_actuator_delta_offset.load_state_dict(optimized_init_actions_ckpt['expanded_actuator_delta_offset'])
        # # ### load init transformations ckpts ### #
        
        # robot actions #
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        ## ##
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        ## robot actuator friction #
        # robot actuator friction forces #
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        params_to_train = []
        
        ''' Only set the robot friction forces optimizable '''
        if not self.optimize_rules:
            params_to_train += list(self.robot_actions.parameters())
            params_to_train += list(self.robot_delta_states.parameters())
            params_to_train += list(self.robot_init_states.parameters())
            params_to_train += list(self.robot_glb_rotation.parameters())
            params_to_train += list(self.robot_glb_trans.parameters())
        
        ''' Add params for expanded points '''
        # params_to_train += list(self.mano_expanded_actuator_delta_offset_nex.parameters())
        # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        if self.optimize_rules:
            params_to_train += list(self.other_bending_network.parameters())
        
        
        ''' Scaling constants '''
        ### affine ###
        # self.maxx_robo_pts = 25. ##
        # self.minn_robo_pts = -15. ##
        # self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        # self.mult_const_after_cent = 0.5437551664260203
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        print(f"mano_to_dyn_corr_pts_idxes: {self.mano_to_dyn_corr_pts_idxes.size()}")
        
        
        # maxx robo pts ##
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        self.mano_mult_const_after_cent = 3.
        
        
        
        ''' Load optimized robot hands '''
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_transformations_fn = self.conf['model.load_optimized_init_transformations']
            ##### Load optimized initial transformations #####
            optimized_init_transformations_ckpt = torch.load(cur_optimized_init_transformations_fn, map_location=self.device, )
            self.robot_init_states.load_state_dict(optimized_init_transformations_ckpt['robot_init_states'])
            self.robot_glb_rotation.load_state_dict(optimized_init_transformations_ckpt['robot_glb_rotation'])
            # if 'robot_delta_states' in optimized_init_transformations_ckpt:
            #     self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            self.robot_actions.load_state_dict(optimized_init_transformations_ckpt['robot_actions'])
            self.robot_actuator_friction_forces.load_state_dict(optimized_init_transformations_ckpt['robot_actuator_friction_forces'])
            self.robot_glb_trans.load_state_dict(optimized_init_transformations_ckpt['robot_glb_trans'])
            
            if not self.load_only_glb:
                self.robot_delta_states.load_state_dict(optimized_init_transformations_ckpt['robot_delta_states'])
            
        # if 'model.load_optimized_expanded' in self.conf and len(self.conf['model.load_optimized_expanded']) > 0:
        #     load_optimized_expanded_ckpt = self.conf['model.load_optimized_expanded']
        #     load_optimized_expanded_ckpt = torch.load(load_optimized_expanded_ckpt, map_location=self.device)
        #     self.mano_expanded_actuator_delta_offset.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset'])
        #     self.mano_expanded_actuator_friction_forces.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_friction_forces'])
        #     if 'mano_expanded_actuator_delta_offset_nex' in load_optimized_expanded_ckpt:
        #         print(f"Loading expanded actuator delta offset nex from the saved checkpoint...")
        #         self.mano_expanded_actuator_delta_offset_nex.load_state_dict(load_optimized_expanded_ckpt['mano_expanded_actuator_delta_offset_nex'])
        
        
        
        # robot_glb_rotation, robot_glb_trans #
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        
        
        # # train the robot or train the expanded points #
        # self.mano_fingers = [745, 279, 320, 444, 555, 672, 234, 121]
        # self.robot_fingers = [521, 624, 846, 973, 606, 459, 383, 265]
        
        # # optimize with intermediates #
        # self.mano_fingers = torch.tensor(self.mano_fingers, dtype=torch.long).cuda()
        # self.robot_fingers = torch.tensor(self.robot_fingers, dtype=torch.long).cuda()
        
        self.nn_ts = self.nn_timesteps - 1
        # self.optimize_with_intermediates = False
        
        # search for the nearest point so that it can manipulate the object #
        ## TODO: co-optimize the bending network weights? ## # optimize active object is true ##
        # if self.optimize_with_intermediates and (not self.optimize_active_object): # optimize with intermediates #
        #     # if not self.optimize_active_object: #
        #     # free_deformation_time_latent, free_deformation_network #
        #     params_to_train = []
        #     params_to_train += list(self.other_bending_network.parameters())
        #     params_to_train += list(self.robot_actuator_friction_forces.parameters())
        #     # params_to_train += list(self.expanded_actuator_friction_forces.parameters()) # expanded actuator 
        
        ''' Set actions for the redmax simulation and add parameters to params-to-train '''
        # robot actions #
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        
        self.redmax_robot_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        
        params_to_train = []
        params_to_train += list(self.redmax_robot_actions.parameters())


        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        # init_rot = R.random().as_quat()
        # init_rot = torch.from_numpy(init_rot).float().cuda()
        # self.robot_glb_rotation.weight.data[0, :] = init_rot[:] # init rot
        
        
        
        ### Constraint set ###
        self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        self.timestep_to_active_mesh = {}
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts #
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        
        self.timestep_to_active_mesh_w_delta_states = {}
        
        
        # states -> get states -> only update the acitons #
        with torch.no_grad():
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                ''' Get rotations, translations, and actions of the current robot '''
                cur_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7) # mano glb rot
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                if cur_ts == 0:
                    links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    self.mano_agent.set_init_states_target_value(links_init_states)
                    cur_visual_pts = self.mano_agent.get_init_state_visual_pts()
                else:
                    if self.conf['model.train_states']:
                        cur_state = self.mano_robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        states = {}
                        states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        states['link_states'] = cur_state
                        # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                        # self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                        self.mano_agent.active_robot.set_delta_state_and_update(cur_state, cur_ts)
                    else:
                        actions = {}
                        actions['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                        actions['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                        actions_link_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        actions['link_actions'] = actions_link_actions
                        self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                    
                    cur_visual_pts = self.mano_agent.get_init_state_visual_pts()
                    # if cur_ts == 10:
                    #     cur_visual_pts, link_name_to_transformations_and_transformed_pts = self.robot_agent.get_init_state_visual_pts(ret_link_name_to_tansformations=True)
                    # else:
                    #     cur_visual_pts = self.robot_agent.get_init_state_visual_pts()

                ### transform the visual pts ###
                # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                # cur_visual_pts = cur_visual_pts * 2. -1.
                # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                
                cur_visual_pts = cur_visual_pts * self.mano_mult_const_after_cent
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                # if not self.use_mano_inputs:
                if self.use_mano_hand_for_test:
                    self.timestep_to_active_mesh[cur_ts] = self.rhand_verts[cur_ts].detach()
                else:
                    self.timestep_to_active_mesh[cur_ts] = cur_visual_pts.detach()
                self.timestep_to_active_mesh_w_delta_states[cur_ts] = cur_visual_pts.detach()
                # timestep_to_active_mesh_opt_ours_sim #
                # self.timestep_to_active_mesh_opt_ours_sim[cur_ts] = cur_visual_pts[sampled_verts_idxes].detach()
                self.timestep_to_active_mesh_opt_ours_sim[cur_ts] = cur_visual_pts.detach()
                
        self.iter_step = 0
        
        ''' Set redmax robot actions '''
        # nn_iters_opt_redmax_actions = 10000
        # nn_iters_opt_redmax_actions = 100
        # nn_iters_opt_redmax_actions = 17
        
        params_to_train_actions = []
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps * nn_substeps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        params_to_train_actions += list(self.redmax_robot_actions.parameters())
        
        self.actions_optimizer = torch.optim.Adam(params_to_train_actions, lr=self.learning_rate)
        
        params_to_train_kines = []
        # params_to_train_kines += list(self.mano_robot_glb_rotation.parameters())
        # params_to_train_kines += list(self.mano_robot_glb_trans.parameters())
        # params_to_train_kines += list(self.mano_robot_delta_states.parameters())
        params_to_train_kines += list(self.mano_robot_states.parameters())
        
        
        # can tray the optimizer ##
        if self.use_LBFGS:
            self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
        else:
            self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        
        if self.optimize_rules:
            params_to_train_kines = []
            params_to_train_kines += list(self.other_bending_network.parameters())
            # self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            if self.use_LBFGS:
                self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
            else:
                self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            # self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=1e-2)
        
        # spring_ks_values, inertia_div_factor
        # self.other_bending_network.spring_ks_values.weight.data[:, :] = 0.1395
        # self.other_bending_network.spring_ks_values.weight.data[0, :] = 0.1395
        # self.other_bending_network.spring_ks_values.weight.data[1, :] = 0.00
        # self.other_bending_network.inertia_div_factor.weight.data[:, :] = 10.0
        # self.other_bending_network.inertia_div_factor.weight.data[:, :] = 1000.0
        # self.other_bending_network.inertia_div_factor.weight.data[:, :] = 100.0
        
        # load redma robot actions #
        load_redmax_robot_actions_fn = "/data3/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_diffh_subs_10_/checkpoints/redmax_robot_actions_ckpt_001900.pth"
        if len(load_redmax_robot_actions_fn) > 0:
            redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
            self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        

        ''' Load optimized redmax robot actions '''
        # load_redmax_robot_actions_fn = "/data3/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_diffh_subs_10_/checkpoints/redmax_robot_actions_ckpt_001900.pth"
        # if len(load_redmax_robot_actions_fn) > 0:
        #     redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
        #     self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        # # self. # # 
        
        # use


        # model_path = self.conf['model.sim_model_path']
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        # self.robot_agent = robot_agent
        
        # delta states -> 
        
        # the simulator env?  #
        ### load optimized actions ###
        # if 'model.load_redmax_robot_actions' in self.conf and len(self.conf['model.load_redmax_robot_actions']) > 0: 
        #     load_redmax_robot_actions_fn = self.conf['model.load_redmax_robot_actions']
        #     redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
        #     self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
        
        # robot_glb_rotation, robot_glb_trans #
        # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
        # ori_redmax_robot_actions = self.redmax_robot_actions.weight.data.clone()    
        # ori_redmax_glb_rot = self.robot_glb_rotation.weight.data.clone()
        # ori_redmax_glb_trans = self.robot_glb_trans.weight.data.clone()
        # ori_robot_delta_states = self.robot_delta_states.weight.data.clone()
        
        
        # # ori_mano_robot_glb_rot, ori_mano_robot_glb_trans, ori_mano_robot_delta_states #
        ori_mano_robot_glb_rot = self.mano_robot_glb_rotation.weight.data.clone()
        ori_mano_robot_glb_trans = self.mano_robot_glb_trans.weight.data.clone()
        ori_mano_robot_delta_states = self.mano_robot_delta_states.weight.data.clone()
        
        
        self.iter_step = 0
        # print()
        # penetration_forces = None
        # self.validate_mesh_robo_a() # valida
        
        # finger_sampled_idxes = None
        
        for i_iter in tqdm(range(100000)):
            tot_losses = []
            tot_tracking_loss = []
            
            # timestep 
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # self.timestep_to_
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            correspondence_pts_idxes = None
            
            # tot_penetrating_depth_penalty = []
            # tot_ragged_dist = []
            # tot_delta_offset_reg_motion = []
            # tot_dist_mano_visual_ori_to_cur = []
            # tot_reg_loss = []
            # tot_diff_cur_states_to_ref_states = []
            # tot_diff_tangential_forces = []
            # penetration_forces = None
            # sampled_visual_pts_joint_idxes = None
            
            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_force_d = {}
            self.ts_to_passive_normals = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            self.ts_to_redmax_states = {}
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16]
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            
            redmax_sim.reset(backward_flag = True)
            
            # tot_grad_qs = []
            
            robo_intermediates_states = []
            
            tot_penetration_depth = []
            
            robo_actions_diff_loss = []
            mano_tracking_loss = []
            
            # init global transformations ##
            # cur_ts_redmax_delta_rotations = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_delta_rotations = torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda()
            
            # for cur_ts in range(self.nn_ts):
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                tot_redmax_actions = []
                
                # # redmax_robot_states # # # #
                # self.redmax_robot_states.weight.data[:, :] = 0.
                # if self.redmax_robot_states.weight.grad is not None:
                #     self.redmax_robot_states.weight.grad.data[:, :] =  self.redmax_robot_states.weight.grad.data[:, :] * 0.
                # mano_robot_glb_rotation, mano_robot_glb_trans, mano_robot_init_states, mano_robot_delta_states #
                    
                actions = {}
                
                self.free_def_bending_weight = 0.0

                # mano_robot_glb_rotation, mano_robot_glb_trans, mano_robot_delta_states #
                cur_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                cur_glb_rot = cur_glb_rot + cur_ts_redmax_delta_rotations
                
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                # cur_glb_rot_quat = cur_glb_rot.clone()
                
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                # cur_ts_redmax_delta_rotations, cur_ts_redmax_robot_trans # 
                # cur_ts_delta_rot, cur_ts_redmax_robot_trans #
                # # print(f"ts: {cur_ts}, cur_ts_redmax_delta_rotations: {cur_ts_redmax_delta_rotations}, cur_glb_rot: {cur_glb_rot_quat}")
                # cur_ts_delta_rot = dyn_model_act.quaternion_to_matrix(cur_ts_redmax_delta_rotations) # mano glboa
                # # cur_ts_delta_trans = 
                
                # # # cur_ts_delta_rot, cur_ts_redmax_robot_trans # # #
                # cur_glb_rot = torch.matmul(cur_ts_delta_rot, cur_glb_rot)
                cur_glb_trans = cur_glb_trans + cur_ts_redmax_robot_trans # 

                link_cur_states = self.mano_robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                self.mano_agent.set_init_states_target_value(link_cur_states)
                cur_visual_pts = self.mano_agent.get_init_state_visual_pts() 

                
                # if cur_ts == 0:
                #     links_init_states = self.mano_robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    
                #     self.mano_agent.set_init_states_target_value(links_init_states)
                    
                #     # print(f"cur_ts: {cur_ts}, calculating mano agent points...")
                #     cur_visual_pts = self.mano_agent.get_init_state_visual_pts() # get the visual points # 
                    
                #     #### robot agent ####
                #     # self.robot_agent.set_init_states_target_value(links_init_states)
                #     # cur_visual_pts, link_name_to_link_transformations, visual_pts_joint_idxes = self.robot_agent.get_init_state_visual_pts(True, True)
                    
                #     ''' Set init q of the redmax ''' # set init q of the redmax # 
                #     # # ### redmax_ndof_u, redmax_ndof_r ### #
                #     # redmax_q_init = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
                #     # redmax_q_init = redmax_q_init[:redmax_ndof_r]
                    
                #     # # # 
                #     # redmax_sim.set_q_init(redmax_q_init)
                    
                #     # redmax_q_init_th = torch.from_numpy(redmax_q_init).float().cuda()
                #     # self.redmax_robot_states.weight.data[cur_ts, :redmax_q_init_th.size(0)] = redmax_q_init_th[:]
                    
                #     # self.ts_to_redmax_states[cur_ts] = redmax_q_init.copy()
                    
                # else:
                #     if self.conf['model.train_states']: # 
                #         # if self.add_delta_state_constraints:
                #         #     self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg] = torch.clamp(self.robot_delta_states.weight.data[cur_ts, self.robot_hand_states_only_allowing_neg], max=0.)
                        
                #         #  add delta states # 
                #         cur_state = self.mano_robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                #         # self.redmax_robot_states.weight.data[0, :cur_state.]
                #         states = {}
                #         states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                #         states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                #         states['link_states'] = cur_state
                #         # self.robot_agent.set_actions_and_update_states(actions=actions, cur_timestep=cur_ts - 1)
                #         # self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)
                        
                #         self.mano_agent.active_robot.set_delta_state_and_update(cur_state, cur_ts)
                #     else:
                #         for i_sub_step in range(nn_substeps):
                #             cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
                #             redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
                #             redmax_u = redmax_actions.detach().cpu().numpy()
                #             redmax_u = redmax_u[: redmax_ndof_u]
                #             redmax_sim.set_u(redmax_u) # sim
                #             tot_redmax_actions.append(redmax_actions)
                #             ''' set and set the virtual forces ''' ## redmax_sim
                #             ###  TODO: get and set the virtual forces ###
                #             # link_maximal_contact_forces # 
                #             ### TODO: add the virtual force objects and set the virtual forces ###
                #             # virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                            
                #             ''' Set virtual forces ''' ### set the 
                #             virtual_force = link_maximal_contact_forces.detach().cpu().numpy()
                #             virtual_force = np.reshape(virtual_force, (virtual_force.shape[0] * virtual_force.shape[1]))
                #             # # virtual forces # #
                #             redmax_sim.set_forces(virtual_force) ### virtual forces ###

                #             redmax_sim.forward(1, verbose = False)
                #             # 
                            
                #             # redmax_sim_q = redmax_sim.get_q()
                #             # if i_sub_step == nn_substeps - 1:
                #             #     robo_intermediates_states.append(redmax_sim_q)
                #             #     df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts - 1]))
                #             #     cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts - 1]) ** 2)
                #             #     tot_diff_qs.append(cur_diff_qs.item())
                #             # else:
                #             #     df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                #         redmax_sim_q = redmax_sim.get_q()
                #         robo_intermediates_states.append(redmax_sim_q)
                        
                #         # print(f"self.redmax_robot_states.weight: {self.redmax_robot_states.weight.data.size()}")
                #         # self.redmax_robot_states.weight[cur_ts, :redmax_sim_q.shape[0]] = torch.from_numpy(redmax_sim_q).float().cuda()[:]
                        
                        
                #         redmax_robot_states_th = torch.from_numpy(redmax_sim_q).float().cuda()
                #         redmax_robot_states_th.requires_grad = True
                #         redmax_robot_states_th.requires_grad_ = True
                #         cur_delta_state = redmax_robot_states_th - (torch.from_numpy(robo_intermediates_states[-1 - 1]).float().cuda() if cur_ts > 1 else torch.from_numpy(redmax_q_init).float().cuda())
                        
                        
                #         cur_delta_state = cur_delta_state.squeeze(0)[:redmax_ndof_r]
                #         # cur_delta_state = redmax_sim_q - (robo_intermediates_states[-1 - 1] if cur_ts > 1 else redmax_q_init)
                #         # cur_delta_state = torch.from_numpy(cur_delta_state).float().cuda()
                        
                #         states = {}
                #         states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                #         states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                #         states['link_states'] = cur_delta_state
                #         self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)    
                #         # cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                        
                #     # cur visual pts #
                #     # cur_visual_pts, link_name_to_link_transformations, visual_pts_joint_idxes = self.robot_agent.get_init_state_visual_pts(True, True)
                #     cur_visual_pts = self.mano_agent.get_init_state_visual_pts() 



                ### transform the visual pts ###
                # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                # cur_visual_pts = cur_visual_pts * 2. - 1.  # cur visual pts # 
                # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                
                cur_visual_pts = cur_visual_pts * self.mano_mult_const_after_cent
                
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                ## 
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) 
                
                
                
                # diff_redmax_visual_pts_with_ori_visual_pts = torch.sum(
                #     (cur_visual_pts[sampled_verts_idxes] - self.timestep_to_active_mesh_opt_ours_sim[cur_ts].detach()) ** 2, dim=-1
                # )
                # diff_redmax_visual_pts_with_ori_visual_pts = diff_redmax_visual_pts_with_ori_visual_pts.mean()
                
                if self.use_mano_hand_for_test:
                    self.timestep_to_active_mesh[cur_ts] = self.rhand_verts[cur_ts] # .detach()
                else:
                    self.timestep_to_active_mesh[cur_ts] = cur_visual_pts
                self.timestep_to_raw_active_meshes[cur_ts] = cur_visual_pts.detach().cpu().numpy()
                
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                
                def evaluate_tracking_loss():
                    self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=self.contact_pairs_set)
                    
                    # init states  
                    # cur_ts % mano_nn_substeps == 0:
                    if (cur_ts + 1) % mano_nn_substeps == 0:
                        cur_passive_big_ts = cur_ts // mano_nn_substeps
                        in_func_tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_passive_big_ts + 1)
                        # tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                    else:
                        in_func_tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    return in_func_tracking_loss

                    
                if contact_pairs_set is None:
                    self.contact_pairs_set = None
                else:
                    self.contact_pairs_set = contact_pairs_set.copy()
                
                # print(f"start fowarding....")
                ### use only the mano hand points ###
                contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)

                # print(f"after fowarding....") # maintain the contacts ####
                # 
                if self.train_with_forces_to_active and (not self.use_mano_inputs): # use mano inputs #
                    # penetration_forces #
                    if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                        net_penetrating_forces = self.other_bending_network.penetrating_forces
                        net_penetrating_points = self.other_bending_network.penetrating_points
                        # cur_penetrating_forces = []
                        # for i_pts in range(net_penetrating_points.size(0)):
                        #     cur_penetration_point = net_penetrating_points[i_pts]
                        #     cur_penetration_point_forces = net_penetrating_forces[i_pts]
                        #     cur_penetrating_forces.append((cur_penetration_point, cur_penetration_point_forces))
                        # penetration_forces = cur_penetrating_forces
                        
                        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                        self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                        self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                        
                        
                        ### transform the visual pts ###
                        # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        # cur_visual_pts = cur_visual_pts * 2. - 1.
                        # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                        
                        # sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[finger_sampled_idxes][self.other_bending_network.penetrating_indicator]
                        
                        # sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[self.other_bending_network.penetrating_indicator]
                        
                        
                        
                        net_penetrating_forces = torch.matmul(
                            cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                        net_penetrating_forces = net_penetrating_forces / 2
                        net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                        
                        net_penetrating_points = torch.matmul(
                            cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                        net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                        net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                        
                        
                        # penetration_forces = {
                        #     'penetration_forces': net_penetrating_forces, 'penetration_forces_points': net_penetrating_points
                        # }
                        
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        # self.robot_agent.set_maximal_contact_forces(link_maximal_contact_forces)
                        # self.robot_agent.set_penetration_forces(penetration_forces, sampled_visual_pts_joint_idxes, link_maximal_contact_forces)
                        
                    else:
                        # penetration_forces = None
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        
                if contact_pairs_set is not None:
                    self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                
                # ts to 
                # contact force d 
                self.ts_to_passive_normals[cur_ts] = self.other_bending_network.cur_passive_obj_ns.detach().cpu().numpy()
                self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                if self.other_bending_network.penalty_based_friction_forces is not None:
                    self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                
                # contact 
                # changing cotnact points # 
                # easy to differentiate through cotnacts # 
                # maintain thee contact points # 
                # approaching and contacting ? # --- #
                # approaching and contacting ? # --- # contacting and approaching #  
                # active points; points; # --- in contact points # 
                # and calculate forces # 
                # if they are no longer in contacts # --- 
                # self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                # tot penetration depth
                # # get the penetration depth of the bending network #
                tot_penetration_depth.append(self.other_bending_network.penetrating_depth_penalty.detach().item())
                
                ### optimize with intermediates ### # optimize with intermediates # 
                # if self.optimize_with_intermediates:
                #     tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1) # 
                # else:
                #     tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    
                
                # init states  
                # cur_ts % mano_nn_substeps == 0:
                if (cur_ts + 1) % mano_nn_substeps == 0:
                    cur_passive_big_ts = cur_ts // mano_nn_substeps
                    tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_passive_big_ts + 1)
                    tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                else:
                    tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()

                hand_tracking_loss = torch.sum(
                    (self.timestep_to_active_mesh_w_delta_states[cur_ts] - cur_visual_pts) ** 2, dim=-1
                )
                hand_tracking_loss = hand_tracking_loss.mean()
                
                
                loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                # diff_redmax_visual_pts_with_ori_visual_pts.backward()
                penetraton_penalty = self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                
                
                # print(f"after calculating losses....")
                
                # # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
                # robot_rotation_diff = torch.sum(
                #     (ori_redmax_glb_rot - self.robot_glb_rotation.weight) ** 2, dim=-1
                # )
                # robot_rotation_diff = robot_rotation_diff.mean()
                
                # robot_trans_diff = torch.sum(
                #     (ori_redmax_glb_trans - self.robot_glb_trans.weight) ** 2, dim=-1
                # )
                # robot_trans_diff = robot_trans_diff.mean()
                
                # robot_delta_states_diff = torch.sum(
                #     (ori_robot_delta_states - self.robot_delta_states.weight) ** 2, dim=-1
                # )
                # robot_delta_states_diff = robot_delta_states_diff.mean()
                
                
                # mano_robot_glb_rotation, mano_robot_glb_trans, mano_robot_delta_states #
                # ori_mano_robot_glb_rot, ori_mano_robot_glb_trans, ori_mano_robot_delta_states #
                mano_robot_rotation_diff = torch.sum(
                    (ori_mano_robot_glb_rot - self.mano_robot_glb_rotation.weight) ** 2, dim=-1
                )
                mano_robot_rotation_diff = mano_robot_rotation_diff.mean()
                
                mano_robot_trans_diff = torch.sum(
                    (ori_mano_robot_glb_trans - self.mano_robot_glb_trans.weight) ** 2, dim=-1
                )
                mano_robot_trans_diff = mano_robot_trans_diff.mean()
                
                mano_robot_delta_states_diff = torch.sum(
                    (ori_mano_robot_delta_states - self.mano_robot_delta_states.weight) ** 2, dim=-1
                )
                mano_robot_delta_states_diff = mano_robot_delta_states_diff.mean()
                
                
                kinematics_trans_diff = (mano_robot_rotation_diff + mano_robot_trans_diff + mano_robot_delta_states_diff) * self.robot_actions_diff_coef
                
                # kinematics_trans_diff = (robot_rotation_diff + robot_trans_diff + robot_delta_states_diff) * self.robot_actions_diff_coef
                
                # delta # rhand vertices #
                if cur_ts % mano_nn_substeps == 0:
                    cur_kine_rhand_verts = self.rhand_verts[cur_ts // mano_nn_substeps]
                    cur_dyn_visual_pts_to_mano_verts = cur_visual_pts[self.mano_to_dyn_corr_pts_idxes]
                    diff_hand_tracking = torch.mean(
                        torch.sum((cur_kine_rhand_verts - cur_dyn_visual_pts_to_mano_verts) ** 2, dim=-1)
                    )
                else:
                    diff_hand_tracking = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                    
                
                
                # kinematics_proj_loss = kinematics_trans_diff + penetraton_penalty + tracking_loss + diff_hand_tracking
                
                # kinematics_proj_loss = kinematics_trans_diff + penetraton_penalty + diff_hand_tracking * 1e2 + tracking_loss
                
                # kinematics_proj_loss = kinematics_trans_diff + penetraton_penalty + diff_hand_tracking * 1e2 + tracking_loss
                
                # diff_hand_tracking_coef
                kinematics_proj_loss = kinematics_trans_diff + penetraton_penalty + diff_hand_tracking * self.diff_hand_tracking_coef + tracking_loss
                
                if self.use_mano_hand_for_test:
                    kinematics_proj_loss = tracking_loss
                
                kinematics_proj_loss = hand_tracking_loss * 1e2
                
                
                # loss = loss # + (robot_rotation_diff + robot_trans_diff) * self.robot_actions_diff_coef
                
                loss = kinematics_proj_loss
                
                
                # diff_hand_tracking # diff hand ## 
                mano_tracking_loss.append(diff_hand_tracking.detach().cpu().item())
                
                
                if contact_pairs_set is not None:
                    self.kines_optimizer.zero_grad()
                    
                    kinematics_proj_loss.backward(retain_graph=True)
                    
                    if self.use_LBFGS:
                        self.kines_optimizer.step(evaluate_tracking_loss) # 
                    else:
                        self.kines_optimizer.step()

                # 
                # tracking_loss.backward(retain_graph=True)
                if self.use_LBFGS:
                    self.other_bending_network.reset_timestep_to_quantities(cur_ts)
                
                
                robot_states_actions_diff_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                robo_actions_diff_loss.append(robot_states_actions_diff_loss)

                
                tot_losses.append(loss.detach().item()) # total losses # # total losses # 
                # tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                # tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                self.update_learning_rate() ## update learning rate ##
                
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache() 
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                torch.cuda.empty_cache()
                
            
            ''' Get nn_forward_ts and backward through the actions for updating '''
            
            
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            # tot_penalty_dot_forces_normals = sum(tot_penalty_dot_forces_normals) / float(len(tot_penalty_dot_forces_normals))
            # tot_penalty_friction_constraint = sum(tot_penalty_friction_constraint) / float(len(tot_penalty_friction_constraint))
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            tot_penetration_depth = sum(tot_penetration_depth) / float(len(tot_penetration_depth))
            robo_actions_diff_loss = sum(robo_actions_diff_loss) / float(len(robo_actions_diff_loss))
            mano_tracking_loss = sum(mano_tracking_loss) / float(len(mano_tracking_loss))
            
            if i_iter % self.report_freq == 0:
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} tracking_loss = {} mano_tracking_loss = {} penetration_depth = {} actions_diff_loss = {} lr={}'.format(self.iter_step, tot_losses, tot_tracking_loss, mano_tracking_loss, tot_penetration_depth, robo_actions_diff_loss, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                ''' Dump to the file '''
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                # spring_ks_values, inertia_div_factor
                print(f"spring_ks_values: {self.other_bending_network.spring_ks_values.weight.data.detach().cpu().numpy()}, inertia_div_factor: {self.other_bending_network.inertia_div_factor.weight.data.detach().cpu().numpy()}")
            
            ## mano tracking loss = {} ## # 
            # mano tracking loss = {} ? # # mano tracking loss # 
            # get the robo intermediates states # # robot #
            # robo_intermediates_states = np.stack(robo_intermediates_states, axis=0) ### robot intermediate states ###
            # robo_intermediates_states = torch.from_numpy(robo_intermediates_states).float() # ## intermediate states #
            # self.robo_intermediates_states = robo_intermediates_states
            
            # self.validate_mesh_robo_a()
            if i_iter % self.val_mesh_freq == 0:
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                
            
            ## test cases ## ## test cases ##
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache() # empty cache # # empty cache # #
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
    
    
    ''' GRAB clips; MANO hand '''
    def train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states_grab(self, ):
        
        # states -> the robot actions --- in this sim ##
        # chagne # # mano notjmano but the mano ---> optimize the mano delta states? # 
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load #
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path'] # 
        robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        
        
        ## sampled verts idxes ## 
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        ''' Load robot hand in DiffHand simulator '''
        redmax_sim = redmax.Simulation(model_path)
        redmax_sim.reset(backward_flag = True) # redmax_sim -- 
        # ### redmax_ndof_u, redmax_ndof_r ### #
        redmax_ndof_u = redmax_sim.ndof_u
        redmax_ndof_r = redmax_sim.ndof_r
        redmax_ndof_m = redmax_sim.ndof_m
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        # mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano) # robot #
        mano_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path_mano) ## model path mano ## # 
        self.mano_agent = mano_agent
        # ''' Load the mano hand '''
        
        self.robo_hand_faces = self.mano_agent.robot_faces
        
        if self.use_mano_hand_for_test:
            self.robo_hand_faces = self.hand_faces
        
        
        nn_substeps = 10
        
        mano_nn_substeps = 1
        # mano_nn_substeps = 10 # 
        self.mano_nn_substeps = mano_nn_substeps
        
        # self.hand_faces # 

        
        ''' Expnad the current visual points ''' 
        # expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        # self.expanded_visual_pts_nn = expanded_visual_pts.size(0)
        # expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        # expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        # np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        # # ''' Expnad the current visual points '''  #  # differentiate through the simulator? # # 
        
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        ''' Define MANO robot actions, delta_states, init_states, frictions, and others '''
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        self.mano_robot_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60, # embedding; a realistic thing # # ## so the optimizable modle deisgn --- approxmimate what you see and approximate the target simulator ## # at a distance; the asymmetric contact froces spring ks -- all of them wold affect model's behaviours ##
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_states.weight)
        self.mano_robot_states.weight.data[0, :] = self.mano_robot_init_states.weight.data[0, :].clone()
        
        
        # self.free_deformation_time_latent = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        # )
        # params_to_train += list(self.free_deformation_time_latent.parameters())
        
        
        # self.deformation_input_dim = 3 + self.bending_latent_size
        # self.deformation_output_dim = 3
        # ### free deformation network ### # free deformation # 
        # self.free_deformation_network = self.construct_field_network(self.deformation_input_dim, self.bending_latent_size,  self.deformation_output_dim)
        # params_to_train += list(self.free_deformation_network.parameters())
        
        
        ''' Load optimized MANO hand actions and states '''
        # ### laod optimized init actions #### # 
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # b and c and a #
            # cur_optimized_init_actions = # optimized init states # no solution in this sim? #
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            
            if 'mano_robot_states' in optimized_init_actions_ckpt:
                self.mano_robot_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_states'])
            
            if 'mano_robot_init_states' in optimized_init_actions_ckpt:
                self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_init_states'])
                
            if 'mano_robot_glb_rotation' in optimized_init_actions_ckpt:
                self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_rotation'])
            
            if 'mano_robot_glb_trans' in optimized_init_actions_ckpt: # mano_robot_glb_trans
                self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_trans'])
        
        
        # robot actions # # 
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        ##  ##
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        ## overfitting to a single sequence ##
        ### local minimum -> ## robot 
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        params_to_train = []
        
        ''' Only set the robot friction forces optimizable '''
        if not self.optimize_rules:
            params_to_train += list(self.robot_actions.parameters())
            params_to_train += list(self.robot_delta_states.parameters())
            params_to_train += list(self.robot_init_states.parameters())
            params_to_train += list(self.robot_glb_rotation.parameters())
            params_to_train += list(self.robot_glb_trans.parameters())
        
        ''' Add params for expanded points '''
        ## prams to train ## ## params to train ## ## 
        # params_to_train += list(self.mano_expanded_actuator_delta_offset_nex.parameters())
        # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        
        if self.optimize_rules:
            params_to_train += list(self.other_bending_network.parameters())
        
        
        ''' Scaling constants '''
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        print(f"mano_to_dyn_corr_pts_idxes: {self.mano_to_dyn_corr_pts_idxes.size()}")
        
        
        # maxx robo pts ##
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        self.mano_mult_const_after_cent = 3.
        
        if 'model.mano_mult_const_after_cent' in self.conf:
            self.mano_mult_const_after_cent = self.conf['model.mano_mult_const_after_cent']
        
        
        # robot_glb_rotation, robot_glb_trans #
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        
        
        self.nn_ts = self.nn_timesteps - 1

        ''' Set actions for the redmax simulation and add parameters to params-to-train '''
        # robot actions #
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        
        self.redmax_robot_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22,
        ).cuda()
        
        params_to_train = []
        params_to_train += list(self.redmax_robot_actions.parameters())


        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)


        
        ### Constraint set ###
        self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        self.timestep_to_active_mesh = {}
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts #
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        
        self.timestep_to_active_mesh_w_delta_states = {}
        
        
        # states -> get states -> only update the acitons #
        with torch.no_grad():
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                ''' Get rotations, translations, and actions of the current robot '''
                cur_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7) # mano glb rot
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                ## mano robot states ## mano robot states ##
                link_cur_states = self.mano_robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                self.mano_agent.set_init_states_target_value(link_cur_states)
                cur_visual_pts = self.mano_agent.get_init_state_visual_pts() 

                cur_visual_pts = cur_visual_pts * self.mano_mult_const_after_cent
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                ## 
                ### transform by the glboal transformation and the translation ### ## cur visual pts ## ## contiguous() ##
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) ## transformed pts ## 
                
                # if not self.use_mano_inputs:
                # if self.use_mano_hand_for_test:
                #     self.timestep_to_active_mesh[cur_ts] = self.rhand_verts[cur_ts].detach()
                # else:
                #     self.timestep_to_active_mesh[cur_ts] = cur_visual_pts.detach()
                self.timestep_to_active_mesh[cur_ts] = cur_visual_pts.detach() # cur visual pts #
                self.timestep_to_active_mesh_w_delta_states[cur_ts] = cur_visual_pts.detach()
                # self.timestep_to_active_mesh_opt_ours_sim[cur_ts] = cur_visual_pts[sampled_verts_idxes].detach()
                self.timestep_to_active_mesh_opt_ours_sim[cur_ts] = cur_visual_pts.detach()
                
        self.iter_step = 0
        
        ''' Set redmax robot actions '''
        
        params_to_train_actions = []
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps * nn_substeps, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        params_to_train_actions += list(self.redmax_robot_actions.parameters())
        
        self.actions_optimizer = torch.optim.Adam(params_to_train_actions, lr=self.learning_rate)
        
        params_to_train_kines = []
        # params_to_train_kines += list(self.mano_robot_glb_rotation.parameters())
        # params_to_train_kines += list(self.mano_robot_glb_trans.parameters())
        # params_to_train_kines += list(self.mano_robot_delta_states.parameters())
        params_to_train_kines += list(self.mano_robot_states.parameters())
        
        
        # can tray the optimizer ## ## mano states ##
        if self.use_LBFGS:
            self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
        else:
            self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        
        if self.optimize_rules:
            params_to_train_kines = []
            params_to_train_kines += list(self.other_bending_network.parameters())
            # self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            if self.use_LBFGS:
                self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
            else:
                self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            # self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=1e-2)
        
        ## policy and the controller ##
        # self.other_bending_network.spring_ks_values.weight.data[:, :] = 0.1395
        # self.other_bending_network.spring_ks_values.weight.data[0, :] = 0.1395
        # self.other_bending_network.spring_ks_values.weight.data[1, :] = 0.00
        # self.other_bending_network.inertia_div_factor.weight.data[:, :] = 10.0
        # self.other_bending_network.inertia_div_factor.weight.data[:, :] = 1000.0
        # self.other_bending_network.inertia_div_factor.weight.data[:, :] = 100.0
        
        
        # load_redmax_robot_actions_fn = "/data3/datasets/diffsim/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_diffh_subs_10_/checkpoints/redmax_robot_actions_ckpt_001900.pth"
        # if len(load_redmax_robot_actions_fn) > 0:
        #     redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
            # self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])

        ''' prepare for keeping the original global rotations, trans, and states '''
        ori_mano_robot_glb_rot = self.mano_robot_glb_rotation.weight.data.clone()
        ori_mano_robot_glb_trans = self.mano_robot_glb_trans.weight.data.clone()
        ori_mano_robot_delta_states = self.mano_robot_delta_states.weight.data.clone()
        
        
        self.iter_step = 0


        for i_iter in tqdm(range(100000)):
            tot_losses = []
            tot_tracking_loss = []
            
            # timestep #
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # # # #
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            # correspondence_pts_idxes = None
            
            # tot_penetrating_depth_penalty = []
            # tot_ragged_dist = []
            # tot_delta_offset_reg_motion = []
            # tot_dist_mano_visual_ori_to_cur = []
            # tot_reg_loss = []
            # tot_diff_cur_states_to_ref_states = []
            # tot_diff_tangential_forces = []
            # penetration_forces = None ###
            # sampled_visual_pts_joint_idxes = None
            
            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_passive_normals = {}
            self.ts_to_passive_normals = {}
            self.ts_to_passive_pts = {}
            self.ts_to_contact_force_d = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            self.ts_to_redmax_states = {}
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16] # pure mano hand # ## pure mano hand ##
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            
            redmax_sim.reset(backward_flag = True)
            
            # tot_grad_qs = []
            
            # robo_intermediates_states = []
            
            tot_penetration_depth = []
            
            robo_actions_diff_loss = []
            mano_tracking_loss = []
            
            # init global transformations ##
            # cur_ts_redmax_delta_rotations = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_delta_rotations = torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda()
            
            # for cur_ts in range(self.nn_ts):
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                # tot_redmax_actions = []
                # actions = {}

                self.free_def_bending_weight = 0.0

                # mano_robot_glb_rotation, mano_robot_glb_trans, mano_robot_delta_states #
                cur_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                cur_glb_rot = cur_glb_rot + cur_ts_redmax_delta_rotations
                
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                # cur_glb_rot_quat = cur_glb_rot.clone()
                
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)


                # # # cur_ts_delta_rot, cur_ts_redmax_robot_trans # # #
                # cur_glb_rot = torch.matmul(cur_ts_delta_rot, cur_glb_rot)
                cur_glb_trans = cur_glb_trans + cur_ts_redmax_robot_trans # 
                link_cur_states = self.mano_robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                self.mano_agent.set_init_states_target_value(link_cur_states)
                cur_visual_pts = self.mano_agent.get_init_state_visual_pts() 


                ### transform the visual pts ###
                # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                # cur_visual_pts = cur_visual_pts * 2. - 1.  # cur visual pts # 
                # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent
                
                cur_visual_pts = cur_visual_pts * self.mano_mult_const_after_cent
                
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                ## 
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) 
                
                
                
                # diff_redmax_visual_pts_with_ori_visual_pts = torch.sum(
                #     (cur_visual_pts[sampled_verts_idxes] - self.timestep_to_active_mesh_opt_ours_sim[cur_ts].detach()) ** 2, dim=-1
                # )
                # diff_redmax_visual_pts_with_ori_visual_pts = diff_redmax_visual_pts_with_ori_visual_pts.mean()
                
                # train the friction net? how to train the friction net? #
                if self.use_mano_hand_for_test:
                    self.timestep_to_active_mesh[cur_ts] = self.rhand_verts[cur_ts] # .detach()
                else:
                    self.timestep_to_active_mesh[cur_ts] = cur_visual_pts
                self.timestep_to_raw_active_meshes[cur_ts] = cur_visual_pts.detach().cpu().numpy()
                
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                
                def evaluate_tracking_loss():
                    self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=self.contact_pairs_set)
                    
                    ## 
                    # init states  # 
                    # cur_ts % mano_nn_substeps == 0:
                    if (cur_ts + 1) % mano_nn_substeps == 0:
                        cur_passive_big_ts = cur_ts // mano_nn_substeps
                        in_func_tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_passive_big_ts + 1)
                        # tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                    else:
                        in_func_tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    return in_func_tracking_loss

                    
                if contact_pairs_set is None:
                    self.contact_pairs_set = None
                else:
                    self.contact_pairs_set = contact_pairs_set.copy()
                
                
                contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)


                if self.train_with_forces_to_active and (not self.use_mano_inputs):
                    # penetration_forces #
                    if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                        net_penetrating_forces = self.other_bending_network.penetrating_forces
                        net_penetrating_points = self.other_bending_network.penetrating_points

                        
                        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                        self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                        self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                        
                        
                        ### transform the visual pts ###
                        # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        # cur_visual_pts = cur_visual_pts * 2. - 1.
                        # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                        
                        # sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[finger_sampled_idxes][self.other_bending_network.penetrating_indicator]
                        
                        # sampled_visual_pts_joint_idxes = visual_pts_joint_idxes[self.other_bending_network.penetrating_indicator]
                        
                        net_penetrating_forces = torch.matmul(
                            cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                        net_penetrating_forces = net_penetrating_forces / 2
                        net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                        
                        net_penetrating_points = torch.matmul(
                            cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                        net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                        net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                        
                        
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        
                    else:
                        # penetration_forces = None
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        
                if contact_pairs_set is not None:
                    self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                
                # contact force d 
                self.ts_to_contact_passive_normals[cur_ts] = self.other_bending_network.tot_contact_passive_normals.detach().cpu().numpy()
                self.ts_to_passive_pts[cur_ts] = self.other_bending_network.cur_passive_obj_verts.detach().cpu().numpy()
                self.ts_to_passive_normals[cur_ts] = self.other_bending_network.cur_passive_obj_ns.detach().cpu().numpy()
                self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                if self.other_bending_network.penalty_based_friction_forces is not None:
                    self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                
                # # get the penetration depth of the bending network #
                tot_penetration_depth.append(self.other_bending_network.penetrating_depth_penalty.detach().item())
                
                ### optimize with intermediates ### # optimize with intermediates # 
                # if self.optimize_with_intermediates:
                #     tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1) # 
                # else:
                #     tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    
                
                # cur_ts % mano_nn_substeps == 0: # 
                if (cur_ts + 1) % mano_nn_substeps == 0:
                    cur_passive_big_ts = cur_ts // mano_nn_substeps
                    tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_passive_big_ts + 1)
                    tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                else:
                    tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()

                # hand_tracking_loss = torch.sum( ## delta states? ##
                #     (self.timestep_to_active_mesh_w_delta_states[cur_ts] - cur_visual_pts) ** 2, dim=-1
                # )
                # hand_tracking_loss = hand_tracking_loss.mean()
                
                
                loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                # diff_redmax_visual_pts_with_ori_visual_pts.backward()
                penetraton_penalty = self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                
                
                # print(f"after calculating losses....")
                
                # # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
                # robot_rotation_diff = torch.sum(
                #     (ori_redmax_glb_rot - self.robot_glb_rotation.weight) ** 2, dim=-1
                # )
                # robot_rotation_diff = robot_rotation_diff.mean()
                
                # robot_trans_diff = torch.sum(
                #     (ori_redmax_glb_trans - self.robot_glb_trans.weight) ** 2, dim=-1
                # )
                # robot_trans_diff = robot_trans_diff.mean()
                
                # robot_delta_states_diff = torch.sum(
                #     (ori_robot_delta_states - self.robot_delta_states.weight) ** 2, dim=-1
                # )
                # robot_delta_states_diff = robot_delta_states_diff.mean()
                
                
                # mano_robot_glb_rotation, mano_robot_glb_trans, mano_robot_delta_states #
                # ori_mano_robot_glb_rot, ori_mano_robot_glb_trans, ori_mano_robot_delta_states #
                
                # ## get rotation and translatiosn diff ## # 
                mano_robot_rotation_diff = torch.sum(
                    (ori_mano_robot_glb_rot - self.mano_robot_glb_rotation.weight) ** 2, dim=-1
                )
                mano_robot_rotation_diff = mano_robot_rotation_diff.mean()
                
                mano_robot_trans_diff = torch.sum(
                    (ori_mano_robot_glb_trans - self.mano_robot_glb_trans.weight) ** 2, dim=-1
                )
                mano_robot_trans_diff = mano_robot_trans_diff.mean()
                
                mano_robot_delta_states_diff = torch.sum(
                    (ori_mano_robot_delta_states - self.mano_robot_delta_states.weight) ** 2, dim=-1
                )
                mano_robot_delta_states_diff = mano_robot_delta_states_diff.mean()
                
                
                kinematics_trans_diff = (mano_robot_rotation_diff + mano_robot_trans_diff + mano_robot_delta_states_diff) * self.robot_actions_diff_coef
                
                # kinematics_trans_diff = (robot_rotation_diff + robot_trans_diff + robot_delta_states_diff) * self.robot_actions_diff_coef
                
                # delta # rhand vertices #
                if cur_ts % mano_nn_substeps == 0:
                    cur_kine_rhand_verts = self.rhand_verts[cur_ts // mano_nn_substeps]
                    cur_dyn_visual_pts_to_mano_verts = cur_visual_pts[self.mano_to_dyn_corr_pts_idxes]
                    diff_hand_tracking = torch.mean(
                        torch.sum((cur_kine_rhand_verts - cur_dyn_visual_pts_to_mano_verts) ** 2, dim=-1)
                    )
                else:
                    diff_hand_tracking = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                ## diff
                # diff_hand_tracking_coef
                # kinematics_proj_loss = kinematics_trans_diff + penetraton_penalty + diff_hand_tracking * self.diff_hand_tracking_coef + tracking_loss
                
                if self.use_mano_hand_for_test:
                    kinematics_proj_loss = tracking_loss
                
                # kinematics_proj_loss = hand_tracking_loss * 1e2
                
                kinematics_proj_loss = diff_hand_tracking * self.diff_hand_tracking_coef + tracking_loss
                
                kinematics_proj_loss = kinematics_proj_loss * self.loss_scale_coef
                
                loss = kinematics_proj_loss
                
                
                mano_tracking_loss.append(diff_hand_tracking.detach().cpu().item())
                
                
                self.kines_optimizer.zero_grad()
                
                kinematics_proj_loss.backward(retain_graph=True)
                
                if self.use_LBFGS:
                    self.kines_optimizer.step(evaluate_tracking_loss) # 
                else:
                    self.kines_optimizer.step()

                # 
                # tracking_loss.backward(retain_graph=True)
                if self.use_LBFGS:
                    self.other_bending_network.reset_timestep_to_quantities(cur_ts)
                
                
                robot_states_actions_diff_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                robo_actions_diff_loss.append(robot_states_actions_diff_loss)

                
                tot_losses.append(loss.detach().item()) # total losses # # total losses # 
                # tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                # tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                # self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)
                # self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)
                # self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)
                # self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)
                # self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step) # 

                if self.iter_step % self.save_freq == 0:
                    self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                self.update_learning_rate() ## update learning rate ##
                
                torch.cuda.empty_cache()
                
            
            ''' Get nn_forward_ts and backward through the actions for updating '''
            
            
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            tot_penetration_depth = sum(tot_penetration_depth) / float(len(tot_penetration_depth))
            robo_actions_diff_loss = sum(robo_actions_diff_loss) / float(len(robo_actions_diff_loss))
            mano_tracking_loss = sum(mano_tracking_loss) / float(len(mano_tracking_loss))
            
            if i_iter % self.report_freq == 0:
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} tracking_loss = {} mano_tracking_loss = {} penetration_depth = {} actions_diff_loss = {} lr={}'.format(self.iter_step, tot_losses, tot_tracking_loss, mano_tracking_loss, tot_penetration_depth, robo_actions_diff_loss, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                ''' Dump to the file '''
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                # spring_ks_values, inertia_div_factor
                print(f"spring_ks_values: {self.other_bending_network.spring_ks_values.weight.data.detach().cpu().numpy()}, inertia_div_factor: {self.other_bending_network.inertia_div_factor.weight.data.detach().cpu().numpy()}")
            
            # self.validate_mesh_robo_a()
            if i_iter % self.val_mesh_freq == 0:
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                
            
            torch.cuda.empty_cache()
    
    
    ''' Optimize diffhand model ''' 
    def train_diffhand_model(self, ):
        
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load #
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path']
        # self.hand_type = "redmax_hand"
        # if model_path.endswith(".xml"):
        #     self.hand_type = "redmax_hand"
        #     robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        # else:
        self.hand_type = "shadow_hand"
        robot_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        
        
        
        
        ## sampled verts idxes ## 
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        ''' Load robot hand in DiffHand simulator '''
        xml_shadow_hand_fn = "rsc/shadow_hand_description/shadowhand_new_scaled_nroot.xml"
        redmax_sim = redmax.Simulation(xml_shadow_hand_fn)
        redmax_sim.reset(backward_flag = True)

        redmax_ndof_u = redmax_sim.ndof_u 
        redmax_ndof_r = redmax_sim.ndof_r
        redmax_ndof_m = redmax_sim.ndof_m
        print(f"[REDMAX] ndof_u: {redmax_ndof_u}, ndof_r: {redmax_ndof_r}, ndof_m: {redmax_ndof_m}")
        
        
        nn_substeps = 10
        
        params_to_train = []
        
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60, 
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        ## robot_delta_states, robot_states # #
        self.robot_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        params_to_train += list(self.robot_states.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        
        
        ### load optimized init transformations for robot actions ###
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_transformations']
            # cur_optimized_init_actions = # optimized init states # ## robot init states ##
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            try:
                self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            except:
                pass
            self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                try:
                    self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
                except:
                    pass
            if 'robot_states' in optimized_init_actions_ckpt:
                self.robot_states.load_state_dict(optimized_init_actions_ckpt['robot_states'])
            # if 'robot_delta_states'  ## robot delta states ##
            if 'robot_actions' in optimized_init_actions_ckpt:
                # self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
                
                if optimized_init_actions_ckpt['robot_actions']['weight'].size(0) > self.robot_actions.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_actions']['weight'].data = optimized_init_actions_ckpt['robot_actions']['weight'].data[:self.robot_actions.weight.data.size(0)]
                if optimized_init_actions_ckpt['robot_actions']['weight'].size(1) < self.robot_actions.weight.data.size(1):
                    optimized_init_actions_ckpt['robot_actions']['weight'].data = torch.cat(
                        [optimized_init_actions_ckpt['robot_actions']['weight'].data, torch.zeros((optimized_init_actions_ckpt['robot_actions']['weight'].size(0), self.robot_actions.weight.data.size(1) - optimized_init_actions_ckpt['robot_actions']['weight'].size(1))).cuda()], dim=1
                    )
                    # self.robot_actions.weight.data[:optimized_init_actions_ckpt['robot_actions']['weight'].size(0)] = optimized_init_actions_ckpt['robot_actions']['weight'].data
                self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            # self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            # self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            
            self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])


        ''' Calculate kinematics delta states from states '''
        tot_robot_delta_states = []
        for i_fr in range(num_steps):
            if i_fr == 0:
                cur_robot_delta_state = self.robot_states.weight.data[0]
            else:
                cur_robot_delta_state = self.robot_states.weight.data[i_fr] - self.robot_states.weight.data[i_fr - 1]
            tot_robot_delta_states.append(cur_robot_delta_state)
        tot_robot_delta_states = torch.stack(tot_robot_delta_states, dim=0)
        self.robot_delta_states.weight.data.copy_(tot_robot_delta_states)
        
        
        
        
        ''' Scaling constants '''
        
        self.nn_ts = self.nn_timesteps - 1
# 
        ''' Set actions for the redmax simulation and add parameters to params-to-train '''


        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)

        ### Constraint set ###
        # self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        
        self.timestep_to_active_mesh = {}
        self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        
        self.timestep_to_active_mesh_w_delta_states = {}
        
        timestep_to_tot_rot = {}
        timestep_to_tot_trans = {}
        
        self.timestep_to_active_mesh_wo_glb = {}
        
        
        with torch.no_grad():
            for cur_ts in range(self.nn_ts):
                ''' Get rotations, translations, and actions of the current robot '''
                cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                
                self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts)
                
                cur_robo_visual_pts = self.robot_agent.get_init_state_visual_pts()
                
                self.timestep_to_active_mesh_wo_glb[cur_ts] = cur_robo_visual_pts.detach().clone()
                
                ### transform the visual pts ###
                # cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                # cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                # cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                
                
                cur_rot = cur_robo_glb_rot
                cur_trans = cur_robo_glb_trans
                
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                
                ### transform by the glboal transformation and the translation ###
                cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0)
                
                
                self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts.clone() 
        
        
        
        nn_iters_opt_redmax_actions = 2000000
        
        nn_substeps = 10
        
        # nn_substeps = 200
        
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps * nn_substeps, embedding_dim=60, #  22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        # params_to_train += list(self.redmax_robot_actions.parameters())
        params_to_train_kines = []
        params_to_train_kines += list(self.redmax_robot_actions.parameters())
        self.actions_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        
        
        
        
        if 'model.load_redmax_robot_actions_fn' in self.conf and len(self.conf['model.load_redmax_robot_actions_fn']) > 0:
            
            load_redmax_robot_actions_fn = self.conf['model.load_redmax_robot_actions_fn'] # 
            
        
            print(f"Loading redmax actions from {load_redmax_robot_actions_fn}")
            
            if load_redmax_robot_actions_fn.endswith(".pth"):
                redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
                self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
            elif load_redmax_robot_actions_fn.endswith(".npy"):
                redmax_robot_actions = np.load(load_redmax_robot_actions_fn, allow_pickle=True)
                redmax_robot_actions = torch.from_numpy(redmax_robot_actions).float().cuda()
                self.redmax_robot_actions.weight.data[: redmax_robot_actions.size(0), : redmax_robot_actions.size(1)] = redmax_robot_actions.clone()
            
        
        ## redmax sim is
        robo_states = self.robot_states.weight.data[:, self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()
        
        for i_iter in tqdm(range(nn_iters_opt_redmax_actions)):
            redmax_sim.reset(backward_flag = True) 
            robo_intermediates_states = []
            tot_redmax_actions = []
            tot_visual_pts = []
            df_dqs = []
            tot_diff_qs = []
            for cur_ts in range(self.nn_ts):
                
                if cur_ts == 0:
                    
                    links_init_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    redmax_q_init = links_init_states[self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()
                    
                    redmax_q_init = redmax_q_init[:redmax_ndof_r]
                    redmax_sim.set_q_init(redmax_q_init)
                    
                    if i_iter % 100 == 0  or (i_iter == 1)  or (i_iter == nn_iters_opt_redmax_actions - 1):
                        
                        redmax_sim_q_th = torch.from_numpy(redmax_q_init).float().cuda()
                        redmax_sim_q_th = torch.cat(
                            [torch.zeros((2,), dtype=torch.float32).float().cuda(), redmax_sim_q_th], dim=0
                        )
                        self.robot_agent.active_robot.set_delta_state_and_update_v2(redmax_sim_q_th, 0, use_real_act_joint=True)
                        cur_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                        
                else:
                    
                    for i_sub_step in range(nn_substeps):
                        cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
                        redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
                        redmax_u = redmax_actions.detach().cpu().numpy()
                        redmax_u = redmax_u[: redmax_ndof_u]
                        
                        
                        redmax_sim.set_u(redmax_u) 
                        tot_redmax_actions.append(redmax_actions)
                        ''' set and set the virtual forces ''' ## redmax_sim
                        virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                        redmax_sim.set_forces(virtual_force) ### virtual forces ###

                        redmax_sim.forward(1, verbose = False)
                        redmax_sim_q = redmax_sim.get_q() # get q # get q ## 
                        if i_sub_step == nn_substeps - 1:
                            robo_intermediates_states.append(redmax_sim_q)
                            df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts]))
                            cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts]) ** 2)
                            tot_diff_qs.append(cur_diff_qs.item())
                        else:
                            df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                    
                    if i_iter % 100 == 0  or (i_iter == 1)  or (i_iter == nn_iters_opt_redmax_actions - 1):
                        
                        
                        redmax_sim_q_th = torch.from_numpy(redmax_sim_q).float().cuda()
                        redmax_sim_q_th = torch.cat(
                            [torch.zeros((2,), dtype=torch.float32).float().cuda(), redmax_sim_q_th], dim=0
                        )
                        self.robot_agent.active_robot.set_delta_state_and_update_v2(redmax_sim_q_th, 0, use_real_act_joint=True) 
                        cur_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                        
                
                        
                        
                        
                if i_iter % 100 == 0   or (i_iter == 1)  or (i_iter == nn_iters_opt_redmax_actions - 1):
                    tot_visual_pts.append(cur_visual_pts.detach().cpu().numpy())
                    

            tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) 
            self.tot_redmax_actions = tot_redmax_actions
            if i_iter > 0:
                df_dq = np.stack(df_dqs, axis=0)
                nn_forward_ts = df_dq.shape[0]
                df_dq = np.reshape(df_dq, (-1,))
                df_du = np.zeros((nn_forward_ts * redmax_ndof_u,))
                
                
                redmax_sim.backward_info.set_flags(False, False, False, True)
                redmax_sim.backward_info.df_dq = df_dq
                redmax_sim.backward_info.df_du = df_du
                redmax_sim.backward()
                # redmax ## redmax ##
                redmax_sim_robot_action_grad = np.copy(redmax_sim.backward_results.df_du)
                redmax_sim_robot_action_grad_th = torch.from_numpy(redmax_sim_robot_action_grad).float().cuda()
                redmax_sim_robot_action_grad_th = redmax_sim_robot_action_grad_th.contiguous().view(nn_forward_ts, -1).contiguous()
                
                
                surrogate_loss = torch.sum(
                    tot_redmax_actions[:, :redmax_ndof_u] * redmax_sim_robot_action_grad_th, dim=-1
                )
                surrogate_loss = surrogate_loss.mean()
                
                self.actions_optimizer.zero_grad()
                surrogate_loss.backward()
                self.actions_optimizer.step()
            
            diff_qs = sum(tot_diff_qs) / float(len(tot_diff_qs))
            cur_log_sv_str = 'iter:{:8>d} diff_qs = {} lr={}'.format(i_iter, diff_qs, self.actions_optimizer.param_groups[0]['lr'])
            print(cur_log_sv_str)
            
            if i_iter % 10 == 0:
                self.save_redmax_actions()
            
            
            self.iter_step = i_iter
            if i_iter % 100 == 0 or (i_iter == 1) or (i_iter == nn_iters_opt_redmax_actions - 1):
                tot_visual_pts = np.stack(tot_visual_pts, axis=0)
                self.tot_visual_pts = tot_visual_pts
                self.validate_mesh_robo_c()
                self.save_checkpoint_redmax_robot_actions()
             
             
    
    
    ''' GRAB clips; Shadow hand '''
    def train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab(self, ):
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load #
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path'] # 
        
        if 'scaled' in model_path:
            self.use_scaled_robot_hand = True
        else:
            self.use_scaled_robot_hand = False
        
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.hand_type = "redmax_hand"
        if model_path.endswith(".xml"):
            self.hand_type = "redmax_hand"
            robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        else:
            self.hand_type = "shadow_hand" ## shadow hand ## robot agent ## robot hand ## two hands? ## three hands ## 
            ## the shadow hand -> from right to left -> how to create the left hand #
            # reflect the hand via hand palm plane -> mirroring # # mirroring the hand using the hand palm plane ##
            robot_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        
        
        ## sampled verts idxes ## 
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        # mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano) # robot #
        mano_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path_mano) ## model path mano ## #  ## modeljpath 
        self.mano_agent = mano_agent
        # ''' Load the mano hand '''
        
        # self.robo_hand_faces = self.mano_agent.robot_faces
        
        # if self.use_mano_hand_for_test:
        #     self.robo_hand_faces = self.hand_faces
        
        
        nn_substeps = 10
        
        mano_nn_substeps = 1
        # mano_nn_substeps = 10 # 
        self.mano_nn_substeps = mano_nn_substeps
        
        
        
        
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        ''' Define MANO robot actions, delta_states, init_states, frictions, and others '''
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        self.mano_robot_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60, # embedding; a realistic thing # # ## so the optimizable modle deisgn --- approxmimate what you see and approximate the target simulator ## # at a distance; the asymmetric contact froces spring ks -- all of them wold affect model's behaviours ##
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_states.weight)
        self.mano_robot_states.weight.data[0, :] = self.mano_robot_init_states.weight.data[0, :].clone()
        
        
        # self.free_deformation_time_latent = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        # )
        # params_to_train += list(self.free_deformation_time_latent.parameters())
        
        
        
        ''' Load optimized MANO hand actions and states '''
        # ### laod optimized init actions #### # 
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            
            if 'mano_robot_states' in optimized_init_actions_ckpt:
                self.mano_robot_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_states'])
            
            if 'mano_robot_init_states' in optimized_init_actions_ckpt:
                self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_init_states'])
                
            if 'mano_robot_glb_rotation' in optimized_init_actions_ckpt:
                self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_rotation'])
            
            if 'mano_robot_glb_trans' in optimized_init_actions_ckpt: # mano_robot_glb_trans
                self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_trans'])
        
        
        # robot actions # # 
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60, #  22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        ## robot_delta_states, robot_states # #
        self.robot_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        params_to_train += list(self.robot_states.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        
        loaded_delta_states = False
        ### load optimized init transformations for robot actions ###
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_transformations']
            # cur_optimized_init_actions = # optimized init states # ## robot init states ##
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            try:
                if optimized_init_actions_ckpt['robot_init_states']['weight'].size(0) > self.robot_init_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_init_states']['weight'].data = optimized_init_actions_ckpt['robot_init_states']['weight'].data[:self.robot_init_states.weight.data.size(0)]
                self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            except:
                pass
            if optimized_init_actions_ckpt['robot_glb_rotation']['weight'].size(0) > self.robot_glb_rotation.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data = optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data[:self.robot_glb_rotation.weight.data.size(0)]
            self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                loaded_delta_states = True
                # try:
                if optimized_init_actions_ckpt['robot_delta_states']['weight'].size(0) > self.robot_delta_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_delta_states']['weight'].data = optimized_init_actions_ckpt['robot_delta_states']['weight'].data[:self.robot_delta_states.weight.data.size(0)]
                self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            if 'robot_states' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_states']['weight'].size(0) > self.robot_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_states']['weight'].data = optimized_init_actions_ckpt['robot_states']['weight'].data[:self.robot_states.weight.data.size(0)]
                self.robot_states.load_state_dict(optimized_init_actions_ckpt['robot_states'])
            # if 'robot_delta_states'  ## robot delta states ##
            if 'robot_actions' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_actions']['weight'].size(0) > self.robot_actions.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_actions']['weight'].data = optimized_init_actions_ckpt['robot_actions']['weight'].data[:self.robot_actions.weight.data.size(0)]
                if optimized_init_actions_ckpt['robot_actions']['weight'].size(1) < self.robot_actions.weight.data.size(1):
                    optimized_init_actions_ckpt['robot_actions']['weight'].data = torch.cat(
                        [optimized_init_actions_ckpt['robot_actions']['weight'].data, torch.zeros((optimized_init_actions_ckpt['robot_actions']['weight'].size(0), self.robot_actions.weight.data.size(1) - optimized_init_actions_ckpt['robot_actions']['weight'].size(1))).cuda()], dim=1
                    )
                    # self.robot_actions.weight.data[:optimized_init_actions_ckpt['robot_actions']['weight'].size(0)] = optimized_init_actions_ckpt['robot_actions']['weight'].data
                self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            # self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            # self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            if optimized_init_actions_ckpt['robot_glb_trans']['weight'].data.size(0) > self.robot_glb_trans.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_trans']['weight'].data = optimized_init_actions_ckpt['robot_glb_trans']['weight'].data[:self.robot_glb_trans.weight.data.size(0)]
            self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])

        
        ''' '''
        print(f"loaded_delta_states: {loaded_delta_states}")
        # if not loaded_delta_states:
        tot_robot_delta_states = []
        for i_fr in range(num_steps):
            if i_fr == 0:
                cur_robot_delta_state = self.robot_states.weight.data[0]
            else:
                cur_robot_delta_state = self.robot_states.weight.data[i_fr] - self.robot_states.weight.data[i_fr - 1]
            tot_robot_delta_states.append(cur_robot_delta_state)
        tot_robot_delta_states = torch.stack(tot_robot_delta_states, dim=0)
        self.robot_delta_states.weight.data.copy_(tot_robot_delta_states) # use delta states ##
            
        
        
        
        ''' Scaling constants '''
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        print(f"mano_to_dyn_corr_pts_idxes: {self.mano_to_dyn_corr_pts_idxes.size()}")
        
        
        # maxx robo pts ##
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        self.mano_mult_const_after_cent = 3. # mult 
        
        if 'model.mano_mult_const_after_cent' in self.conf:
            self.mano_mult_const_after_cent = self.conf['model.mano_mult_const_after_cent']
            
        
        ## redmax hand ##
        if self.hand_type == "redmax_hand":
            self.maxx_robo_pts = 25.
            self.minn_robo_pts = -15.
            self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
            self.mult_const_after_cent = 0.5437551664260203 ## 
        else:
            self.minn_robo_pts = -0.1
            self.maxx_robo_pts = 0.2
            self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
            self.mult_const_after_cent = 0.437551664260203 ## should modify
        ## for grab ##
        self.mult_const_after_cent = self.mult_const_after_cent / 3. * 0.9507
        
        
        
        # robot_states_ori, robot_glb_trans_ori, robot_glb_rotation_ori # 
        # robot_glb_rotation, robot_glb_trans #
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        self.robot_states_ori = self.robot_states.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        
        self.robot_states_sv = self.robot_states.weight.data.clone()
        
        self.nn_ts = self.nn_timesteps - 1

        ''' Set actions for the redmax simulation and add parameters to params-to-train '''
        # # robot actions #
        # self.redmax_robot_actions = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=22,
        # ).cuda()
        # torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        
        # self.redmax_robot_states = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=22,
        # ).cuda()
        
        # params_to_train = []
        # params_to_train += list(self.redmax_robot_actions.parameters())


        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)


        
        ### Constraint set ###
        self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        self.timestep_to_active_mesh = {}
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts #
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        
        self.timestep_to_active_mesh_w_delta_states = {}
        
        timestep_to_tot_rot = {}
        timestep_to_tot_trans = {}
        
        self.timestep_to_active_mesh_wo_glb = {}
        
        # robot_agent #
        
        # states -> get states -> only update the acitons #
        with torch.no_grad():
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                ''' Get rotations, translations, and actions of the current robot '''
                cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                # robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                # 
                # self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts)
                
                robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts) #
                
                # robo_links_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                # self.robot_agent.set_init_states_target_value(robo_links_states)
                # cur_robo_visual_pts = self.robot_agent.get_init_state_visual_pts()
                cur_robo_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                
                
                self.timestep_to_active_mesh_wo_glb[cur_ts] = cur_robo_visual_pts.detach().clone()
                
                
                if not self.use_scaled_robot_hand:
                    ### transform the visual pts ###
                    cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                    cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                    cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                    
                
                cur_rot = cur_robo_glb_rot
                cur_trans = cur_robo_glb_trans
                
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                
                ### transform by the glboal transformation and the translation ###
                # cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0) ## transformed pts ## 
                cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0)
                
                
                self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts.detach().clone() # robo visual pts #
                
                
        self.iter_step = 0
        self.validate_mesh_robo()
        
        ''' Set redmax robot actions '''
        
        # params_to_train_actions = []
        # self.redmax_robot_actions = nn.Embedding(
        #     num_embeddings=num_steps * nn_substeps, embedding_dim=22,
        # ).cuda()
        # torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        # params_to_train_actions += list(self.redmax_robot_actions.parameters())
        
        # self.actions_optimizer = torch.optim.Adam(params_to_train_actions, lr=self.learning_rate)
        
        params_to_train_kines = []


        # opt_robo_states, opt_robo_glb_trans, opt_robo_glb_rot #
        if self.opt_robo_states:
            params_to_train_kines += list(self.robot_delta_states.parameters())
            params_to_train_kines += list(self.robot_actions.parameters())
        if self.opt_robo_glb_trans:
            params_to_train_kines += list(self.robot_glb_trans.parameters())
        if self.opt_robo_glb_rot:
            params_to_train_kines += list(self.robot_glb_rotation.parameters())
        
        
        
        if self.use_LBFGS:
            self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
        else:
            self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        
        if self.optimize_rules:
            params_to_train_kines = []
            print("optimizing rules!")
            params_to_train_kines += list(self.other_bending_network.parameters())
            # params_to_train_kines += list(self.other_bending_network.optimizable_obj_mass.parameters())
            # params_to_train_kines += list(self.other_bending_network.obj_inertia.parameters())
            # params_to_train_kines += list(self.other_bending_network.optimizable_spring_ks.parameters())
            # params_to_train_kines += list(self.other_bending_network.friction_network.parameters())
            # self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            if self.use_LBFGS:
                self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
            else:
                self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            # self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=1e-2)
        

        ''' prepare for keeping the original global rotations, trans, and states '''
        # ori_mano_robot_glb_rot = self.mano_robot_glb_rotation.weight.data.clone()
        # ori_mano_robot_glb_trans = self.mano_robot_glb_trans.weight.data.clone()
        # ori_mano_robot_delta_states = self.mano_robot_delta_states.weight.data.clone()
        
        
        self.iter_step = 0

        self.minn_tracking_loss = 1e27
        

        for i_iter in tqdm(range(100000)):
            tot_losses = []
            tot_tracking_loss = []
            
            # timestep #
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # # # #
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}


            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_passive_normals = {}
            self.ts_to_passive_normals = {}
            self.ts_to_passive_pts = {}
            self.ts_to_contact_force_d = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            self.ts_to_redmax_states = {}
            self.ts_to_mano_rhand_meshes = {}
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16] # pure mano hand # ## pure mano hand ##
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            
            # redmax_sim.reset(backward_flag = True)
            
            # tot_grad_qs = []
            
            # robo_intermediates_states = []
            
            tot_penetration_depth = []
            
            robo_actions_diff_loss = []
            mano_tracking_loss = []
            
            
            penetration_forces = None
            sampled_visual_pts_link_idxes = None
            
            ### 
            # init global transformations ##
            # cur_ts_redmax_delta_rotations = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_delta_rotations = torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda()
            
            # for cur_ts in range(self.nn_ts):
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                # tot_redmax_actions = []
                # actions = {}

                self.free_def_bending_weight = 0.0

                cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                
                # robo_links_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                # self.robot_agent.set_init_states_target_value(robo_links_states)
                
                if self.drive_robot == 'actions':
                    if cur_ts == 0:
                        robo_init_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_init_states, cur_ts) 
                    else:
                        # (robot_actions_dim, )
                        cur_ts_robo_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        self.robot_agent.set_actions_and_update_states_v2(cur_ts_robo_actions, cur_ts, penetration_forces=penetration_forces, sampled_visual_pts_joint_idxes=sampled_visual_pts_link_idxes)
                    # print(f"updating states...")
                    self.robot_states.weight.data[cur_ts, :] = self.robot_agent.get_joint_state(cur_ts, self.robot_states.weight.data[cur_ts, :])
                    self.robot_states_sv[cur_ts, : ] = self.robot_agent.get_joint_state(cur_ts, self.robot_states_sv[cur_ts, :])
                else:
                    robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts) ## delta states ##
                    self.robot_states.weight.data[cur_ts, :] = self.robot_agent.get_joint_state(cur_ts - 1, self.robot_states.weight.data[cur_ts, :])
                    self.robot_states_sv[cur_ts, : ] = self.robot_agent.get_joint_state(cur_ts - 1, self.robot_states_sv[cur_ts, :])
                
                # tate_vals = self.robot_agent.get_joint_state( cur_ts, state_vals, link_name_to_link_struct)
                # self.robot_states.weight.data[cur_ts, :] = self.robot_agent.get_joint_state(cur_ts, self.robot_states.weight.data[cur_ts, :])
                
                # robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                # self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts) ## delta states ##
                #### get the visual pts ####
                # cur_verts, joint_idxes =  get_init_state_visual_pts(expanded_pts=False, ret_joint_idxes=True)
                cur_robo_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                
                # timestep_to_active_mesh_wo_glb # 
                saved_robo_visual_pts = self.timestep_to_active_mesh_wo_glb[cur_ts]
                ### calculate distance ###
                dist_cur_robo_visual_pts_w_saved_robot_pts = torch.sum(
                    (cur_robo_visual_pts - saved_robo_visual_pts) ** 2, dim=-1
                )
                dist_cur_robo_visual_pts_w_saved_robot_pts = dist_cur_robo_visual_pts_w_saved_robot_pts.mean()
                

                if not self.use_scaled_robot_hand:
                    ### transform the visual pts ###
                    cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                    cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                    cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                
                
                cur_rot = cur_robo_glb_rot
                cur_trans = cur_robo_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                ## timestep ## vi
                
                ## 
                ### transform by the glboal transformation and the translation ###
                cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0)
                
                
                # if self.optim_sim_model_params_from_mano:
                #     self.timestep_to_active_mesh[cur_ts] = 
                
                self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts # .clone() # robo visual pts #
                
                self.ts_to_mano_rhand_meshes[cur_ts] = self.rhand_verts[cur_ts]
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                
                # def evaluate_tracking_loss():
                #     self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=self.timestep_to_active_mesh, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=self.contact_pairs_set)
                #     # cur_ts % mano_nn_substeps == 0:
                #     if (cur_ts + 1) % mano_nn_substeps == 0:
                #         cur_passive_big_ts = cur_ts // mano_nn_substeps
                #         in_func_tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_passive_big_ts + 1)
                #         # tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                #     else:
                #         in_func_tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    # return in_func_tracking_loss

                    
                if contact_pairs_set is None:
                    self.contact_pairs_set = None
                else:
                    self.contact_pairs_set = contact_pairs_set.copy()
                
                
                if self.optim_sim_model_params_from_mano:
                    # print(f"Using mano mesh models for optimization")
                    act_mesh_pts_dict_to_model = self.ts_to_mano_rhand_meshes
                else:
                    act_mesh_pts_dict_to_model = self.timestep_to_active_mesh

                
                contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=act_mesh_pts_dict_to_model, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)
                
                

 
                if self.train_with_forces_to_active and (not self.use_mano_inputs): 
                    # penetration_forces #
                    if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                        net_penetrating_forces = self.other_bending_network.penetrating_forces
                        net_penetrating_points = self.other_bending_network.penetrating_points

                        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                        self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                        self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                        
                        
                        ### transform the visual pts ###
                        # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        # cur_visual_pts = cur_visual_pts * 2. - 1.
                        # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                        
                        # sampled_visual_pts_joint_idxes = robo_visual_pts_link_idxes[finger_sampled_idxes][self.other_bending_network.penetrating_indicator]
                        # print(f"robo_visual_pts_link_idxes: {robo_visual_pts_link_idxes.size()}")
                        sampled_visual_pts_link_idxes = robo_visual_pts_link_idxes[self.other_bending_network.penetrating_indicator]
                        
                        net_penetrating_forces = torch.matmul(
                            cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                        net_penetrating_forces = net_penetrating_forces / 2
                        net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                        
                        net_penetrating_points = torch.matmul(
                            cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                        net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                        net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                        
                        penetration_forces = {
                            'penetration_forces': net_penetrating_forces,
                            'penetration_forces_points': net_penetrating_points
                        }
                        
                        # link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        
                    else:
                        penetration_forces = None
                        sampled_visual_pts_link_idxes = None
                        # link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        
                if contact_pairs_set is not None:
                    self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                
                # contact force d 
                self.ts_to_contact_passive_normals[cur_ts] = self.other_bending_network.tot_contact_passive_normals.detach().cpu().numpy()
                self.ts_to_passive_pts[cur_ts] = self.other_bending_network.cur_passive_obj_verts.detach().cpu().numpy()
                self.ts_to_passive_normals[cur_ts] = self.other_bending_network.cur_passive_obj_ns.detach().cpu().numpy()
                self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                if self.other_bending_network.penalty_based_friction_forces is not None:
                    self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                
                # # get the penetration depth of the bending network #
                ## total penetration depth ##
                tot_penetration_depth.append(self.other_bending_network.penetrating_depth_penalty.detach().item())
                
                # tot_penetration_depth.append(self.other_bending_network.penetrating_depth_penalty.detach().item())
                # ## other bending network # #
                
                
                ### optimize with intermediates ### # optimize with intermediates # 
                # if self.optimize_with_intermediates:
                #     tracking_loss = self.compute_loss_optimized_transformations(cur_ts + 1) # 
                # else:
                #     tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                    
                
                # cur_ts % mano_nn_substeps == 0: # 
                # if (cur_ts + 1) % mano_nn_substeps == 0:
                #     cur_passive_big_ts = cur_ts // mano_nn_substeps
                #     tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_passive_big_ts + 1)
                #     tot_tracking_loss.append(tracking_loss.detach().cpu().item())
                # else:
                #     tracking_loss = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                ## loss optimizaed transformations ##
                tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_ts + 1)
                tot_tracking_loss.append(tracking_loss.detach().cpu().item())

                # print(f"tracking_loss: {tracking_loss}")

                # hand_tracking_loss = torch.sum( ## delta states? ##
                #     (self.timestep_to_active_mesh_w_delta_states[cur_ts] - cur_visual_pts) ** 2, dim=-1
                # )
                # hand_tracking_loss = hand_tracking_loss.mean()
                
                
                # loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                # # diff_redmax_visual_pts_with_ori_visual_pts.backward()
                penetraton_penalty = self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                
                
                # # print(f"after calculating losses....") # #
                
                # robot_states_ori, robot_glb_trans_ori, robot_glb_rotation_ori # 
                # # # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
                robot_rotation_diff = torch.sum(
                    (self.robot_glb_rotation_ori - self.robot_glb_rotation.weight) ** 2, dim=-1
                )
                robot_rotation_diff = robot_rotation_diff.mean()
                
                robot_trans_diff = torch.sum(
                    (self.robot_glb_trans_ori - self.robot_glb_trans.weight) ** 2, dim=-1
                )
                robot_trans_diff = robot_trans_diff.mean()
                
                robot_states_diff = torch.sum(
                    (self.robot_states_ori - self.robot_states.weight) ** 2, dim=-1
                )
                robot_states_diff = robot_states_diff.mean()
                
                robot_delta_states_diff = torch.sum(
                    (self.robot_delta_states_ori - self.robot_delta_states.weight) ** 2, dim=-1
                )
                robot_delta_states_diff = robot_delta_states_diff.mean()
                
                transformation_diff_loss = robot_rotation_diff + robot_trans_diff + robot_states_diff + robot_delta_states_diff
                
                
                # kinematics_proj_loss = hand_tracking_loss * 1e2
                
                
                # kinematics_proj_loss = tracking_loss * self.loss_scale_coef  + transformation_diff_loss * 1.0 + penetraton_penalty
                kinematics_proj_loss = tracking_loss * self.loss_scale_coef  + transformation_diff_loss * self.diff_reg_coef + penetraton_penalty
                
                
                
                if self.drive_robot == 'actions':
                    kinematics_proj_loss = kinematics_proj_loss + dist_cur_robo_visual_pts_w_saved_robot_pts
                    # kinematics_proj_loss =  dist_cur_robo_visual_pts_w_saved_robot_pts
     
                
                loss = kinematics_proj_loss
                
                
                diff_hand_tracking = dist_cur_robo_visual_pts_w_saved_robot_pts
                mano_tracking_loss.append(diff_hand_tracking.detach().cpu().item())
                
                
                tot_losses.append(kinematics_proj_loss)
                
                robot_states_actions_diff_loss = transformation_diff_loss.item()
                robo_actions_diff_loss.append(robot_states_actions_diff_loss)

                self.iter_step += 1
                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                
                
                
                # self.kines_optimizer.zero_grad()
                
                # try:
                #     kinematics_proj_loss.backward(retain_graph=True)
                #     # print(f"has grad")
                #     if self.use_LBFGS:
                #         self.kines_optimizer.step(evaluate_tracking_loss) # 
                #     else:
                #         self.kines_optimizer.step()
                # except:
                #     pass

                # ## try to find the bug and try to improve your code here ##
                # # tracking_loss.backward(retain_graph=True) # try to solve problems ##
                # if self.use_LBFGS:
                #     self.other_bending_network.reset_timestep_to_quantities(cur_ts)
                
                
                # robot_states_actions_diff_loss = transformation_diff_loss.item()
                # robo_actions_diff_loss.append(robot_states_actions_diff_loss)

                
                # tot_losses.append(loss.detach().item()) # total losses # # total losses # 
                # # tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                # # tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                # self.iter_step += 1

                # self.writer.add_scalar('Loss/loss', loss, self.iter_step)

                # self.update_learning_rate() ## update learning rate ##
                
                torch.cuda.empty_cache()
                
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            saved_best = False
            if tot_tracking_loss < self.minn_tracking_loss:
                print(f"Saving best checkpoint with tracking_loss: {tot_tracking_loss} at iter_step: {self.iter_step}")
                self.minn_tracking_loss = tot_tracking_loss
                self.save_checkpoint(tag="best")
                
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                saved_best = True
            
            ########## Update v2 ##########
            loss = sum(tot_losses)
            self.kines_optimizer.zero_grad()
            try:
                loss.backward(retain_graph=True)
                # print(f"has grad")
                # if self.use_LBFGS:
                #     self.kines_optimizer.step(evaluate_tracking_loss) # 
                # else:
                self.kines_optimizer.step()
            except:
                pass
            self.update_learning_rate() 
            ########## Update v2 ##########
            
            
            ''' Get nn_forward_ts and backward through the actions for updating '''
            ## save ckpt ##
            if i_iter == 0 or (i_iter % self.ckpt_sv_freq == 0):
                self.save_checkpoint() 
            
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            
            tot_penetration_depth = sum(tot_penetration_depth) / float(len(tot_penetration_depth))
            robo_actions_diff_loss = sum(robo_actions_diff_loss) / float(len(robo_actions_diff_loss))
            mano_tracking_loss = sum(mano_tracking_loss) / float(len(mano_tracking_loss))
            
            
            
            if i_iter % self.report_freq == 0:
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} tracking_loss = {} mano_tracking_loss = {} penetration_depth = {} actions_diff_loss = {} lr={}'.format(self.iter_step, tot_losses, tot_tracking_loss, mano_tracking_loss, tot_penetration_depth, robo_actions_diff_loss, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                ''' Dump to the file '''
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                print(f"spring_ks_values: { self.other_bending_network.optimizable_spring_ks.weight.data.detach().cpu().numpy() }")
            
            # self.validate_mesh_robo_a() ## not saved best and not 
            if (i_iter % self.val_mesh_freq == 0) and (not saved_best):
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                
            
            torch.cuda.empty_cache()
    
    
    
    
    ''' GRAB clips; Shadow hand '''
    def train_manip_acts_params(self, ):
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load #
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path'] # 
        
        if 'scaled' in model_path:
            self.use_scaled_robot_hand = True
        else:
            self.use_scaled_robot_hand = False
        
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        # self.hand_type = "redmax_hand"
        # if model_path.endswith(".xml"):
        #     self.hand_type = "redmax_hand"
        #     robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        # else:
        self.hand_type = "shadow_hand" ## shadow hand ## robot agent ## robot hand ## two hands? ## three hands ## 
        ## the shadow hand -> from right to left -> how to create the left hand #
        # reflect the hand via hand palm plane -> mirroring # # mirroring the hand using the hand palm plane ##
        robot_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        
        
        ## sampled verts idxes ## 
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        mano_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path_mano) ## model path mano ## #  ## modeljpath 
        self.mano_agent = mano_agent
        # ''' Load the mano hand '''
        
        
        ''' Load robot hand in DiffHand simulator '''
        xml_shadow_hand_fn = "rsc/shadow_hand_description/shadowhand_new_scaled_nroot.xml"
        redmax_sim = redmax.Simulation(xml_shadow_hand_fn)
        redmax_sim.reset(backward_flag = True)
        
        redmax_ndof_u = redmax_sim.ndof_u
        redmax_ndof_r = redmax_sim.ndof_r
        redmax_ndof_m = redmax_sim.ndof_m
        print(f"[REDMAX] ndof_u: {redmax_ndof_u}, ndof_r: {redmax_ndof_r}, ndof_m: {redmax_ndof_m}")
        ''' Load robot hand in DiffHand simulator '''
        
        
        nn_substeps = 10
        
        mano_nn_substeps = 1
        self.mano_nn_substeps = mano_nn_substeps
        
        
        ''' Expnad the current visual points ''' 
        params_to_train = []
        ''' Define MANO robot actions, delta_states, init_states, frictions, and others '''
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        self.mano_robot_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_states.weight)
        self.mano_robot_states.weight.data[0, :] = self.mano_robot_init_states.weight.data[0, :].clone()
        
        
        ''' Load optimized MANO hand actions and states '''
        # ### laod optimized init actions #### # 
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            
            if 'mano_robot_states' in optimized_init_actions_ckpt:
                self.mano_robot_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_states'])
            
            if 'mano_robot_init_states' in optimized_init_actions_ckpt:
                self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_init_states'])
                
            if 'mano_robot_glb_rotation' in optimized_init_actions_ckpt:
                self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_rotation'])
            
            if 'mano_robot_glb_trans' in optimized_init_actions_ckpt: # mano_robot_glb_trans
                self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_trans'])
        
        
        # robot actions # # 
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60, #  22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        ## robot_delta_states, robot_states # #
        self.robot_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        params_to_train += list(self.robot_states.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        
        loaded_delta_states = False
        ### load optimized init transformations for robot actions ###
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_transformations']
            # cur_optimized_init_actions = # optimized init states # ## robot init states ##
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            try:
                if optimized_init_actions_ckpt['robot_init_states']['weight'].size(0) > self.robot_init_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_init_states']['weight'].data = optimized_init_actions_ckpt['robot_init_states']['weight'].data[:self.robot_init_states.weight.data.size(0)]
                self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            except:
                pass
            if optimized_init_actions_ckpt['robot_glb_rotation']['weight'].size(0) > self.robot_glb_rotation.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data = optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data[:self.robot_glb_rotation.weight.data.size(0)]
            self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                loaded_delta_states = True
                # try:
                if optimized_init_actions_ckpt['robot_delta_states']['weight'].size(0) > self.robot_delta_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_delta_states']['weight'].data = optimized_init_actions_ckpt['robot_delta_states']['weight'].data[:self.robot_delta_states.weight.data.size(0)]
                self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            if 'robot_states' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_states']['weight'].size(0) > self.robot_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_states']['weight'].data = optimized_init_actions_ckpt['robot_states']['weight'].data[:self.robot_states.weight.data.size(0)]
                self.robot_states.load_state_dict(optimized_init_actions_ckpt['robot_states'])
            # if 'robot_delta_states'  ## robot delta states ##
            if 'robot_actions' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_actions']['weight'].size(0) > self.robot_actions.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_actions']['weight'].data = optimized_init_actions_ckpt['robot_actions']['weight'].data[:self.robot_actions.weight.data.size(0)]
                if optimized_init_actions_ckpt['robot_actions']['weight'].size(1) < self.robot_actions.weight.data.size(1):
                    optimized_init_actions_ckpt['robot_actions']['weight'].data = torch.cat(
                        [optimized_init_actions_ckpt['robot_actions']['weight'].data, torch.zeros((optimized_init_actions_ckpt['robot_actions']['weight'].size(0), self.robot_actions.weight.data.size(1) - optimized_init_actions_ckpt['robot_actions']['weight'].size(1))).cuda()], dim=1
                    )
                    # self.robot_actions.weight.data[:optimized_init_actions_ckpt['robot_actions']['weight'].size(0)] = optimized_init_actions_ckpt['robot_actions']['weight'].data
                self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            # self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            # self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            if optimized_init_actions_ckpt['robot_glb_trans']['weight'].data.size(0) > self.robot_glb_trans.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_trans']['weight'].data = optimized_init_actions_ckpt['robot_glb_trans']['weight'].data[:self.robot_glb_trans.weight.data.size(0)]
            self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])

        
        
        print(f"loaded_delta_states: {loaded_delta_states}")
        # if not loaded_delta_states:
        tot_robot_delta_states = []
        for i_fr in range(num_steps):
            if i_fr == 0:
                cur_robot_delta_state = self.robot_states.weight.data[0]
            else:
                cur_robot_delta_state = self.robot_states.weight.data[i_fr] - self.robot_states.weight.data[i_fr - 1]
            tot_robot_delta_states.append(cur_robot_delta_state)
        tot_robot_delta_states = torch.stack(tot_robot_delta_states, dim=0)
        self.robot_delta_states.weight.data.copy_(tot_robot_delta_states) # use delta states ##
        
        
        
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps * nn_substeps, embedding_dim=60, #  22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        # params_to_train += list(self.redmax_robot_actions.parameters())
        params_to_train_kines = []
        params_to_train_kines += list(self.redmax_robot_actions.parameters())
        self.actions_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate_actions)
        
        
        
        if 'model.load_redmax_robot_actions_fn' in self.conf and len(self.conf['model.load_redmax_robot_actions_fn']) > 0:
            
            load_redmax_robot_actions_fn = self.conf['model.load_redmax_robot_actions_fn'] # 
            
        
            print(f"Loading redmax actions from {load_redmax_robot_actions_fn}")
            
            if load_redmax_robot_actions_fn.endswith(".pth"):
                redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
                self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
            elif load_redmax_robot_actions_fn.endswith(".npy"):
                redmax_robot_actions = np.load(load_redmax_robot_actions_fn, allow_pickle=True)
                redmax_robot_actions = torch.from_numpy(redmax_robot_actions).float().cuda()
                self.redmax_robot_actions.weight.data[: redmax_robot_actions.size(0), : redmax_robot_actions.size(1)] = redmax_robot_actions.clone()
        ##### redmax robot actions #####
            
        
        
        
        ''' Scaling constants '''
        # self.mano_mult_const_after_cent = 3.
        # mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        # if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
        #     mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        # self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        # self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        print(f"mano_to_dyn_corr_pts_idxes: {self.mano_to_dyn_corr_pts_idxes.size()}")
        
        
        # maxx robo pts ##
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        self.mano_mult_const_after_cent = 3. # mult 
        
        if 'model.mano_mult_const_after_cent' in self.conf:
            self.mano_mult_const_after_cent = self.conf['model.mano_mult_const_after_cent']
            
        
        ## redmax hand ##
        if self.hand_type == "redmax_hand":
            self.maxx_robo_pts = 25.
            self.minn_robo_pts = -15.
            self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
            self.mult_const_after_cent = 0.5437551664260203 ## 
        else:
            self.minn_robo_pts = -0.1
            self.maxx_robo_pts = 0.2
            self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
            self.mult_const_after_cent = 0.437551664260203 ## should modify
        ## for grab ##
        self.mult_const_after_cent = self.mult_const_after_cent / 3. * 0.9507
        
        
        
        # robot_states_ori, robot_glb_trans_ori, robot_glb_rotation_ori # 
        # robot_glb_rotation, robot_glb_trans #
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        self.robot_states_ori = self.robot_states.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        
        self.robot_states_sv = self.robot_states.weight.data.clone()
        
        self.nn_ts = self.nn_timesteps - 1

        ''' Set actions for the redmax simulation and add parameters to params-to-train '''

        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)


        
        ### Constraint set ###
        self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        self.timestep_to_active_mesh = {}
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts #
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        
        self.timestep_to_active_mesh_w_delta_states = {}
        
        timestep_to_tot_rot = {}
        timestep_to_tot_trans = {}
        
        self.timestep_to_active_mesh_wo_glb = {}
        
        # robot_agent #
        
        # states -> get states -> only update the acitons #
        with torch.no_grad():
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                ''' Get rotations, translations, and actions of the current robot '''
                cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                # robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                # 
                # self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts)
                
                robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts) #
                
                # robo_links_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                # self.robot_agent.set_init_states_target_value(robo_links_states)
                # cur_robo_visual_pts = self.robot_agent.get_init_state_visual_pts()
                cur_robo_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                
                
                self.timestep_to_active_mesh_wo_glb[cur_ts] = cur_robo_visual_pts.detach().clone()
                
                
                if not self.use_scaled_robot_hand:
                    ### transform the visual pts ###
                    cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                    cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                    cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                    
                
                cur_rot = cur_robo_glb_rot
                cur_trans = cur_robo_glb_trans
                
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                
                ### transform by the glboal transformation and the translation ###
                # cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0) ## transformed pts ## 
                cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0)
                
                
                self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts.detach().clone() # robo visual pt
        
        self.iter_step = 0
        self.validate_mesh_robo() # 
        
        ''' Set redmax robot actions '''
        
        
        
        ## redmax sim is
        robo_states = self.robot_states.weight.data[:, self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()
        
        ''' Initialize '''
        with torch.no_grad():
            nn_iters_opt_redmax_actions = 2
            for i_iter in tqdm(range(nn_iters_opt_redmax_actions)):
                redmax_sim.reset(backward_flag = True) 
                robo_intermediates_states = []
                tot_redmax_actions = []
                tot_visual_pts = []
                df_dqs = []
                tot_diff_qs = []
                for cur_ts in range(self.nn_ts):
                    
                    if cur_ts == 0:
                        
                        # robot_init_state = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                        # redmax_q_init = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
                        
                        links_init_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                        # self.robot_agent.set_init_states_target_value(links_init_states)
                        # cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                        # cur_visual_pts = self.timestep_to_active_mesh_wo_glb[cur_ts]
                        ## redmax q init ## link initstates ## ## robot agent ## 
                        redmax_q_init = links_init_states[self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()
                        
                        # redmax_q_init = np.zeros((redmax_ndof_r,), dtype=np.float32)
                        # redmax_q_init = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
                        redmax_q_init = redmax_q_init[:redmax_ndof_r]
                        redmax_sim.set_q_init(redmax_q_init)
                        
                        if i_iter % 100 == 0  or (i_iter == 1)  or (i_iter == nn_iters_opt_redmax_actions - 1):
               
                            
                            ### redmax_sim_q ## ---> use that for the real act joint idx setting ##? 
                            redmax_sim_q_th = torch.from_numpy(redmax_q_init).float().cuda()
                            redmax_sim_q_th = torch.cat(
                                [torch.zeros((2,), dtype=torch.float32).float().cuda(), redmax_sim_q_th], dim=0 ### use the redmax sim q th
                            )
                            self.robot_agent.active_robot.set_delta_state_and_update_v2(redmax_sim_q_th, 0, use_real_act_joint=True) ## delta states ##
                            cur_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                            
                            
                    
                    else:
                        
                        for i_sub_step in range(nn_substeps): ## # ## #
                            cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
                            redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
                            redmax_u = redmax_actions.detach().cpu().numpy()
                            redmax_u = redmax_u[: redmax_ndof_u]
                            
                            ## redmax_u ##
                            # redmax_u = redmax_robot_actions[cur_tot_substeps].detach().cpu().numpy()[ : redmax_ndof_u]
                            
                            redmax_sim.set_u(redmax_u) # sim
                            tot_redmax_actions.append(redmax_actions)
                            ''' set and set the virtual forces ''' ## redmax_sim
                            virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                            redmax_sim.set_forces(virtual_force) ### virtual forces ###

                            redmax_sim.forward(1, verbose = False)
                            redmax_sim_q = redmax_sim.get_q() # get q # get q ## 
                            if i_sub_step == nn_substeps - 1:
                                robo_intermediates_states.append(redmax_sim_q)
                                df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts]))
                                cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts]) ** 2)
                                tot_diff_qs.append(cur_diff_qs.item())
                            else:
                                df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                        
                        if i_iter % 100 == 0  or (i_iter == 1)  or (i_iter == nn_iters_opt_redmax_actions - 1):
                            
                            ########## robot_agent and setting active joint states #############
                            redmax_sim_q_th = torch.from_numpy(redmax_sim_q).float().cuda()
                            redmax_sim_q_th = torch.cat(
                                [torch.zeros((2,), dtype=torch.float32).float().cuda(), redmax_sim_q_th], dim=0 ### use the redmax sim q th
                            )
                            self.robot_agent.active_robot.set_delta_state_and_update_v2(redmax_sim_q_th, 0, use_real_act_joint=True) ## delta states ##
                            cur_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                            
                    
                    if i_iter % 100 == 0   or (i_iter == 1)  or (i_iter == nn_iters_opt_redmax_actions - 1):
                        tot_visual_pts.append(cur_visual_pts.detach().cpu().numpy())
                
                
                tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) 
                self.tot_redmax_actions = tot_redmax_actions
                
                
                diff_qs = sum(tot_diff_qs) / float(len(tot_diff_qs)) ## tot diff qs ##
                cur_log_sv_str = 'iter:{:8>d} diff_qs = {} lr={}'.format(i_iter, diff_qs, self.actions_optimizer.param_groups[0]['lr'])
                # get states and then use the obtianed states for actions optim?
                print(cur_log_sv_str)
                
                
        
        redmax_sim.reset(backward_flag = True) 
        robo_intermediates_states = []
        tot_redmax_actions = []
        tot_visual_pts = []
        df_dqs = []
        tot_diff_qs = []
        
        
        params_to_train_kines = []


        # opt_robo_states, opt_robo_glb_trans, opt_robo_glb_rot #
        if self.opt_robo_states:
            params_to_train_kines += list(self.robot_delta_states.parameters())
            params_to_train_kines += list(self.robot_actions.parameters())
        if self.opt_robo_glb_trans:
            params_to_train_kines += list(self.robot_glb_trans.parameters())
        if self.opt_robo_glb_rot:
            params_to_train_kines += list(self.robot_glb_rotation.parameters())
        
        
        if self.use_LBFGS:
            self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
        else:
            self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        
        if self.optimize_rules:
            params_to_train_kines = []
            print("optimizing rules!")
            params_to_train_kines += list(self.other_bending_network.parameters())
            # if self.use_LBFGS:
            #     self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
            # else:
            self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        # self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=1e-2)
        

        ''' prepare for keeping the original global rotations, trans, and states '''
        # ori_mano_robot_glb_rot = self.mano_robot_glb_rotation.weight.data.clone()
        # ori_mano_robot_glb_trans = self.mano_robot_glb_trans.weight.data.clone()
        # ori_mano_robot_delta_states = self.mano_robot_delta_states.weight.data.clone()
        
        
        self.iter_step = 0

        self.minn_tracking_loss = 1e27
        

        for i_iter in tqdm(range(100000)):
            tot_losses = []
            tot_tracking_loss = []
            
            
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}


            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_passive_normals = {}
            self.ts_to_passive_normals = {}
            self.ts_to_passive_pts = {}
            self.ts_to_contact_force_d = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            self.ts_to_redmax_states = {}
            self.ts_to_mano_rhand_meshes = {}
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16] # pure mano hand # ## pure mano hand ##
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            
            # redmax_sim.reset(backward_flag = True)
            
            # tot_grad_qs = []
            
            # robo_intermediates_states = []
            
            tot_penetration_depth = []
            
            robo_actions_diff_loss = []
            mano_tracking_loss = []
            
            ########## Reset redmax_sim and related quantities ##########
            redmax_sim.reset(backward_flag = True) 
            robo_intermediates_states = []
            tot_redmax_actions = []
            tot_visual_pts = []
            df_dqs = []
            tot_diff_qs = []
            tot_redmax_qs_th = []    
            ########## Reset redmax_sim and related quantities ##########
            
            # tot_diff_qs = []
            
            
            # tot_redmax_qs_th = []    
        
            ## gradients towards the robot states -> gradients to the actions ##3
            ## get df_dq from the task related objective ##
            ## get df_du via back propagating trhough the redmax_sim ##
            
            
            penetration_forces = None
            sampled_visual_pts_link_idxes = None
            link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                
            
            
            # # cur_ts_redmax_delta_rotations = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda()
            # cur_ts_redmax_delta_rotations = torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda()
            # cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda()
            
            # for cur_ts in range(self.nn_ts):
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                # tot_redmax_actions = []
                # actions = {}

                self.free_def_bending_weight = 0.0

                cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                
                
                
                ### use the redmax robot ###
                if cur_ts == 0:
                    links_init_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                    redmax_sim_q = links_init_states[self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()
                    redmax_sim_q = redmax_sim_q[:redmax_ndof_r]
                    redmax_sim.set_q_init(redmax_sim_q)
                else:
                    for i_sub_step in range(nn_substeps):
                        cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
                        redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
                        redmax_u = redmax_actions.detach().cpu().numpy()
                        redmax_u = redmax_u[: redmax_ndof_u]
                        
                        redmax_sim.set_u(redmax_u) # set joint actions ###
                        tot_redmax_actions.append(redmax_actions)
                        ''' set and set the virtual forces ''' ## redmax_sim
                        ## virtual forces --- set the virtue forces ##
                        # virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32)
                        
                        virtual_force = link_maximal_contact_forces.detach().cpu().numpy()
                        virtual_force = np.reshape(virtual_force, (virtual_force.shape[0] * virtual_force.shape[1]))
                        
                        redmax_sim.set_forces(virtual_force) ### virtual forces ###

                        redmax_sim.forward(1, verbose = False)
                        redmax_sim_q = redmax_sim.get_q() # get q # get q ## 
                        # if i_sub_step == nn_substeps - 1:
                        #     robo_intermediates_states.append(redmax_sim_q)
                        #     df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts]))
                        #     cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts]) ** 2)
                        #     tot_diff_qs.append(cur_diff_qs.item())
                        # else:
                        #     df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                        
                
                cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts]) ** 2)
                
                tot_diff_qs.append(cur_diff_qs.item())
                
                ## from  ## can st the virutal forces to the redmax robot ##
                redmax_sim_q_th = torch.from_numpy(redmax_sim_q).float().cuda() ### get the redmax_sim_q ## 
                redmax_sim_q_th.requires_grad = True
                redmax_sim_q_th.requires_grad_()
                redmax_sim_q_th.requires_grad_ = True
                
                tot_redmax_qs_th.append(redmax_sim_q_th.detach().clone()) ## redmax_sim_q_th ## 
                tot_redmax_qs_th[cur_ts].requires_grad = True
                tot_redmax_qs_th[cur_ts].requires_grad_()
                tot_redmax_qs_th[cur_ts].requires_grad_ = True
                ## redmax_sim_q_th ## 
                
                # redmax_sim_q_th = torch.cat(
                #     [torch.zeros((2,), dtype=torch.float32).float().cuda(), redmax_sim_q_th], dim=0
                # )
                
                
                self.robot_agent.active_robot.set_delta_state_and_update_v2(
                    # redmax_sim_q_th, 
                    torch.cat( ## get the redmax qs th ### 
                        [torch.zeros((2,), dtype=torch.float32).float().cuda(), tot_redmax_qs_th[cur_ts]], dim=0
                    ),
                0, use_real_act_joint=True) ## delta states ##
                
                # self.robot_agent.active_robot.set_delta_state_and_update_v2(redmax_sim_q_th, 0, use_real_act_joint=True) ## delta states ##
                cur_robo_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                
                
                
                # timestep_to_active_mesh_wo_glb # 
                saved_robo_visual_pts = self.timestep_to_active_mesh_wo_glb[cur_ts]
                ### calculate distance ###
                dist_cur_robo_visual_pts_w_saved_robot_pts = torch.sum(
                    (cur_robo_visual_pts - saved_robo_visual_pts) ** 2, dim=-1
                )
                dist_cur_robo_visual_pts_w_saved_robot_pts = dist_cur_robo_visual_pts_w_saved_robot_pts.mean()
                

                if not self.use_scaled_robot_hand:
                    ### transform the visual pts ###
                    cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                    cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                    cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                
                
                cur_rot = cur_robo_glb_rot
                cur_trans = cur_robo_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                
                ### transform by the glboal transformation and the translation ###
                cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0)
                
                
                # if self.optim_sim_model_params_from_mano:
                #     self.timestep_to_active_mesh[cur_ts] = 
                
                self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts
                
                self.ts_to_mano_rhand_meshes[cur_ts] = self.rhand_verts[cur_ts]
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                    
                if contact_pairs_set is None:
                    self.contact_pairs_set = None
                else:
                    self.contact_pairs_set = contact_pairs_set.copy()
                
                
                # if self.optim_sim_model_params_from_mano:
                #     # print(f"Using mano mesh models for optimization")
                #     act_mesh_pts_dict_to_model = self.ts_to_mano_rhand_meshes
                # else:
                act_mesh_pts_dict_to_model = self.timestep_to_active_mesh

                
                contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=act_mesh_pts_dict_to_model, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)
                
                

 
                if self.train_with_forces_to_active and (not self.use_mano_inputs): 
                    # penetration_forces #
                    if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                        net_penetrating_forces = self.other_bending_network.penetrating_forces
                        net_penetrating_points = self.other_bending_network.penetrating_points

                        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                        self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                        self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                        
                        
                        ### transform the visual pts ###
                        # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        # cur_visual_pts = cur_visual_pts * 2. - 1.
                        # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                        
                        # sampled_visual_pts_joint_idxes = robo_visual_pts_link_idxes[finger_sampled_idxes][self.other_bending_network.penetrating_indicator]
                        # print(f"robo_visual_pts_link_idxes: {robo_visual_pts_link_idxes.size()}")
                        sampled_visual_pts_link_idxes = robo_visual_pts_link_idxes[self.other_bending_network.penetrating_indicator]
                        
                        net_penetrating_forces = torch.matmul(
                            cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                        net_penetrating_forces = net_penetrating_forces / 2
                        net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                        
                        net_penetrating_points = torch.matmul(
                            cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                        net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                        net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                        
                        penetration_forces = {
                            'penetration_forces': net_penetrating_forces,
                            'penetration_forces_points': net_penetrating_points
                        }
                        
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        # self.robot_agent.set_maximal_contact_forces(link_maximal_contact_forces)
                        self.robot_agent.active_robot.set_penetration_forces(penetration_forces, sampled_visual_pts_link_idxes, link_maximal_contact_forces)
                        
                    else:
                        penetration_forces = None
                        sampled_visual_pts_link_idxes = None
                        # link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        
                if contact_pairs_set is not None:
                    self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                
                # contact force d 
                self.ts_to_contact_passive_normals[cur_ts] = self.other_bending_network.tot_contact_passive_normals.detach().cpu().numpy()
                self.ts_to_passive_pts[cur_ts] = self.other_bending_network.cur_passive_obj_verts.detach().cpu().numpy()
                self.ts_to_passive_normals[cur_ts] = self.other_bending_network.cur_passive_obj_ns.detach().cpu().numpy()
                self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                if self.other_bending_network.penalty_based_friction_forces is not None:
                    self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                
                
                tot_penetration_depth.append(self.other_bending_network.penetrating_depth_penalty.detach().item())
                
                ## loss optimizaed transformations ##
                tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_ts + 1)
                tot_tracking_loss.append(tracking_loss.detach().cpu().item())

                penetraton_penalty = self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                
                
                robot_rotation_diff = torch.sum(
                    (self.robot_glb_rotation_ori - self.robot_glb_rotation.weight) ** 2, dim=-1
                )
                robot_rotation_diff = robot_rotation_diff.mean()
                
                robot_trans_diff = torch.sum(
                    (self.robot_glb_trans_ori - self.robot_glb_trans.weight) ** 2, dim=-1
                )
                robot_trans_diff = robot_trans_diff.mean()
                
                robot_states_diff = torch.sum(
                    (self.robot_states_ori - self.robot_states.weight) ** 2, dim=-1
                )
                robot_states_diff = robot_states_diff.mean()
                
                robot_delta_states_diff = torch.sum(
                    (self.robot_delta_states_ori - self.robot_delta_states.weight) ** 2, dim=-1
                )
                robot_delta_states_diff = robot_delta_states_diff.mean()
                
                transformation_diff_loss = robot_rotation_diff + robot_trans_diff + robot_states_diff + robot_delta_states_diff
                
                
                # kinematics_proj_loss = hand_tracking_loss * 1e2
                
                
                # kinematics_proj_loss = tracking_loss * self.loss_scale_coef  + transformation_diff_loss * 1.0 + penetraton_penalty
                kinematics_proj_loss = tracking_loss * self.loss_scale_coef  + transformation_diff_loss * self.diff_reg_coef + penetraton_penalty
                
                
                
                if self.drive_robot == 'actions':
                    kinematics_proj_loss = kinematics_proj_loss + dist_cur_robo_visual_pts_w_saved_robot_pts
                    # kinematics_proj_loss =  dist_cur_robo_visual_pts_w_saved_robot_pts
     
                
                loss = kinematics_proj_loss
                
                
                diff_hand_tracking = dist_cur_robo_visual_pts_w_saved_robot_pts
                mano_tracking_loss.append(diff_hand_tracking.detach().cpu().item())
                
                
                tot_losses.append(kinematics_proj_loss)
                
                robot_states_actions_diff_loss = transformation_diff_loss.item()
                robo_actions_diff_loss.append(robot_states_actions_diff_loss)

                self.iter_step += 1
                self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                
                
                torch.cuda.empty_cache()
                
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            saved_best = False
            if tot_tracking_loss < self.minn_tracking_loss:
                print(f"Saving best checkpoint with tracking_loss: {tot_tracking_loss} at iter_step: {self.iter_step}")
                self.minn_tracking_loss = tot_tracking_loss
                self.save_checkpoint(tag="best")
                
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                saved_best = True
            
            ########## Update v2 ##########
            loss = sum(tot_losses) ### tot_losses ###
            self.kines_optimizer.zero_grad()
            try:
                loss.backward(retain_graph=True)
                # print(f"has grad")
                # if self.use_LBFGS:
                #     self.kines_optimizer.step(evaluate_tracking_loss) # 
                # else:
                if self.optimize_rules:
                    self.kines_optimizer.step()
            except:
                pass
            self.update_learning_rate() 
            ########## Update v2 ##########
            
            ## 
            ## df dq ### 
            tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) 
            self.tot_redmax_actions = tot_redmax_actions
            
            tot_df_dqs = []
            for cur_ts in range(1, self.nn_ts):
                for i_sub_step in range(nn_substeps):
                    if i_sub_step < nn_substeps - 1:
                        tot_df_dqs.append(np.zeros((tot_redmax_qs_th[0].size(0),), dtype=np.float32)) ## get df_dqs ## 
                    else:
                        cur_df_dq = tot_redmax_qs_th[cur_ts].grad.data.detach().cpu().numpy()
                        tot_df_dqs.append(cur_df_dq) ## get the current df dqs ## 
            df_dq = np.stack(tot_df_dqs, axis=0)
            
            # df_dq_summ = 
            
            nn_forward_ts = df_dq.shape[0]
            df_dq = np.reshape(df_dq, (-1,))
            df_du = np.zeros((nn_forward_ts * redmax_ndof_u,))
            
            redmax_sim.backward_info.set_flags(False, False, False, True) 
            # nn_forward_ts = len(tot_grad_qs)
            # tot_grad_qs = np.concatenate(tot_grad_qs, axis=0)
            redmax_sim.backward_info.df_dq = df_dq
            redmax_sim.backward_info.df_du = df_du
            # print(f"df_du: {df_du.shape}, df_dq: {df_dq.shape}")
            
            redmax_sim.backward()
            
            
            
            redmax_sim_robot_action_grad = np.copy(redmax_sim.backward_results.df_du)
            redmax_sim_robot_action_grad_th = torch.from_numpy(redmax_sim_robot_action_grad).float().cuda()
            redmax_sim_robot_action_grad_th = redmax_sim_robot_action_grad_th.contiguous().view(nn_forward_ts, -1).contiguous()
            
            ## nn_forward_steps x nn_us_dim # 
            
            ## redmax_sim as the redmax_sim ## ## surrogate loss #### surrogte lsoss ## 
            surrogate_loss = torch.sum(
                tot_redmax_actions[:, :redmax_ndof_u] * redmax_sim_robot_action_grad_th, dim=-1
            )
            surrogate_loss = surrogate_loss.mean()
            
            self.actions_optimizer.zero_grad()
            surrogate_loss.backward()
            
            
            avg_tot_diff_qs = sum(tot_diff_qs) / float(len(tot_diff_qs))
            grad_redmax_robot_actions = self.redmax_robot_actions.weight.grad.data
            summ_grad_redmax_robot_actions = torch.sum(grad_redmax_robot_actions).item()
            print('iter:{:8>d} redmax_robot_actions_grad_sum = {} avg_tot_diff_qs = {}'.format(self.iter_step, summ_grad_redmax_robot_actions, avg_tot_diff_qs))
            
            if not self.optimize_rules:
                self.actions_optimizer.step()
                
                
            
            ''' Get nn_forward_ts and backward through the actions for updating '''
            ## save ckpt ##
            if i_iter == 0 or (i_iter % self.ckpt_sv_freq == 0): ## ckpt sv freq ##
                self.save_checkpoint() 
            
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            
            tot_penetration_depth = sum(tot_penetration_depth) / float(len(tot_penetration_depth))
            robo_actions_diff_loss = sum(robo_actions_diff_loss) / float(len(robo_actions_diff_loss))
            mano_tracking_loss = sum(mano_tracking_loss) / float(len(mano_tracking_loss))
            
            
            
            if i_iter % self.report_freq == 0:
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} tracking_loss = {} mano_tracking_loss = {} penetration_depth = {} actions_diff_loss = {} lr={}'.format(self.iter_step, tot_losses, tot_tracking_loss, mano_tracking_loss, tot_penetration_depth, robo_actions_diff_loss, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                ''' Dump to the file '''
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                print(f"spring_ks_values: { self.other_bending_network.optimizable_spring_ks.weight.data.detach().cpu().numpy() }")
            
            
            if (i_iter % self.val_mesh_freq == 0) and (not saved_best):
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                
            torch.cuda.empty_cache()
    
    
    
    
    ''' GRAB clips; Shadow hand '''
    def train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab_multi_stages(self, ):
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load #
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path'] # 
        
        if 'scaled' in model_path:
            self.use_scaled_robot_hand = True
        else:
            self.use_scaled_robot_hand = False
        
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        self.hand_type = "redmax_hand"
        if model_path.endswith(".xml"):
            self.hand_type = "redmax_hand"
            robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        else:
            self.hand_type = "shadow_hand" ## shadow hand ## robot agent ## robot hand ## two hands? ## three hands ## 
            ## the shadow hand -> from right to left -> how to create the left hand #
            # reflect the hand via hand palm plane -> mirroring # # mirroring the hand using the hand palm plane ##
            robot_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        
        
        ## sampled verts idxes ## 
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        # mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano) # robot #
        mano_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path_mano) ## model path mano ## #  ## modeljpath 
        self.mano_agent = mano_agent
        # ''' Load the mano hand '''
        
        # self.robo_hand_faces = self.mano_agent.robot_faces
        
        # if self.use_mano_hand_for_test:
        #     self.robo_hand_faces = self.hand_faces
        
        
        nn_substeps = 10
        
        mano_nn_substeps = 1
        # mano_nn_substeps = 10 # 
        self.mano_nn_substeps = mano_nn_substeps


        
        params_to_train = []

        ''' Define MANO robot actions, delta_states, init_states, frictions, and others '''
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        self.mano_robot_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60, # embedding; a realistic thing # # ## so the optimizable modle deisgn --- approxmimate what you see and approximate the target simulator ## # at a distance; the asymmetric contact froces spring ks -- all of them wold affect model's behaviours ##
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_states.weight)
        self.mano_robot_states.weight.data[0, :] = self.mano_robot_init_states.weight.data[0, :].clone()
        
        
        # self.free_deformation_time_latent = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        # )
        # params_to_train += list(self.free_deformation_time_latent.parameters())
        
        
        
        ''' Load optimized MANO hand actions and states '''
        # ### laod optimized init actions #### # 
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            
            if 'mano_robot_states' in optimized_init_actions_ckpt:
                self.mano_robot_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_states'])
            
            if 'mano_robot_init_states' in optimized_init_actions_ckpt:
                self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_init_states'])
                
            if 'mano_robot_glb_rotation' in optimized_init_actions_ckpt:
                self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_rotation'])
            
            if 'mano_robot_glb_trans' in optimized_init_actions_ckpt: # mano_robot_glb_trans
                self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_trans'])
        
        
        # robot actions # # 
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22, #  22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        ## robot_delta_states, robot_states # #
        self.robot_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        params_to_train += list(self.robot_states.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        

        
        loaded_delta_states = False
        ### load optimized init transformations for robot actions ###
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_transformations']
            # cur_optimized_init_actions = # optimized init states # ## robot init states ##
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            try: ### load robot init states ###
                if optimized_init_actions_ckpt['robot_init_states']['weight'].size(0) > self.robot_init_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_init_states']['weight'].data = optimized_init_actions_ckpt['robot_init_states']['weight'].data[:self.robot_init_states.weight.data.size(0)]
                self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            except:
                pass
            if optimized_init_actions_ckpt['robot_glb_rotation']['weight'].size(0) > self.robot_glb_rotation.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data = optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data[:self.robot_glb_rotation.weight.data.size(0)]
            self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                loaded_delta_states = True
                if optimized_init_actions_ckpt['robot_delta_states']['weight'].size(0) > self.robot_delta_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_delta_states']['weight'].data = optimized_init_actions_ckpt['robot_delta_states']['weight'].data[:self.robot_delta_states.weight.data.size(0)]
                self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
                
            if 'robot_states' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_states']['weight'].size(0) > self.robot_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_states']['weight'].data = optimized_init_actions_ckpt['robot_states']['weight'].data[:self.robot_states.weight.data.size(0)]
                self.robot_states.load_state_dict(optimized_init_actions_ckpt['robot_states'])

            if 'robot_actions' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_actions']['weight'].size(0) > self.robot_actions.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_actions']['weight'].data = optimized_init_actions_ckpt['robot_actions']['weight'].data[:self.robot_actions.weight.data.size(0)]
                self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
                
            if optimized_init_actions_ckpt['robot_glb_trans']['weight'].data.size(0) > self.robot_glb_trans.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_trans']['weight'].data = optimized_init_actions_ckpt['robot_glb_trans']['weight'].data[:self.robot_glb_trans.weight.data.size(0)]
            self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])

        
        ''' Get delta states from states '''
        print(f"loaded_delta_states: {loaded_delta_states}")
        # if not loaded_delta_states:
        tot_robot_delta_states = []
        for i_fr in range(num_steps):
            if i_fr == 0:
                cur_robot_delta_state = self.robot_states.weight.data[0]
            else:
                cur_robot_delta_state = self.robot_states.weight.data[i_fr] - self.robot_states.weight.data[i_fr - 1]
            tot_robot_delta_states.append(cur_robot_delta_state)
        tot_robot_delta_states = torch.stack(tot_robot_delta_states, dim=0)
        self.robot_delta_states.weight.data.copy_(tot_robot_delta_states) # use delta states ##
            
        
        
        
        ''' mano corres '''
        # self.mano_mult_const_after_cent = 3.
        # mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        # if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
        #     mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        # self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        # self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        # print(f"mano_to_dyn_corr_pts_idxes: {self.mano_to_dyn_corr_pts_idxes.size()}")
        
        
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        self.robot_states_ori = self.robot_states.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        
        self.robot_states_sv = self.robot_states.weight.data.clone()
        
        self.nn_ts = self.nn_timesteps - 1

        ''' Set actions for the redmax simulation and add parameters to params-to-train '''
        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        
        
        
        def load_optimized_init_robo_transformations(robo_state_fn):
            
            # robot actions # # 
            self.robot_actions = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=22, #  22,
            ).cuda()
            torch.nn.init.zeros_(self.robot_actions.weight)
            # params_to_train += list(self.robot_actions.parameters())
            
            self.robot_delta_states = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=60,
            ).cuda()
            torch.nn.init.zeros_(self.robot_delta_states.weight)
            # params_to_train += list(self.robot_delta_states.parameters())
            
            ## robot_delta_states, robot_states # #
            self.robot_states = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=60,
            ).cuda()
            torch.nn.init.zeros_(self.robot_states.weight)
            # params_to_train += list(self.robot_states.parameters())
            
            self.robot_delta_states = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=60,
            ).cuda()
            torch.nn.init.zeros_(self.robot_states.weight)
            
            self.robot_init_states = nn.Embedding(
                num_embeddings=1, embedding_dim=22,
            ).cuda()
            torch.nn.init.zeros_(self.robot_init_states.weight)
            # params_to_train += list(self.robot_init_states.parameters())
            
            self.robot_glb_rotation = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=4
            ).cuda()
            self.robot_glb_rotation.weight.data[:, 0] = 1.
            self.robot_glb_rotation.weight.data[:, 1:] = 0.
            
            
            self.robot_glb_trans = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=3
            ).cuda()
            torch.nn.init.zeros_(self.robot_glb_trans.weight)
            
            
            
            print(f"[Robot] Loading optimized init transformations from {robo_state_fn}")
            cur_optimized_init_actions_fn = robo_state_fn ## get robot state fn ##
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            print(f"optimized_init_actions_ckpt loaded with keys: {optimized_init_actions_ckpt.keys()}")
            try:
                if optimized_init_actions_ckpt['robot_init_states']['weight'].size(0) > self.robot_init_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_init_states']['weight'].data = optimized_init_actions_ckpt['robot_init_states']['weight'].data[:self.robot_init_states.weight.data.size(0)]
                self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            except:
                pass
            print("a1")
            if optimized_init_actions_ckpt['robot_glb_rotation']['weight'].size(0) > self.robot_glb_rotation.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data = optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data[:self.robot_glb_rotation.weight.data.size(0)]
            self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            print("a2")
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                loaded_delta_states = True
                if optimized_init_actions_ckpt['robot_delta_states']['weight'].size(0) > self.robot_delta_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_delta_states']['weight'].data = optimized_init_actions_ckpt['robot_delta_states']['weight'].data[:self.robot_delta_states.weight.data.size(0)]
                self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            print("a3") 
            if 'robot_states' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_states']['weight'].size(0) > self.robot_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_states']['weight'].data = optimized_init_actions_ckpt['robot_states']['weight'].data[:self.robot_states.weight.data.size(0)]
                self.robot_states.load_state_dict(optimized_init_actions_ckpt['robot_states'])
            print("a4")
            if 'robot_actions' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_actions']['weight'].size(0) > self.robot_actions.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_actions']['weight'].data = optimized_init_actions_ckpt['robot_actions']['weight'].data[:self.robot_actions.weight.data.size(0)]
                self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            print("a5")
            if optimized_init_actions_ckpt['robot_glb_trans']['weight'].data.size(0) > self.robot_glb_trans.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_trans']['weight'].data = optimized_init_actions_ckpt['robot_glb_trans']['weight'].data[:self.robot_glb_trans.weight.data.size(0)]
            self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
            print("a6")
            tot_robot_delta_states = []
            for i_fr in range(num_steps):
                if i_fr == 0:
                    cur_robot_delta_state = self.robot_states.weight.data[0]
                else:
                    cur_robot_delta_state = self.robot_states.weight.data[i_fr] - self.robot_states.weight.data[i_fr - 1]
                tot_robot_delta_states.append(cur_robot_delta_state)
            tot_robot_delta_states = torch.stack(tot_robot_delta_states, dim=0)
            self.robot_delta_states.weight.data.copy_(tot_robot_delta_states) # use delta states ##
            
            print(f"All weights loaded!")
            # self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
            print(f"b1")
            self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
            print(f"b2")
            self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
            print(f"b3")
            self.robot_actions_ori = self.robot_actions.weight.data.clone()
            print(f"b4")
            self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
            print(f"b5")
            self.robot_states_ori = self.robot_states.weight.data.clone()
            print(f"b6")
            # self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
            # self.robot_states_sv = self.robot_states.weight.data.clone()
            
            print(f"Ori weights saved!")
            return loaded_delta_states
            
        
        # maxx robo pts ##
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        # self.mano_mult_const_after_cent = 3. # mult 
        
        if 'model.mano_mult_const_after_cent' in self.conf:
            self.mano_mult_const_after_cent = self.conf['model.mano_mult_const_after_cent']
            
        
        ## redmax hand ##
        if self.hand_type == "redmax_hand":
            self.maxx_robo_pts = 25.
            self.minn_robo_pts = -15.
            self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
            self.mult_const_after_cent = 0.5437551664260203 ## 
        else:
            self.minn_robo_pts = -0.1
            self.maxx_robo_pts = 0.2
            self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
            self.mult_const_after_cent = 0.437551664260203 ## should modify
        ## for grab ##
        self.mult_const_after_cent = self.mult_const_after_cent / 3. * 0.9507
        
        
        
        ### Constraint set ###
        # self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        self.timestep_to_active_mesh = {}
        # self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        
        self.timestep_to_active_mesh_w_delta_states = {}
        
        timestep_to_tot_rot = {}
        timestep_to_tot_trans = {}
        
        self.timestep_to_active_mesh_wo_glb = {}
        
        
        with torch.no_grad():
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                ''' Get rotations, translations, and actions of the current robot '''
                cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                
                robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts) #
                
                # robo_links_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                # self.robot_agent.set_init_states_target_value(robo_links_states)
                # cur_robo_visual_pts = self.robot_agent.get_init_state_visual_pts()
                cur_robo_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                
                
                self.timestep_to_active_mesh_wo_glb[cur_ts] = cur_robo_visual_pts.detach().clone()
                
                
                if not self.use_scaled_robot_hand:
                    ### transform the visual pts ###
                    cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                    cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                    cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                    
                
                cur_rot = cur_robo_glb_rot
                cur_trans = cur_robo_glb_trans
                
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                
                cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0)
                
                self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts.detach().clone() # robo visual pts #

        self.iter_step = 0
        self.validate_mesh_robo()
        
        
        
        def get_initial_meshes():
            with torch.no_grad():
                for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                    cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                    cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                    cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts) #


                    cur_robo_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                    
                    self.timestep_to_active_mesh_wo_glb[cur_ts] = cur_robo_visual_pts.detach().clone()
                    
                    if not self.use_scaled_robot_hand:
                        ### transform the visual pts ###
                        cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                        cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                        
                    cur_rot = cur_robo_glb_rot
                    cur_trans = cur_robo_glb_trans
                    
                    timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                    timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                    
                    cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0)
                    
                    self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts.detach().clone() # robo visual pts #

            self.iter_step = 0
            self.validate_mesh_robo()
        
        def get_optimizer():
            
            ''' Set redmax robot actions '''
            params_to_train_kines = []
            
            
            if self.opt_robo_states:
                params_to_train_kines += list(self.robot_delta_states.parameters())
                params_to_train_kines += list(self.robot_actions.parameters())
            if self.opt_robo_glb_trans:
                params_to_train_kines += list(self.robot_glb_trans.parameters())
            if self.opt_robo_glb_rot:
                params_to_train_kines += list(self.robot_glb_rotation.parameters())
            
            
            
            if self.use_LBFGS:
                self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
            else:
                self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            
            if self.optimize_rules:
                params_to_train_kines = []
                print("optimizing rules!")
                params_to_train_kines += list(self.other_bending_network.parameters())
                if self.use_LBFGS:
                    self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
                else:
                    self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
                # self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=1e-2)
            
            self.iter_step = 0
        
        
        ''' Set redmax robot actions '''
        params_to_train_kines = []
        
        
        if self.opt_robo_states:
            params_to_train_kines += list(self.robot_delta_states.parameters())
            params_to_train_kines += list(self.robot_actions.parameters())
        if self.opt_robo_glb_trans:
            params_to_train_kines += list(self.robot_glb_trans.parameters())
        if self.opt_robo_glb_rot:
            params_to_train_kines += list(self.robot_glb_rotation.parameters())
        
        
        
        if self.use_LBFGS:
            self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
        else:
            self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        
        if self.optimize_rules:
            params_to_train_kines = []
            print("optimizing rules!")
            params_to_train_kines += list(self.other_bending_network.parameters())
            if self.use_LBFGS:
                self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
            else:
                self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            # self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=1e-2)
        

        self.iter_step = 0

        self.minn_tracking_loss = 1e27
        
        # best res from previous stage? ##
        
        # threshold 0d3 - threshold 0d3 with optimizing robot 
        
        base_exp_dir_root = self.base_exp_dir
        
        # exp_tags = ["thres0d3", "robo", "thres0d25", "robo", "thres0d2", "robo", "thres0d1", "robo", "thres0d0", "robo"]
        # distance_threshold = [0.3, 0.3, 0.25, 0.25, 0.2, 0.2, 0.1, 0.1, 0.0, 0.0]
        # # exp_nn_iters = [1500, 100, 1500, 100, 1500, 100, 1500, 100, 1500, 100]
        # exp_nn_iters = [1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100]
        # optimizing_rules = [True, False, True, False, True, False, True, False, True, False]
        
        
        exp_tags = ["thres0d1", "robo", "thres0d05", "robo", "thres0d03", "robo", "thres0d025", "robo", "thres0d02", "robo", "thres0d015", "robo", "thres0d01", "robo", "thres0d00", "robo"]
        distance_threshold = [0.1, 0.1, 0.05, 0.05, 0.03, 0.03, 0.025, 0.025, 0.02, 0.02, 0.015, 0.015, 0.01, 0.01, 0.00, 0.00]
        spring_ks_contact = [4e4, 4e4, 8e4, 8e4, 1e5, 1e5, 2e5, 2e5, 3e5, 3e5, 3.5e5, 3.5e5, 4e5, 4e5, 4e6, 4e6]
        friction_spring_ks_contact = [1e5, 1e5, 2e5, 2e5, 4e5, 4e5, 5e5, 5e5, 6e5, 6e5, 8e5, 8e5, 1e6, 1e6, 1e7, 1e7]
        # exp_nn_iters = [1500, 100, 1500, 100, 1500, 100, 1500, 100, 1500, 100]
        exp_nn_iters = [1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, ]
        optimizing_rules = [True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False,]
        
        print(f"exp_tags: {len(exp_tags)}, distance_threshold: {len(distance_threshold)}, spring_ks_contact: {len(spring_ks_contact)}, friction_spring_ks_contact: {len(friction_spring_ks_contact)}, exp_nn_iters: {len(exp_nn_iters)}, optimizing_rules: {len(optimizing_rules)}")
        
        # exp_tags = ["thres0d25", "robo", "thres0d2", "robo", "thres0d1", "robo", "thres0d0", "robo"]
        # distance_threshold = [ 0.25, 0.25, 0.2, 0.2, 0.1, 0.1, 0.0, 0.0]
        # # exp_nn_iters = [1500, 100, 1500, 100, 1500, 100, 1500, 100, 1500, 100]
        # exp_nn_iters = [1000, 100, 1000, 100, 1000, 100, 1000, 100]
        # optimizing_rules = [ True, False, True, False, True, False, True, False]
        
        full_exp_tag = "" ## 

        for i_exp, (cur_exp_tag, cur_dist_thres, cur_nn_iters) in enumerate(zip(exp_tags, distance_threshold, exp_nn_iters)):
            print(f"i_exp: {i_exp}, cur_exp_tag: {cur_exp_tag}, dist_thres: {cur_dist_thres}, cur_nn_iters: {cur_nn_iters}")
            full_exp_tag = full_exp_tag + cur_exp_tag + "_"
            
            self.base_exp_dir = os.path.join(base_exp_dir_root, full_exp_tag)
            os.makedirs(self.base_exp_dir, exist_ok=True)
            print(f"Current base_exp_dir: {self.base_exp_dir}")
            
            self.other_bending_network.minn_dist_threshold_robot_to_obj = cur_dist_thres
            self.minn_dist_threshold_robot_to_obj = cur_dist_thres
            
            # self.other_bending_network.minn_dist_threshold_robot_to_obj = cur_dist_thres
            self.other_bending_network.penetration_proj_k_to_robot = spring_ks_contact[i_exp]
            ## i_exp ##
            self.other_bending_network.penetration_proj_k_to_robot_friction = friction_spring_ks_contact[i_exp]
            
            # self.minn_dist_threshold_robot_to_obj = cur_dist_thres
            
            
            self.minn_tracking_loss = 1e27
            self.best_iter = 0
            
            # if i_exp <= 5:
            #     continue
            # if i_exp == 6:
            #     best_ckpt_path = "/data2/datasets/xueyi/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_train_retargeted_shadow_hand_states_optrobot__seq_taco_20230926_035_thres0d3_multi_stages_/thres0d3_robo_thres0d25_robo_thres0d2_robo_thres0d1_/checkpoints/ckpt_best.pth"
            #     load_optimized_init_robo_transformations(best_ckpt_path)
            #     self.load_checkpoint_via_fn(best_ckpt_path)
            #     get_initial_meshes()
                
            #     nex_optimizing_rules = optimizing_rules[i_exp + 1]
            #     self.optimize_rules = nex_optimizing_rules
            #     get_optimizer()
            #     continue
            
            
            for i_iter in tqdm(range(cur_nn_iters)):
                
                # for i_iter in tqdm(range(100000)):
                tot_losses = []
                tot_tracking_loss = []
                
                
                self.timestep_to_posed_active_mesh = {}
                self.timestep_to_posed_mano_active_mesh = {}
                self.timestep_to_mano_active_mesh = {}
                self.timestep_to_corr_mano_pts = {}
                timestep_to_tot_rot = {}
                timestep_to_tot_trans = {}

                self.timestep_to_raw_active_meshes = {}
                self.timestep_to_penetration_points = {}
                self.timestep_to_penetration_points_forces = {}
                self.joint_name_to_penetration_forces_intermediates = {}
                
                
                self.ts_to_contact_passive_normals = {}
                self.ts_to_passive_normals = {}
                self.ts_to_passive_pts = {}
                self.ts_to_contact_force_d = {}
                self.ts_to_penalty_frictions = {}
                self.ts_to_penalty_disp_pts = {}
                self.ts_to_redmax_states = {}
                self.ts_to_mano_rhand_meshes = {}
                
                contact_pairs_set = None
                self.contact_pairs_sets = {}
                
                tot_penetration_depth = []
                
                robo_actions_diff_loss = []
                mano_tracking_loss = []
                
                
                penetration_forces = None
                sampled_visual_pts_link_idxes = None
                
                ### 
                # init global transformations ##
                # cur_ts_redmax_delta_rotations = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda()
                # cur_ts_redmax_delta_rotations = torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda()
                # cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda()
                
                # for cur_ts in range(self.nn_ts):
                for cur_ts in range(self.nn_ts * self.mano_nn_substeps):

                    self.free_def_bending_weight = 0.0

                    cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                    cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                    cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    
                    # robo_links_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    # self.robot_agent.set_init_states_target_value(robo_links_states)
                    
                    if self.drive_robot == 'actions':
                        if cur_ts == 0:
                            robo_init_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                            self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_init_states, cur_ts) 
                        else:
                            # (robot_actions_dim, )
                            cur_ts_robo_actions = self.robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                            self.robot_agent.set_actions_and_update_states_v2(cur_ts_robo_actions, cur_ts, penetration_forces=penetration_forces, sampled_visual_pts_joint_idxes=sampled_visual_pts_link_idxes)
                        # print(f"updating states...")
                        self.robot_states.weight.data[cur_ts, :] = self.robot_agent.get_joint_state(cur_ts, self.robot_states.weight.data[cur_ts, :])
                        self.robot_states_sv[cur_ts, : ] = self.robot_agent.get_joint_state(cur_ts, self.robot_states_sv[cur_ts, :])
                    else:
                        robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                        self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts) ## delta states ##
                        self.robot_states.weight.data[cur_ts, :] = self.robot_agent.get_joint_state(cur_ts - 1, self.robot_states.weight.data[cur_ts, :])
                        self.robot_states_sv[cur_ts, : ] = self.robot_agent.get_joint_state(cur_ts - 1, self.robot_states_sv[cur_ts, :])
                    
                    # tate_vals = self.robot_agent.get_joint_state( cur_ts, state_vals, link_name_to_link_struct)
                    # self.robot_states.weight.data[cur_ts, :] = self.robot_agent.get_joint_state(cur_ts, self.robot_states.weight.data[cur_ts, :])
                    
                    # robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    # self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts) ## delta states ##
                    #### get the visual pts ####
                    ## 
                    # cur_verts, joint_idxes =  get_init_state_visual_pts(expanded_pts=False, ret_joint_idxes=True)
                    cur_robo_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                    
                    # timestep_to_active_mesh_wo_glb # 
                    saved_robo_visual_pts = self.timestep_to_active_mesh_wo_glb[cur_ts]
                    ### calculate distance ###
                    dist_cur_robo_visual_pts_w_saved_robot_pts = torch.sum(
                        (cur_robo_visual_pts - saved_robo_visual_pts) ** 2, dim=-1
                    )
                    dist_cur_robo_visual_pts_w_saved_robot_pts = dist_cur_robo_visual_pts_w_saved_robot_pts.mean()
                    

                    if not self.use_scaled_robot_hand:
                        ### transform the visual pts ###
                        cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                        cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                    
                    
                    cur_rot = cur_robo_glb_rot
                    cur_trans = cur_robo_glb_trans
                    
                    timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                    timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                    
                    ## timestep ## vi
                    
                    ## 
                    ### transform by the glboal transformation and the translation ###
                    cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0)
                    
                    

                    self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts # .clone() # robo visual pts #
                    
                    self.ts_to_mano_rhand_meshes[cur_ts] = self.rhand_verts[cur_ts]
           
                    self.free_def_bending_weight = 0.0

                        
                    if contact_pairs_set is None:
                        self.contact_pairs_set = None
                    else:
                        self.contact_pairs_set = contact_pairs_set.copy()
                    
                    
                    if self.optim_sim_model_params_from_mano:
                        # print(f"Using mano mesh models for optimization")
                        act_mesh_pts_dict_to_model = self.ts_to_mano_rhand_meshes
                    else:
                        act_mesh_pts_dict_to_model = self.timestep_to_active_mesh

                    
                    contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=act_mesh_pts_dict_to_model, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)
                    
                    

    
                    if self.train_with_forces_to_active and (not self.use_mano_inputs): 
                        # penetration_forces #
                        if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                            net_penetrating_forces = self.other_bending_network.penetrating_forces
                            net_penetrating_points = self.other_bending_network.penetrating_points

                            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                            self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                            self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                            
                            
                            ### transform the visual pts ###
                            # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                            # cur_visual_pts = cur_visual_pts * 2. - 1.
                            # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                            
                            # sampled_visual_pts_joint_idxes = robo_visual_pts_link_idxes[finger_sampled_idxes][self.other_bending_network.penetrating_indicator]
                            # print(f"robo_visual_pts_link_idxes: {robo_visual_pts_link_idxes.size()}")
                            sampled_visual_pts_link_idxes = robo_visual_pts_link_idxes[self.other_bending_network.penetrating_indicator]
                            
                            net_penetrating_forces = torch.matmul(
                                cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                            ).transpose(1, 0)
                            net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                            net_penetrating_forces = net_penetrating_forces / 2
                            net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                            
                            net_penetrating_points = torch.matmul(
                                cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                            ).transpose(1, 0)
                            net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                            net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                            net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                            
                            penetration_forces = {
                                'penetration_forces': net_penetrating_forces,
                                'penetration_forces_points': net_penetrating_points
                            }
                            
                            # link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                            
                        else:
                            penetration_forces = None
                            sampled_visual_pts_link_idxes = None
                            # link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                            
                    if contact_pairs_set is not None:
                        self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                    
                    # contact force d 
                    self.ts_to_contact_passive_normals[cur_ts] = self.other_bending_network.tot_contact_passive_normals.detach().cpu().numpy()
                    self.ts_to_passive_pts[cur_ts] = self.other_bending_network.cur_passive_obj_verts.detach().cpu().numpy()
                    self.ts_to_passive_normals[cur_ts] = self.other_bending_network.cur_passive_obj_ns.detach().cpu().numpy()
                    self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                    self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                    if self.other_bending_network.penalty_based_friction_forces is not None:
                        self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                    
                    # # get the penetration depth of the bending network #
                    ## total penetration depth ##
                    tot_penetration_depth.append(self.other_bending_network.penetrating_depth_penalty.detach().item())
                    
                    tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_ts + 1)
                    tot_tracking_loss.append(tracking_loss.detach().cpu().item())

                    # print(f"tracking_loss: {tracking_loss}")

                    # hand_tracking_loss = torch.sum( ## delta states? ##
                    #     (self.timestep_to_active_mesh_w_delta_states[cur_ts] - cur_visual_pts) ** 2, dim=-1
                    # )
                    # hand_tracking_loss = hand_tracking_loss.mean()
                    
                    
                    # loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                    # # diff_redmax_visual_pts_with_ori_visual_pts.backward()
                    penetraton_penalty = self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                    
                    
                    # # print(f"after calculating losses....") # #
                    
                    # robot_states_ori, robot_glb_trans_ori, robot_glb_rotation_ori ### robot glb rotation ori ###
                    # # # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
                    robot_rotation_diff = torch.sum(
                        (self.robot_glb_rotation_ori - self.robot_glb_rotation.weight) ** 2, dim=-1
                    )
                    robot_rotation_diff = robot_rotation_diff.mean()
                    
                    robot_trans_diff = torch.sum(
                        (self.robot_glb_trans_ori - self.robot_glb_trans.weight) ** 2, dim=-1
                    )
                    robot_trans_diff = robot_trans_diff.mean()
                    
                    robot_states_diff = torch.sum(
                        (self.robot_states_ori - self.robot_states.weight) ** 2, dim=-1
                    )
                    robot_states_diff = robot_states_diff.mean()
                    
                    robot_delta_states_diff = torch.sum(
                        (self.robot_delta_states_ori - self.robot_delta_states.weight) ** 2, dim=-1
                    )
                    robot_delta_states_diff = robot_delta_states_diff.mean()
                    
                    transformation_diff_loss = robot_rotation_diff + robot_trans_diff + robot_states_diff + robot_delta_states_diff
                    
                    
                    # kinematics_proj_loss = hand_tracking_loss * 1e2
                    
                    
                    # kinematics_proj_loss = tracking_loss * self.loss_scale_coef  + transformation_diff_loss * 1.0 + penetraton_penalty
                    kinematics_proj_loss = tracking_loss * self.loss_scale_coef  + transformation_diff_loss * self.diff_reg_coef + penetraton_penalty
                    
                    
                    
                    if self.drive_robot == 'actions':
                        kinematics_proj_loss = kinematics_proj_loss + dist_cur_robo_visual_pts_w_saved_robot_pts
                        # kinematics_proj_loss =  dist_cur_robo_visual_pts_w_saved_robot_pts
        
                    
                    loss = kinematics_proj_loss
                    
                    
                    diff_hand_tracking = dist_cur_robo_visual_pts_w_saved_robot_pts
                    mano_tracking_loss.append(diff_hand_tracking.detach().cpu().item())
                    
                    
                    tot_losses.append(kinematics_proj_loss)
                    
                    robot_states_actions_diff_loss = transformation_diff_loss.item()
                    robo_actions_diff_loss.append(robot_states_actions_diff_loss)

                    self.iter_step += 1
                    self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                    
                    
                    torch.cuda.empty_cache()
                    
                tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
                saved_best = False
                # best_iter, minn_tracking_loss, best_ckpt_path
                if tot_tracking_loss < self.minn_tracking_loss:
                    print(f"Saving best checkpoint with tracking_loss: {tot_tracking_loss} at iter_step: {self.iter_step}")
                    self.best_iter = i_iter ## get i_iter here ##
                    self.minn_tracking_loss = tot_tracking_loss
                    best_ckpt_path = self.save_checkpoint(tag="best")
                    
                    # self.validate_mesh_robo()
                    # ### test for contact infos ###
                    # self.validate_contact_info_robo()
                    saved_best = True
                
                ########## Update v2 ##########
                loss = sum(tot_losses)
                self.kines_optimizer.zero_grad()
                try:
                    loss.backward(retain_graph=True)
                    self.kines_optimizer.step()
                except:
                    pass
                self.update_learning_rate() 
                ########## Update v2 ##########
                
                
                ''' Get nn_forward_ts and backward through the actions for updating '''
                ## save ckpt ##
                if i_iter == 0 or (i_iter % self.ckpt_sv_freq == 0):
                    self.save_checkpoint() 
                
                tot_losses = sum(tot_losses) / float(len(tot_losses))
                
                tot_penetration_depth = sum(tot_penetration_depth) / float(len(tot_penetration_depth))
                robo_actions_diff_loss = sum(robo_actions_diff_loss) / float(len(robo_actions_diff_loss))
                mano_tracking_loss = sum(mano_tracking_loss) / float(len(mano_tracking_loss))
                
                
                
                if i_iter % self.report_freq == 0:
                    logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                    
                    cur_log_sv_str = 'iter:{:8>d} loss = {} tracking_loss = {} mano_tracking_loss = {} penetration_depth = {} actions_diff_loss = {} lr={}'.format(self.iter_step, tot_losses, tot_tracking_loss, mano_tracking_loss, tot_penetration_depth, robo_actions_diff_loss, self.optimizer.param_groups[0]['lr'])
                    
                    print(cur_log_sv_str)
                    ''' Dump to the file '''
                    with open(logs_sv_fn, 'a') as log_file: 
                        log_file.write(cur_log_sv_str + '\n')
                    print(f"spring_ks_values: { self.other_bending_network.optimizable_spring_ks.weight.data.detach().cpu().numpy() }")
                
                # self.validate_mesh_robo_a() ## not saved best and not 
                if (i_iter % self.val_mesh_freq == 0): #  and (not saved_best):
                    self.validate_mesh_robo()
                    ### test for contact infos ###
                    self.validate_contact_info_robo()
                    
                
                torch.cuda.empty_cache()
                
                if (i_iter >= 100) and self.minn_tracking_loss < 4e-5:
                    break
    
            # best_iter, minn_tracking_loss, best_ckpt_path
            print(f"Beest iter: {self.best_iter}, minn_tracking_loss: {self.minn_tracking_loss}, best_ckpt_path: {best_ckpt_path}")
            ## best_ckpt_path ##
            
            if i_exp < len(exp_tags) - 1:
                
                load_optimized_init_robo_transformations(best_ckpt_path)
                self.load_checkpoint_via_fn(best_ckpt_path)
                get_initial_meshes()
                
                nex_optimizing_rules = optimizing_rules[i_exp + 1]
                self.optimize_rules = nex_optimizing_rules
                get_optimizer()
                
            
    
    ''' GRAB clips; Shadow hand '''
    def train_manip_acts_params_curriculum(self, ):
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load #
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path'] # 
        
        if 'scaled' in model_path:
            self.use_scaled_robot_hand = True
        else:
            self.use_scaled_robot_hand = False
        
        self.hand_type = "redmax_hand"
        # if model_path.endswith(".xml"):
        #     self.hand_type = "redmax_hand"
        #     robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        # else:
        self.hand_type = "shadow_hand" ## shadow hand ## robot agent ## robot hand ## two hands? ## three hands ## 
        ## the shadow hand -> from right to left -> how to create the left hand #
        # reflect the hand via hand palm plane -> mirroring # # mirroring the hand using the hand palm plane ##
        robot_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        
        
        ## sampled verts idxes ## 
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        
        ''' Load the mano hand '''
        model_path_mano = self.conf['model.mano_sim_model_path']
        # mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano) # robot #
        mano_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path_mano) ## model path mano ## #  ## modeljpath 
        self.mano_agent = mano_agent
        # ''' Load the mano hand '''
        
        # self.robo_hand_faces = self.mano_agent.robot_faces
        
        # if self.use_mano_hand_for_test:
        #     self.robo_hand_faces = self.hand_faces
        
        
        
        ''' Load robot hand in DiffHand simulator '''
        xml_shadow_hand_fn = "rsc/shadow_hand_description/shadowhand_new_scaled_nroot.xml"
        redmax_sim = redmax.Simulation(xml_shadow_hand_fn) ## diffhand simulator ##
        redmax_sim.reset(backward_flag = True) # redmax_sim -- 
        # # ### redmax_ndof_u, redmax_ndof_r ###
        redmax_ndof_u = redmax_sim.ndof_u
        redmax_ndof_r = redmax_sim.ndof_r
        redmax_ndof_m = redmax_sim.ndof_m
        print(f"[REDMAX] ndof_u: {redmax_ndof_u}, ndof_r: {redmax_ndof_r}, ndof_m: {redmax_ndof_m}")
        ''' Load robot hand in DiffHand simulator '''
        
        
        
        nn_substeps = 10
        
        mano_nn_substeps = 1
        # mano_nn_substeps = 10 # 
        self.mano_nn_substeps = mano_nn_substeps


        
        params_to_train = []

        ''' Define MANO robot actions, delta_states, init_states, frictions, and others '''
        self.mano_robot_actions = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # params_to_train += list(self.robot_actions.parameters())
        
        self.mano_robot_delta_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # params_to_train += list(self.robot_delta_states.parameters())
        
        self.mano_robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        # params_to_train += list(self.robot_glb_rotation.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        # params_to_train += list(self.robot_glb_trans.parameters())   
        
        self.mano_robot_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_states.weight)
        self.mano_robot_states.weight.data[0, :] = self.mano_robot_init_states.weight.data[0, :].clone()
        
        
        # self.free_deformation_time_latent = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=self.bending_latent_size
        # )
        # params_to_train += list(self.free_deformation_time_latent.parameters())
        
        
        
        ''' Load optimized MANO hand actions and states '''
        # ### laod optimized init actions #### # 
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            
            if 'mano_robot_states' in optimized_init_actions_ckpt:
                self.mano_robot_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_states'])
            
            if 'mano_robot_init_states' in optimized_init_actions_ckpt:
                self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_init_states'])
                
            if 'mano_robot_glb_rotation' in optimized_init_actions_ckpt:
                self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_rotation'])
            
            if 'mano_robot_glb_trans' in optimized_init_actions_ckpt: # mano_robot_glb_trans
                self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_trans'])
        
        
        # robot actions # # 
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=22, #  22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        ## robot_delta_states, robot_states # #
        self.robot_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        params_to_train += list(self.robot_states.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        

        
        loaded_delta_states = False
        ### load optimized init transformations for robot actions ###
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_transformations']
            # cur_optimized_init_actions = # optimized init states # ## robot init states ##
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            try: ### load robot init states ###
                if optimized_init_actions_ckpt['robot_init_states']['weight'].size(0) > self.robot_init_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_init_states']['weight'].data = optimized_init_actions_ckpt['robot_init_states']['weight'].data[:self.robot_init_states.weight.data.size(0)]
                self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            except:
                pass
            if optimized_init_actions_ckpt['robot_glb_rotation']['weight'].size(0) > self.robot_glb_rotation.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data = optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data[:self.robot_glb_rotation.weight.data.size(0)]
            self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                loaded_delta_states = True
                if optimized_init_actions_ckpt['robot_delta_states']['weight'].size(0) > self.robot_delta_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_delta_states']['weight'].data = optimized_init_actions_ckpt['robot_delta_states']['weight'].data[:self.robot_delta_states.weight.data.size(0)]
                self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
                
            if 'robot_states' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_states']['weight'].size(0) > self.robot_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_states']['weight'].data = optimized_init_actions_ckpt['robot_states']['weight'].data[:self.robot_states.weight.data.size(0)]
                self.robot_states.load_state_dict(optimized_init_actions_ckpt['robot_states'])

            if 'robot_actions' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_actions']['weight'].size(0) > self.robot_actions.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_actions']['weight'].data = optimized_init_actions_ckpt['robot_actions']['weight'].data[:self.robot_actions.weight.data.size(0)]
                self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
                
            if optimized_init_actions_ckpt['robot_glb_trans']['weight'].data.size(0) > self.robot_glb_trans.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_trans']['weight'].data = optimized_init_actions_ckpt['robot_glb_trans']['weight'].data[:self.robot_glb_trans.weight.data.size(0)]
            self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])

        
        ''' Get delta states from states '''
        print(f"loaded_delta_states: {loaded_delta_states}")
        # if not loaded_delta_states:
        tot_robot_delta_states = []
        for i_fr in range(num_steps):
            if i_fr == 0:
                cur_robot_delta_state = self.robot_states.weight.data[0]
            else:
                cur_robot_delta_state = self.robot_states.weight.data[i_fr] - self.robot_states.weight.data[i_fr - 1]
            tot_robot_delta_states.append(cur_robot_delta_state)
        tot_robot_delta_states = torch.stack(tot_robot_delta_states, dim=0)
        self.robot_delta_states.weight.data.copy_(tot_robot_delta_states) # use delta states ##
            
        ''' define and run through the redmax sim '''
        ### redmax robot actions ####
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps * nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        # params_to_train += list(self.redmax_robot_actions.parameters())
        params_to_train_kines = []
        params_to_train_kines += list(self.redmax_robot_actions.parameters())
        self.actions_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate_actions)
        
        
        if 'model.load_redmax_robot_actions_fn' in self.conf and len(self.conf['model.load_redmax_robot_actions_fn']) > 0:
            
            load_redmax_robot_actions_fn = self.conf['model.load_redmax_robot_actions_fn'] # 
            
            print(f"Loading redmax actions from {load_redmax_robot_actions_fn}")
            
            if load_redmax_robot_actions_fn.endswith(".pth"):
                redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
                self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
            elif load_redmax_robot_actions_fn.endswith(".npy"):
                redmax_robot_actions = np.load(load_redmax_robot_actions_fn, allow_pickle=True)
                redmax_robot_actions = torch.from_numpy(redmax_robot_actions).float().cuda()
                self.redmax_robot_actions.weight.data[: redmax_robot_actions.size(0), : redmax_robot_actions.size(1)] = redmax_robot_actions.clone()
        ##### redmax robot actions #####
        
        
        self.redmax_robot_actions_ori = self.redmax_robot_actions.weight.data.clone()
            
        
        ## redmax sim is
        robo_states = self.robot_states.weight.data[:, self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()
        
        self.nn_ts = self.nn_timesteps - 1
        
        ''' Initialize '''
        with torch.no_grad():
            nn_iters_opt_redmax_actions = 2
            for i_iter in tqdm(range(nn_iters_opt_redmax_actions)):
                redmax_sim.reset(backward_flag = True) 
                robo_intermediates_states = []
                tot_redmax_actions = []
                tot_visual_pts = []
                df_dqs = []
                tot_diff_qs = []
                for cur_ts in range(self.nn_ts):
                    
                    if cur_ts == 0:
                        links_init_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                        # self.robot_agent.set_init_states_target_value(links_init_states)
                        # cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                        # cur_visual_pts = self.timestep_to_active_mesh_wo_glb[cur_ts]
                        ## redmax q init ## link initstates ## ## robot agent ## 
                        redmax_q_init = links_init_states[self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()
                        
                        # redmax_q_init = np.zeros((redmax_ndof_r,), dtype=np.float32)
                        # redmax_q_init = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
                        redmax_q_init = redmax_q_init[:redmax_ndof_r]
                        redmax_sim.set_q_init(redmax_q_init)
                        
                        if i_iter % 100 == 0  or (i_iter == 1)  or (i_iter == nn_iters_opt_redmax_actions - 1):
                            
                            
                            ### redmax_sim_q ## ---> use that for the real act joint idx setting ##? 
                            redmax_sim_q_th = torch.from_numpy(redmax_q_init).float().cuda()
                            redmax_sim_q_th = torch.cat(
                                [torch.zeros((2,), dtype=torch.float32).float().cuda(), redmax_sim_q_th], dim=0 ### use the redmax sim q th
                            )
                            self.robot_agent.active_robot.set_delta_state_and_update_v2(redmax_sim_q_th, 0, use_real_act_joint=True) ## delta states ##
                            cur_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                            
                            
                    
                    else:
                        
                        for i_sub_step in range(nn_substeps): ## # ## #
                            cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
                            redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
                            redmax_u = redmax_actions.detach().cpu().numpy()
                            redmax_u = redmax_u[: redmax_ndof_u]
                            
                            ## redmax_u ##
                            # redmax_u = redmax_robot_actions[cur_tot_substeps].detach().cpu().numpy()[ : redmax_ndof_u]
                            
                            redmax_sim.set_u(redmax_u) # sim
                            tot_redmax_actions.append(redmax_actions)
                            ''' set and set the virtual forces ''' ## redmax_sim
                            virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                            redmax_sim.set_forces(virtual_force) ### virtual forces ###

                            redmax_sim.forward(1, verbose = False)
                            redmax_sim_q = redmax_sim.get_q() # get q # get q ## 
                            if i_sub_step == nn_substeps - 1:
                                robo_intermediates_states.append(redmax_sim_q)
                                df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts]))
                                cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts]) ** 2)
                                tot_diff_qs.append(cur_diff_qs.item())
                            else:
                                df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                        
                        if i_iter % 100 == 0  or (i_iter == 1)  or (i_iter == nn_iters_opt_redmax_actions - 1):
                            
                            
                            ########## robot_agent and setting active joint states #############
                            redmax_sim_q_th = torch.from_numpy(redmax_sim_q).float().cuda()
                            redmax_sim_q_th = torch.cat(
                                [torch.zeros((2,), dtype=torch.float32).float().cuda(), redmax_sim_q_th], dim=0 ### use the redmax sim q th
                            )
                            self.robot_agent.active_robot.set_delta_state_and_update_v2(redmax_sim_q_th, 0, use_real_act_joint=True) ## delta states ##
                            cur_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                            
                    
                    if i_iter % 100 == 0   or (i_iter == 1)  or (i_iter == nn_iters_opt_redmax_actions - 1):
                        tot_visual_pts.append(cur_visual_pts.detach().cpu().numpy())
                
                
                tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) 
                self.tot_redmax_actions = tot_redmax_actions
                
                diff_qs = sum(tot_diff_qs) / float(len(tot_diff_qs)) ## tot diff qs ##
                cur_log_sv_str = 'iter:{:8>d} diff_qs = {} lr={}'.format(i_iter, diff_qs, self.actions_optimizer.param_groups[0]['lr'])
                # get states and then use the obtianed states for actions optim?
                print(cur_log_sv_str)
        
        
        
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        self.robot_states_ori = self.robot_states.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        
        self.robot_states_sv = self.robot_states.weight.data.clone()
        
        self.nn_ts = self.nn_timesteps - 1

        ''' Set actions for the redmax simulation and add parameters to params-to-train '''
        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        
        
        
        def load_optimized_init_robo_transformations(robo_state_fn):
            
            # robot actions # # 
            self.robot_actions = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=22, #  22,
            ).cuda()
            torch.nn.init.zeros_(self.robot_actions.weight)
            # params_to_train += list(self.robot_actions.parameters())
            
            self.robot_delta_states = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=60,
            ).cuda()
            torch.nn.init.zeros_(self.robot_delta_states.weight)
            # params_to_train += list(self.robot_delta_states.parameters())
            
            ## robot_delta_states, robot_states # #
            self.robot_states = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=60,
            ).cuda()
            torch.nn.init.zeros_(self.robot_states.weight)
            # params_to_train += list(self.robot_states.parameters())
            
            self.robot_delta_states = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=60,
            ).cuda()
            torch.nn.init.zeros_(self.robot_states.weight)
            
            self.robot_init_states = nn.Embedding(
                num_embeddings=1, embedding_dim=22,
            ).cuda()
            torch.nn.init.zeros_(self.robot_init_states.weight)
            # params_to_train += list(self.robot_init_states.parameters())
            
            self.robot_glb_rotation = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=4
            ).cuda()
            self.robot_glb_rotation.weight.data[:, 0] = 1.
            self.robot_glb_rotation.weight.data[:, 1:] = 0.
            
            
            self.robot_glb_trans = nn.Embedding(
                num_embeddings=num_steps, embedding_dim=3
            ).cuda()
            torch.nn.init.zeros_(self.robot_glb_trans.weight)
            
            
            
            print(f"[Robot] Loading optimized init transformations from {robo_state_fn}")
            cur_optimized_init_actions_fn = robo_state_fn ## get robot state fn ##
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            print(f"optimized_init_actions_ckpt loaded with keys: {optimized_init_actions_ckpt.keys()}")
            try:
                if optimized_init_actions_ckpt['robot_init_states']['weight'].size(0) > self.robot_init_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_init_states']['weight'].data = optimized_init_actions_ckpt['robot_init_states']['weight'].data[:self.robot_init_states.weight.data.size(0)]
                self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            except:
                pass
            print("a1")
            if optimized_init_actions_ckpt['robot_glb_rotation']['weight'].size(0) > self.robot_glb_rotation.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data = optimized_init_actions_ckpt['robot_glb_rotation']['weight'].data[:self.robot_glb_rotation.weight.data.size(0)]
            self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            print("a2")
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                loaded_delta_states = True
                if optimized_init_actions_ckpt['robot_delta_states']['weight'].size(0) > self.robot_delta_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_delta_states']['weight'].data = optimized_init_actions_ckpt['robot_delta_states']['weight'].data[:self.robot_delta_states.weight.data.size(0)]
                self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
            print("a3") 
            if 'robot_states' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_states']['weight'].size(0) > self.robot_states.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_states']['weight'].data = optimized_init_actions_ckpt['robot_states']['weight'].data[:self.robot_states.weight.data.size(0)]
                self.robot_states.load_state_dict(optimized_init_actions_ckpt['robot_states'])
            print("a4")
            if 'robot_actions' in optimized_init_actions_ckpt:
                if optimized_init_actions_ckpt['robot_actions']['weight'].size(0) > self.robot_actions.weight.data.size(0):
                    optimized_init_actions_ckpt['robot_actions']['weight'].data = optimized_init_actions_ckpt['robot_actions']['weight'].data[:self.robot_actions.weight.data.size(0)]
                self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])
            print("a5")
            if optimized_init_actions_ckpt['robot_glb_trans']['weight'].data.size(0) > self.robot_glb_trans.weight.data.size(0):
                optimized_init_actions_ckpt['robot_glb_trans']['weight'].data = optimized_init_actions_ckpt['robot_glb_trans']['weight'].data[:self.robot_glb_trans.weight.data.size(0)]
            self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])
            print("a6")
            tot_robot_delta_states = []
            for i_fr in range(num_steps):
                if i_fr == 0:
                    cur_robot_delta_state = self.robot_states.weight.data[0]
                else:
                    cur_robot_delta_state = self.robot_states.weight.data[i_fr] - self.robot_states.weight.data[i_fr - 1]
                tot_robot_delta_states.append(cur_robot_delta_state)
            tot_robot_delta_states = torch.stack(tot_robot_delta_states, dim=0)
            self.robot_delta_states.weight.data.copy_(tot_robot_delta_states) # use delta states ##
            
            print(f"All weights loaded!")
            # self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
            print(f"b1")
            self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
            print(f"b2")
            self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
            print(f"b3")
            self.robot_actions_ori = self.robot_actions.weight.data.clone()
            print(f"b4")
            self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
            print(f"b5")
            self.robot_states_ori = self.robot_states.weight.data.clone()
            print(f"b6")
            # self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
            # self.robot_states_sv = self.robot_states.weight.data.clone()
            
            print(f"Ori weights saved!")
            
            
            
            
            print(f"loaded_delta_states: {loaded_delta_states}")
            # # if not loaded_delta_states: ## 
            # tot_robot_delta_states = []
            # for i_fr in range(num_steps):
            #     if i_fr == 0:
            #         cur_robot_delta_state = self.robot_states.weight.data[0]
            #     else:
            #         cur_robot_delta_state = self.robot_states.weight.data[i_fr] - self.robot_states.weight.data[i_fr - 1]
            #     tot_robot_delta_states.append(cur_robot_delta_state)
            # tot_robot_delta_states = torch.stack(tot_robot_delta_states, dim=0)
            # self.robot_delta_states.weight.data.copy_(tot_robot_delta_states) # use delta states ##
                
            ''' define and run through the redmax sim '''
            ### redmax robot actions ####
            self.redmax_robot_actions = nn.Embedding(
                num_embeddings=num_steps * nn_substeps, embedding_dim=60,
            ).cuda()
            torch.nn.init.zeros_(self.redmax_robot_actions.weight)
            # params_to_train += list(self.redmax_robot_actions.parameters())
            # params_to_train_kines = []
            # params_to_train_kines += list(self.redmax_robot_actions.parameters())
            # self.actions_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate_actions)
            
            self.redmax_robot_actions_ori = self.redmax_robot_actions.weight.data.clone()
            
            # if 'model.load_redmax_robot_actions_fn' in self.conf and len(self.conf['model.load_redmax_robot_actions_fn']) > 0:
                
            #     load_redmax_robot_actions_fn = self.conf['model.load_redmax_robot_actions_fn'] # 
                
            #     print(f"Loading redmax actions from {load_redmax_robot_actions_fn}")
                
            #     if load_redmax_robot_actions_fn.endswith(".pth"):
            # redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
            self.redmax_robot_actions.load_state_dict(optimized_init_actions_ckpt['redmax_robot_actions'])


            with torch.no_grad():
                nn_iters_opt_redmax_actions = 2
                for i_iter in tqdm(range(nn_iters_opt_redmax_actions)):
                    redmax_sim.reset(backward_flag = True) 
                    robo_intermediates_states = []
                    tot_redmax_actions = []
                    tot_visual_pts = []
                    df_dqs = []
                    tot_diff_qs = []
                    for cur_ts in range(self.nn_ts):
                        
                        if cur_ts == 0:
                            links_init_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                            # self.robot_agent.set_init_states_target_value(links_init_states)
                            # cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                            # cur_visual_pts = self.timestep_to_active_mesh_wo_glb[cur_ts]
                            ## redmax q init ## link initstates ## ## robot agent ## 
                            redmax_q_init = links_init_states[self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()
                            
                            # redmax_q_init = np.zeros((redmax_ndof_r,), dtype=np.float32)
                            # redmax_q_init = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
                            redmax_q_init = redmax_q_init[:redmax_ndof_r]
                            redmax_sim.set_q_init(redmax_q_init)
                            
                            # if i_iter % 100 == 0  or (i_iter == 1)  or (i_iter == nn_iters_opt_redmax_actions - 1):
                                
                                
                            #     ### redmax_sim_q ## ---> use that for the real act joint idx setting ##? 
                            #     redmax_sim_q_th = torch.from_numpy(redmax_q_init).float().cuda()
                            #     redmax_sim_q_th = torch.cat(
                            #         [torch.zeros((2,), dtype=torch.float32).float().cuda(), redmax_sim_q_th], dim=0 ### use the redmax sim q th
                            #     )
                            #     self.robot_agent.active_robot.set_delta_state_and_update_v2(redmax_sim_q_th, 0, use_real_act_joint=True) ## delta states ##
                            #     cur_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                                
                        
                        else:
                            
                            for i_sub_step in range(nn_substeps): ## # ## #
                                cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
                                redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
                                redmax_u = redmax_actions.detach().cpu().numpy()
                                redmax_u = redmax_u[: redmax_ndof_u]
                                
                                ## redmax_u ##
                                # redmax_u = redmax_robot_actions[cur_tot_substeps].detach().cpu().numpy()[ : redmax_ndof_u]
                                
                                redmax_sim.set_u(redmax_u) # sim
                                tot_redmax_actions.append(redmax_actions)
                                ''' set and set the virtual forces ''' ## redmax_sim
                                virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                                redmax_sim.set_forces(virtual_force) ### virtual forces ###

                                redmax_sim.forward(1, verbose = False)
                                redmax_sim_q = redmax_sim.get_q() # get q # get q ## 
                                if i_sub_step == nn_substeps - 1:
                                    robo_intermediates_states.append(redmax_sim_q)
                                    df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts]))
                                    cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts]) ** 2)
                                    tot_diff_qs.append(cur_diff_qs.item())
                                else:
                                    df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                            
                    tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) 
                    self.tot_redmax_actions = tot_redmax_actions
                    
                    diff_qs = sum(tot_diff_qs) / float(len(tot_diff_qs)) ## tot diff qs ##
                    cur_log_sv_str = 'iter:{:8>d} diff_qs = {} lr={}'.format(i_iter, diff_qs, self.actions_optimizer.param_groups[0]['lr'])
                    # get states and then use the obtianed states for actions optim?
                    print(cur_log_sv_str)
                    
            return loaded_delta_states
            
        
        # maxx robo pts ##
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203


        if 'model.mano_mult_const_after_cent' in self.conf:
            self.mano_mult_const_after_cent = self.conf['model.mano_mult_const_after_cent']
        
        
        if self.hand_type == "redmax_hand":
            self.maxx_robo_pts = 25.
            self.minn_robo_pts = -15.
            self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
            self.mult_const_after_cent = 0.5437551664260203 ## 
        else:
            self.minn_robo_pts = -0.1
            self.maxx_robo_pts = 0.2
            self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
            self.mult_const_after_cent = 0.437551664260203 ## should modify
        ## for grab ##
        self.mult_const_after_cent = self.mult_const_after_cent / 3. * 0.9507
        

        self.timestep_to_active_mesh = {}
        # self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        
        self.timestep_to_active_mesh_w_delta_states = {}
        
        timestep_to_tot_rot = {}
        timestep_to_tot_trans = {}
        
        self.timestep_to_active_mesh_wo_glb = {}
        
        
        with torch.no_grad():
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                ''' Get rotations, translations, and actions of the current robot '''
                cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                
                robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts) #
                
                # robo_links_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                # self.robot_agent.set_init_states_target_value(robo_links_states)
                # cur_robo_visual_pts = self.robot_agent.get_init_state_visual_pts()
                cur_robo_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                
                
                self.timestep_to_active_mesh_wo_glb[cur_ts] = cur_robo_visual_pts.detach().clone()
                
                
                if not self.use_scaled_robot_hand:
                    ### transform the visual pts ###
                    cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                    cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                    cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                    
                
                cur_rot = cur_robo_glb_rot
                cur_trans = cur_robo_glb_trans
                
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                
                cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0)
                
                self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts.detach().clone() # robo visual pts #

        self.iter_step = 0
        self.validate_mesh_robo()
        
        
        
        def get_initial_meshes():
            with torch.no_grad():
                for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                    cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                    cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                    cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts) #


                    cur_robo_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                    
                    self.timestep_to_active_mesh_wo_glb[cur_ts] = cur_robo_visual_pts.detach().clone()
                    
                    if not self.use_scaled_robot_hand:
                        ### transform the visual pts ###
                        cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                        cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                        
                    cur_rot = cur_robo_glb_rot
                    cur_trans = cur_robo_glb_trans
                    
                    timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                    timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                    
                    cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0)
                    
                    self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts.detach().clone() # robo visual pts #

            self.iter_step = 0
            self.validate_mesh_robo()
        
        def get_optimizer():
            
            ''' Set redmax robot actions '''
            params_to_train_kines = []
            
            
            if self.opt_robo_states:
                params_to_train_kines += list(self.robot_delta_states.parameters())
                params_to_train_kines += list(self.robot_actions.parameters())
            if self.opt_robo_glb_trans:
                params_to_train_kines += list(self.robot_glb_trans.parameters())
            if self.opt_robo_glb_rot:
                params_to_train_kines += list(self.robot_glb_rotation.parameters())
            
            
            
            if self.use_LBFGS:
                self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
            else:
                self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            
            if self.optimize_rules:
                params_to_train_kines = []
                print("optimizing rules!")
                params_to_train_kines += list(self.other_bending_network.parameters())
                if self.use_LBFGS:
                    self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
                else:
                    self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
                # self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=1e-2)
            
            
            params_to_train_redmax_acts = []
            params_to_train_redmax_acts += list(self.redmax_robot_actions.parameters())
            self.actions_optimizer = torch.optim.Adam(params_to_train_redmax_acts, lr=self.learning_rate_actions)
            
            
            self.iter_step = 0
        
        
        ''' Set redmax robot actions '''
        params_to_train_kines = []
        
        
        if self.opt_robo_states:
            params_to_train_kines += list(self.robot_delta_states.parameters())
            params_to_train_kines += list(self.robot_actions.parameters())
        if self.opt_robo_glb_trans:
            params_to_train_kines += list(self.robot_glb_trans.parameters())
        if self.opt_robo_glb_rot:
            params_to_train_kines += list(self.robot_glb_rotation.parameters())
        
        
        
        if self.use_LBFGS:
            self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
        else:
            self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        
        if self.optimize_rules:
            params_to_train_kines = []
            print("optimizing rules!")
            params_to_train_kines += list(self.other_bending_network.parameters())
            if self.use_LBFGS:
                self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
            else:
                self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            # self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=1e-2)
        

        self.iter_step = 0

        self.minn_tracking_loss = 1e27
        
        base_exp_dir_root = self.base_exp_dir
        


        exp_tags = ["thres0d1", "robo", "thres0d05", "robo", "thres0d03", "robo", "thres0d025", "robo", "thres0d02", "robo", "thres0d015", "robo", "thres0d01", "robo", "thres0d00", "robo"]
        distance_threshold = [0.1, 0.1, 0.05, 0.05, 0.03, 0.03, 0.025, 0.025, 0.02, 0.02, 0.015, 0.015, 0.01, 0.01, 0.00, 0.00]
        spring_ks_contact = [4e4, 4e4, 8e4, 8e4, 1e5, 1e5, 2e5, 2e5, 3e5, 3e5, 3.5e5, 3.5e5, 4e5, 4e5, 4e6, 4e6]
        friction_spring_ks_contact = [1e5, 1e5, 2e5, 2e5, 4e5, 4e5, 5e5, 5e5, 6e5, 6e5, 8e5, 8e5, 1e6, 1e6, 1e7, 1e7]
        # exp_nn_iters = [1500, 100, 1500, 100, 1500, 100, 1500, 100, 1500, 100]
        # exp_nn_iters = [1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, ]
        exp_nn_iters = [4000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, 1000, 100, ]
        optimizing_rules = [True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False,]
        
        print(f"exp_tags: {len(exp_tags)}, distance_threshold: {len(distance_threshold)}, spring_ks_contact: {len(spring_ks_contact)}, friction_spring_ks_contact: {len(friction_spring_ks_contact)}, exp_nn_iters: {len(exp_nn_iters)}, optimizing_rules: {len(optimizing_rules)}")



        full_exp_tag = "" 

        for i_exp, (cur_exp_tag, cur_dist_thres, cur_nn_iters) in enumerate(zip(exp_tags, distance_threshold, exp_nn_iters)):
            print(f"i_exp: {i_exp}, cur_exp_tag: {cur_exp_tag}, dist_thres: {cur_dist_thres}, cur_nn_iters: {cur_nn_iters}")
            full_exp_tag = full_exp_tag + cur_exp_tag + "_"
            
            self.base_exp_dir = os.path.join(base_exp_dir_root, full_exp_tag)
            os.makedirs(self.base_exp_dir, exist_ok=True)
            print(f"Current base_exp_dir: {self.base_exp_dir}")
            
            self.other_bending_network.minn_dist_threshold_robot_to_obj = cur_dist_thres
            self.minn_dist_threshold_robot_to_obj = cur_dist_thres
            
            # self.other_bending_network.minn_dist_threshold_robot_to_obj = cur_dist_thres
            self.other_bending_network.penetration_proj_k_to_robot = spring_ks_contact[i_exp]
            ## i_exp ##
            self.other_bending_network.penetration_proj_k_to_robot_friction = friction_spring_ks_contact[i_exp]
            
            # self.minn_dist_threshold_robot_to_obj = cur_dist_thres
            
            
            self.minn_tracking_loss = 1e27
            self.best_iter = 0
            
            # if i_exp <= 1:
            #     continue
            # if i_exp == 2: ## arti dyn ##
            #     best_ckpt_path = "/data2/datasets/xueyi/neus/exp/hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_train_retargeted_shadow_hand_states_optrobot__seq_102_optrules_frommano_thre00_robooptrules_optrobo_redmaxmultistages_/thres0d1_robo_thres0d05_/checkpoints/ckpt_best.pth"
            #     load_optimized_init_robo_transformations(best_ckpt_path)
            #     self.load_checkpoint_via_fn(best_ckpt_path)
            #     get_initial_meshes()
                
            #     nex_optimizing_rules = optimizing_rules[i_exp + 1]
            #     self.optimize_rules = nex_optimizing_rules
            #     get_optimizer()
                
            #     continue
            
            
            for i_iter in tqdm(range(cur_nn_iters)):
                
                # for i_iter in tqdm(range(100000)):
                tot_losses = []
                tot_tracking_loss = []
                
                
                self.timestep_to_posed_active_mesh = {}
                self.timestep_to_posed_mano_active_mesh = {}
                self.timestep_to_mano_active_mesh = {}
                self.timestep_to_corr_mano_pts = {}
                timestep_to_tot_rot = {}
                timestep_to_tot_trans = {}

                self.timestep_to_raw_active_meshes = {}
                self.timestep_to_penetration_points = {}
                self.timestep_to_penetration_points_forces = {}
                self.joint_name_to_penetration_forces_intermediates = {}
                
                
                self.ts_to_contact_passive_normals = {}
                self.ts_to_passive_normals = {}
                self.ts_to_passive_pts = {}
                self.ts_to_contact_force_d = {}
                self.ts_to_penalty_frictions = {}
                self.ts_to_penalty_disp_pts = {}
                self.ts_to_redmax_states = {}
                self.ts_to_mano_rhand_meshes = {}
                
                contact_pairs_set = None
                self.contact_pairs_sets = {}
                
                tot_penetration_depth = []
                
                robo_actions_diff_loss = []
                mano_tracking_loss = []
                
                ########## Reset redmax_sim and related quantities ##########
                redmax_sim.reset(backward_flag = True) 
                robo_intermediates_states = []
                tot_redmax_actions = []
                tot_visual_pts = []
                df_dqs = []
                tot_diff_qs = []
                tot_redmax_qs = []
                tot_redmax_qs_th = []    
                ########## Reset redmax_sim and related quantities ##########
                
                    
                
                penetration_forces = None
                sampled_visual_pts_link_idxes = None
                link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                
                
                # for cur_ts in range(self.nn_ts):
                for cur_ts in range(self.nn_ts * self.mano_nn_substeps):

                    self.free_def_bending_weight = 0.0

                    cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                    cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                    cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    ### use the redmax robot ###
                    if cur_ts == 0:
                        links_init_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                        redmax_sim_q = links_init_states[self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()
                        redmax_sim_q = redmax_sim_q[:redmax_ndof_r]
                        redmax_sim.set_q_init(redmax_sim_q)
                    else:
                        for i_sub_step in range(nn_substeps):
                            cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
                            redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
                            redmax_u = redmax_actions.detach().cpu().numpy()
                            redmax_u = redmax_u[: redmax_ndof_u]
                            
                            redmax_sim.set_u(redmax_u) # set joint actions ###
                            tot_redmax_actions.append(redmax_actions)
                            ''' set and set the virtual forces ''' ## redmax_sim
                            ## virtual forces --- set the virtue forces ##
                            
                            # virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                            # redmax_sim.set_forces(virtual_force) ### virtual forces ###
                            
                            virtual_force = link_maximal_contact_forces.detach().cpu().numpy() # * 1e5
                            virtual_force = np.reshape(virtual_force, (virtual_force.shape[0] * virtual_force.shape[1]))
                            
                            # # virtual forces # # ## at each time set the link virtual forces here ###
                            summ_virtual_force = np.sum(virtual_force)
                            # if i_sub_step == nn_substeps - 1:
                            #     print(f"cur_ts: {cur_ts}, summ_virtual_force: {summ_virtual_force}")
                            redmax_sim.set_forces(virtual_force) ### virtual forces ###

                            redmax_sim.forward(1, verbose = False)
                            redmax_sim_q = redmax_sim.get_q() # get q # get q ## 


                    tot_redmax_qs.append(torch.from_numpy(redmax_sim_q).float().cuda() ) ## ##
                    
                    ''' Update saved robot joint states ''' 
                    self.robot_states.weight.data[cur_ts, self.robot_agent.active_robot.act_joint_idxes] = tot_redmax_qs[cur_ts].clone()
                    if cur_ts == 0:
                        self.robot_delta_states.weight.data[cur_ts, self.robot_agent.active_robot.act_joint_idxes] = tot_redmax_qs[cur_ts].clone()
                    else:
                        self.robot_delta_states.weight.data[cur_ts, self.robot_agent.active_robot.act_joint_idxes] = (tot_redmax_qs[cur_ts] - tot_redmax_qs[cur_ts - 1]).clone()
                    ''' Update saved robot joint states ''' 
                    
                    
                    
                    ''' Get redmax q_th and forward the robot agent '''
                    redmax_sim_q_th = torch.from_numpy(redmax_sim_q).float().cuda() ### get the redmax_sim_q ## 
                    redmax_sim_q_th.requires_grad = True
                    redmax_sim_q_th.requires_grad_()
                    redmax_sim_q_th.requires_grad_ = True
                    
                    tot_redmax_qs_th.append(redmax_sim_q_th.detach().clone()) ## redmax_sim_q_th ## 
                    tot_redmax_qs_th[cur_ts].requires_grad = True
                    tot_redmax_qs_th[cur_ts].requires_grad_()
                    tot_redmax_qs_th[cur_ts].requires_grad_ = True
                    ## redmax_sim_q_th ## 
                    self.robot_agent.active_robot.set_delta_state_and_update_v2(
                        # redmax_sim_q_th, 
                        torch.cat( ## get the redmax qs th ### 
                            [torch.zeros((2,), dtype=torch.float32).float().cuda(), tot_redmax_qs_th[cur_ts]], dim=0
                        ),
                    0, use_real_act_joint=True) ## delta states ##
                    
                    cur_robo_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                    ''' Get redmax q_th and forward the robot agent '''
                    
                        
                    
                    # timestep_to_active_mesh_wo_glb #  # wo_glb #
                    saved_robo_visual_pts = self.timestep_to_active_mesh_wo_glb[cur_ts]
                    ### calculate distance ###
                    dist_cur_robo_visual_pts_w_saved_robot_pts = torch.sum(
                        (cur_robo_visual_pts - saved_robo_visual_pts) ** 2, dim=-1
                    )
                    dist_cur_robo_visual_pts_w_saved_robot_pts = dist_cur_robo_visual_pts_w_saved_robot_pts.mean()
                    

                    if not self.use_scaled_robot_hand:
                        ### transform the visual pts ###
                        cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                        cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                    
                    
                    cur_rot = cur_robo_glb_rot
                    cur_trans = cur_robo_glb_trans
                    
                    timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                    timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                    
                    ## timestep ## vi
                    
                    ## 
                    ### transform by the glboal transformation and the translation ###
                    cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0)
                    
                    

                    self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts # .clone() # robo visual pts #
                    
                    self.ts_to_mano_rhand_meshes[cur_ts] = self.rhand_verts[cur_ts]
           
                    self.free_def_bending_weight = 0.0

                        
                    if contact_pairs_set is None:
                        self.contact_pairs_set = None
                    else:
                        self.contact_pairs_set = contact_pairs_set.copy()
                    
                    
                    if self.optim_sim_model_params_from_mano:
                        # print(f"Using mano mesh models for optimization")
                        act_mesh_pts_dict_to_model = self.ts_to_mano_rhand_meshes
                    else:
                        act_mesh_pts_dict_to_model = self.timestep_to_active_mesh

                    
                    contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=act_mesh_pts_dict_to_model, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)
                    
                    

    
                    if self.train_with_forces_to_active and (not self.use_mano_inputs): 
                        # penetration_forces #
                        if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                            net_penetrating_forces = self.other_bending_network.penetrating_forces
                            net_penetrating_points = self.other_bending_network.penetrating_points

                            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                            self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                            self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                            
                            
                            sampled_visual_pts_link_idxes = robo_visual_pts_link_idxes[self.other_bending_network.penetrating_indicator]
                            
                            net_penetrating_forces = torch.matmul(
                                cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                            ).transpose(1, 0)
                            net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                            net_penetrating_forces = net_penetrating_forces / 2
                            net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                            
                            net_penetrating_points = torch.matmul(
                                cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                            ).transpose(1, 0)
                            net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                            net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                            net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                            
                            penetration_forces = {
                                'penetration_forces': net_penetrating_forces,
                                'penetration_forces_points': net_penetrating_points
                            }
                            
                            link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                            # self.robot_agent.set_maximal_contact_forces(link_maximal_contact_forces)
                            self.robot_agent.active_robot.set_penetration_forces(penetration_forces, sampled_visual_pts_link_idxes, link_maximal_contact_forces)
                            
                        else:
                            penetration_forces = None
                            sampled_visual_pts_link_idxes = None
                            link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                            
                            
                    if contact_pairs_set is not None:
                        self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                    
                    # contact force d 
                    self.ts_to_contact_passive_normals[cur_ts] = self.other_bending_network.tot_contact_passive_normals.detach().cpu().numpy()
                    self.ts_to_passive_pts[cur_ts] = self.other_bending_network.cur_passive_obj_verts.detach().cpu().numpy()
                    self.ts_to_passive_normals[cur_ts] = self.other_bending_network.cur_passive_obj_ns.detach().cpu().numpy()
                    self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                    self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                    if self.other_bending_network.penalty_based_friction_forces is not None:
                        self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                    
                    # # get the penetration depth of the bending network #
                    ## total penetration depth ##
                    tot_penetration_depth.append(self.other_bending_network.penetrating_depth_penalty.detach().item())
                    
                    tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_ts + 1)
                    tot_tracking_loss.append(tracking_loss.detach().cpu().item())


                    penetraton_penalty = self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                    
                    
                    # # print(f"after calculating losses....") # #
                    
                    # robot_states_ori, robot_glb_trans_ori, robot_glb_rotation_ori ### robot glb rotation ori ###
                    # # # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
                    robot_rotation_diff = torch.sum(
                        (self.robot_glb_rotation_ori - self.robot_glb_rotation.weight) ** 2, dim=-1
                    )
                    robot_rotation_diff = robot_rotation_diff.mean()
                    
                    robot_trans_diff = torch.sum(
                        (self.robot_glb_trans_ori - self.robot_glb_trans.weight) ** 2, dim=-1
                    )
                    robot_trans_diff = robot_trans_diff.mean()
                    
                    robot_states_diff = torch.sum(
                        (self.robot_states_ori - self.robot_states.weight) ** 2, dim=-1
                    )
                    robot_states_diff = robot_states_diff.mean()
                    
                    robot_delta_states_diff = torch.sum(
                        (self.robot_delta_states_ori - self.robot_delta_states.weight) ** 2, dim=-1
                    )
                    robot_delta_states_diff = robot_delta_states_diff.mean()
                    
                    
                    robot_redmax_acts_diff = torch.sum(
                        (self.redmax_robot_actions_ori - self.redmax_robot_actions.weight) ** 2, dim=-1
                    )
                    robot_redmax_acts_diff = robot_redmax_acts_diff.mean()
                    
                    transformation_diff_loss = robot_rotation_diff + robot_trans_diff + robot_states_diff + robot_delta_states_diff + robot_redmax_acts_diff
                    
                    
                    
                    # kinematics_proj_loss = tracking_loss * self.loss_scale_coef  + transformation_diff_loss * 1.0 + penetraton_penalty
                    ###### get the tracking loss and transformation diff loss and regularization loss #######
                    kinematics_proj_loss = tracking_loss * self.loss_scale_coef  + transformation_diff_loss * self.diff_reg_coef + penetraton_penalty
                    
                    
                    
                    if self.drive_robot == 'actions': # proj w saved robot pts ## 
                        kinematics_proj_loss = kinematics_proj_loss + dist_cur_robo_visual_pts_w_saved_robot_pts
                        # kinematics_proj_loss =  dist_cur_robo_visual_pts_w_saved_robot_pts
        
                    
                    loss = kinematics_proj_loss
                    
                    
                    diff_hand_tracking = dist_cur_robo_visual_pts_w_saved_robot_pts
                    mano_tracking_loss.append(diff_hand_tracking.detach().cpu().item())
                    
                    
                    tot_losses.append(kinematics_proj_loss)
                    
                    robot_states_actions_diff_loss = transformation_diff_loss.item()
                    robo_actions_diff_loss.append(robot_states_actions_diff_loss)

                    self.iter_step += 1
                    self.writer.add_scalar('Loss/loss', loss, self.iter_step)
                    
                    
                    torch.cuda.empty_cache()
                    
                tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
                # saved_best = False
                # best_iter, minn_tracking_loss, best_ckpt_path
                if tot_tracking_loss < self.minn_tracking_loss:
                    print(f"Saving best checkpoint with tracking_loss: {tot_tracking_loss} at iter_step: {self.iter_step}")
                    self.best_iter = i_iter ## get i_iter here ##
                    self.minn_tracking_loss = tot_tracking_loss
                    best_ckpt_path = self.save_checkpoint(tag="best")
                    
                    # self.validate_mesh_robo()
                    # ### test for contact infos ###
                    # self.validate_contact_info_robo()
                    # saved_best = True
                
                ########## Update v2 ##########
                loss = sum(tot_losses)
                self.kines_optimizer.zero_grad()
                try:
                    loss.backward(retain_graph=True)
                    self.kines_optimizer.step()
                except:
                    pass
                self.update_learning_rate() 
                ########## Update v2 ##########
                
                tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) 
            
                self.tot_redmax_actions = tot_redmax_actions
            
                tot_df_dqs = []
                for cur_ts in range(1, self.nn_ts):
                    for i_sub_step in range(nn_substeps):
                        if i_sub_step < nn_substeps - 1:
                            tot_df_dqs.append(np.zeros((tot_redmax_qs_th[0].size(0),), dtype=np.float32)) ## get df_dqs ## 
                        else:
                            if tot_redmax_qs_th[cur_ts].grad is  None: ### no gradientts over the cur_ts's qs ###
                                cur_df_dq = torch.zeros_like(tot_redmax_qs_th[cur_ts]).detach().cpu().numpy()
                            else:
                                cur_df_dq = tot_redmax_qs_th[cur_ts].grad.data.detach().cpu().numpy()
                            tot_df_dqs.append(cur_df_dq) ## get the current df dqs ## 
                df_dq = np.stack(tot_df_dqs, axis=0)
                
                # df_dq_summ = 
                
                nn_forward_ts = df_dq.shape[0]
                df_dq = np.reshape(df_dq, (-1,))
                df_du = np.zeros((nn_forward_ts * redmax_ndof_u,))
                
                redmax_sim.backward_info.set_flags(False, False, False, True) 
                # nn_forward_ts = len(tot_grad_qs)
                # tot_grad_qs = np.concatenate(tot_grad_qs, axis=0)
                redmax_sim.backward_info.df_dq = df_dq
                redmax_sim.backward_info.df_du = df_du
                # print(f"df_du: {df_du.shape}, df_dq: {df_dq.shape}")
                
                redmax_sim.backward()
                
                
                
                redmax_sim_robot_action_grad = np.copy(redmax_sim.backward_results.df_du)
                redmax_sim_robot_action_grad_th = torch.from_numpy(redmax_sim_robot_action_grad).float().cuda()
                redmax_sim_robot_action_grad_th = redmax_sim_robot_action_grad_th.contiguous().view(nn_forward_ts, -1).contiguous()
                
                ## nn_forward_steps x nn_us_dim # 
                
                ## redmax_sim as the redmax_sim ## ## surrogate loss #### surrogte lsoss ## 
                surrogate_loss = torch.sum(
                    tot_redmax_actions[:, :redmax_ndof_u] * redmax_sim_robot_action_grad_th, dim=-1
                )
                surrogate_loss = surrogate_loss.mean()
                
                self.actions_optimizer.zero_grad()
                surrogate_loss.backward()
                
                
                # avg_tot_diff_qs = sum(tot_diff_qs) / float(len(tot_diff_qs))
                grad_redmax_robot_actions = self.redmax_robot_actions.weight.grad.data
                summ_grad_redmax_robot_actions = torch.sum(grad_redmax_robot_actions).item()
                print('iter:{:8>d} redmax_robot_actions_grad_sum = {}'.format(self.iter_step, summ_grad_redmax_robot_actions))
                
                if not self.optimize_rules:
                    self.actions_optimizer.step()

            
                
                
                ''' Get nn_forward_ts and backward through the actions for updating '''
                ## save ckpt ##
                if i_iter == 0 or (i_iter % self.ckpt_sv_freq == 0):
                    self.save_checkpoint() 
                
                tot_losses = sum(tot_losses) / float(len(tot_losses))
                
                tot_penetration_depth = sum(tot_penetration_depth) / float(len(tot_penetration_depth))
                robo_actions_diff_loss = sum(robo_actions_diff_loss) / float(len(robo_actions_diff_loss))
                mano_tracking_loss = sum(mano_tracking_loss) / float(len(mano_tracking_loss))
                
                
                
                if i_iter % self.report_freq == 0:
                    logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                    
                    cur_log_sv_str = 'iter:{:8>d} loss = {} tracking_loss = {} mano_tracking_loss = {} penetration_depth = {} actions_diff_loss = {} lr={}'.format(self.iter_step, tot_losses, tot_tracking_loss, mano_tracking_loss, tot_penetration_depth, robo_actions_diff_loss, self.optimizer.param_groups[0]['lr'])
                    
                    print(cur_log_sv_str)
                    ''' Dump to the file '''
                    with open(logs_sv_fn, 'a') as log_file: 
                        log_file.write(cur_log_sv_str + '\n')
                    print(f"spring_ks_values: { self.other_bending_network.optimizable_spring_ks.weight.data.detach().cpu().numpy() }")
                
                if (i_iter % self.val_mesh_freq == 0):
                    self.validate_mesh_robo()
                    ### test for contact infos ###
                    self.validate_contact_info_robo()
                    
                
                torch.cuda.empty_cache()
                
                if (i_iter >= 100) and self.minn_tracking_loss < 4e-5:
                    break
    
            # best_iter, minn_tracking_loss, best_ckpt_path #
            print(f"Beest iter: {self.best_iter}, minn_tracking_loss: {self.minn_tracking_loss}, best_ckpt_path: {best_ckpt_path}")
            ## best_ckpt_path ##
            
            if i_exp < len(exp_tags) - 1:
                load_optimized_init_robo_transformations(best_ckpt_path)
                self.load_checkpoint_via_fn(best_ckpt_path)
                get_initial_meshes()
                
                nex_optimizing_rules = optimizing_rules[i_exp + 1]
                self.optimize_rules = nex_optimizing_rules
                get_optimizer()
                
         
            
    
    
    ''' GRAB clips; Shadow hand; Redmax action model ''' ## optimize those params ##
    def train_redmax_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab(self, ):
        #### redmax robot actions ####
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate()
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load #
        ''' Load the robot hand '''
        model_path = self.conf['model.sim_model_path'] # 
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        if 'scaled' in model_path:
            self.use_scaled_robot_hand = True
        else:
            self.use_scaled_robot_hand = False
        self.hand_type = "redmax_hand"
        if model_path.endswith(".xml"):
            self.hand_type = "redmax_hand" ## robot agent ##
            robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        else:
            self.hand_type = "shadow_hand" ## shadow hand ## robot agent ## robot hand ## two hands? ## three hands ## 
            ## the shadow hand -> from right to left -> how to create the left hand #
            # reflect the hand via hand palm plane -> mirroring # # mirroring the hand using the hand palm plane ##
            robot_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path, args=None)
        self.robot_agent = robot_agent
        robo_init_verts = self.robot_agent.robot_pts
        robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        if os.path.exists(robo_sampled_verts_idxes_fn):
            sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
            sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        else:
            n_sampling = 1000
            pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
            sampled_verts_idxes = pts_fps_idx
            np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        self.robo_hand_faces = self.robot_agent.robot_faces
        
        
        ## sampled verts idxes ## 
        # self.sampled_verts_idxes = sampled_verts_idxes ##
        ''' Load the robot hand '''
        # sim_model_path = "/home/xueyi/diffsim/NeuS/rsc/shadow_hand_description/shadowhand_new_scaled_nroot_new.urdf"
        # real_nroot_shadow_mesh_fn = "/home/xueyi/diffsim/NeuS/rsc/shadow_hand_description/shadowhand_new_scaled_nroot.urdf"
        real_nroot_shadow_mesh_fn = "/home/xueyi/diffsim/NeuS/rsc/shadow_hand_description/shadowhand_new_scaled_nroot_new.urdf"
        real_nroot_robot_agent = dyn_model_act_mano.RobotAgent(xml_fn=real_nroot_shadow_mesh_fn, args=None)
        self.real_nroot_robot_agent = real_nroot_robot_agent
        
        
        ## sampled verts idxes ## # shadow mesh ## # shadow mesh ## ## load the robot hand ##
        self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        
        ''' Load robot hand in DiffHand simulator '''
        xml_shadow_hand_fn = "/home/xueyi/diffsim/NeuS/rsc/shadow_hand_description/shadowhand_new_scaled_nroot.xml"
        redmax_sim = redmax.Simulation(xml_shadow_hand_fn) ## diffhand simulator ##
        redmax_sim.reset(backward_flag = True) #
        # # ### redmax_ndof_u, redmax_ndof_r ###
        redmax_ndof_u = redmax_sim.ndof_u
        redmax_ndof_r = redmax_sim.ndof_r
        redmax_ndof_m = redmax_sim.ndof_m
        
        print(f"[REDMAX] ndof_u: {redmax_ndof_u}, ndof_r: {redmax_ndof_r}, ndof_m: {redmax_ndof_m}")
        
        
        nn_substeps = 10
        
        
        ''' Load the mano hand '''
        # model_path_mano = self.conf['model.mano_sim_model_path']
        # # mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano) # robot #
        # mano_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path_mano) ## model path mano ## #  ## modeljpath 
        # self.mano_agent = mano_agent
        # # ''' Load the mano hand '''
        
        # self.robo_hand_faces = self.mano_agent.robot_faces
        
        # if self.use_mano_hand_for_test:
        #     self.robo_hand_faces = self.hand_faces
        
        
        nn_substeps = 10
        
        # mano_nn_substeps = 1
        # # mano_nn_substeps = 10 # 
        # self.mano_nn_substeps = mano_nn_substeps
        
        
        ''' Expnad the current visual points ''' 
        # expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        # self.expanded_visual_pts_nn = expanded_visual_pts.size(0)
        # expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        # expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        # np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        # # ''' Expnad the current visual points '''  #  # differentiate through the simulator? # # 
        
        
        params_to_train = [] # params to train #
        # ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        
        # oursjsim #
        # robot actions # # 
        self.robot_actions = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60, #  22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_actions.weight)
        params_to_train += list(self.robot_actions.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_delta_states.weight)
        params_to_train += list(self.robot_delta_states.parameters())
        
        ## robot_delta_states, robot_states # #
        self.robot_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        params_to_train += list(self.robot_states.parameters())
        
        self.robot_delta_states = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.robot_states.weight)
        
        self.robot_init_states = nn.Embedding(
            num_embeddings=1, embedding_dim=22,
        ).cuda()
        torch.nn.init.zeros_(self.robot_init_states.weight)
        params_to_train += list(self.robot_init_states.parameters())
        
        self.robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=4
        ).cuda()
        self.robot_glb_rotation.weight.data[:, 0] = 1.
        self.robot_glb_rotation.weight.data[:, 1:] = 0.
        
        
        self.robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_glb_trans.weight)
        
        ##  ##
        # self.robot_actuator_friction_forces = nn.Embedding(
        #     num_embeddings=778 * 60, embedding_dim=3
        # ).cuda()
        # torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)

        ## overfitting to a single sequence ##
        self.robot_actuator_friction_forces = nn.Embedding(
            num_embeddings=365428 * 60, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.robot_actuator_friction_forces.weight)
        
        
        
        ### load optimized init transformations for robot actions ###
        if 'model.load_optimized_init_transformations' in self.conf and len(self.conf['model.load_optimized_init_transformations']) > 0: 
            print(f"[Robot] Loading optimized init transformations from {self.conf['model.load_optimized_init_transformations']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_transformations']
            # cur_optimized_init_actions = # optimized init states # ## robot init states ##
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            try:
                self.robot_init_states.load_state_dict(optimized_init_actions_ckpt['robot_init_states'])
            except:
                pass
            self.robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['robot_glb_rotation'])
            if 'robot_delta_states' in optimized_init_actions_ckpt:
                try:
                    self.robot_delta_states.load_state_dict(optimized_init_actions_ckpt['robot_delta_states'])
                except:
                    pass
            if 'robot_states' in optimized_init_actions_ckpt:
                self.robot_states.load_state_dict(optimized_init_actions_ckpt['robot_states'])
            # if 'robot_delta_states'  ## robot delta states ##
            if 'robot_actions' in optimized_init_actions_ckpt:
                self.robot_actions.load_state_dict(optimized_init_actions_ckpt['robot_actions'])

            self.robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['robot_glb_trans'])

        
        ''' ''' 
        ## robot_delta_states, robot_states # #
        tot_robot_delta_states = []
        for i_fr in range(num_steps):
            if i_fr == 0:
                cur_robot_delta_state = self.robot_states.weight.data[0]
            else:
                cur_robot_delta_state = self.robot_states.weight.data[i_fr] - self.robot_states.weight.data[i_fr - 1] ### nn_delta states
            tot_robot_delta_states.append(cur_robot_delta_state)
        tot_robot_delta_states = torch.stack(tot_robot_delta_states, dim=0)
        self.robot_delta_states.weight.data.copy_(tot_robot_delta_states) # use delta states ##
        
        
        
        
        ''' Only set the robot friction forces optimizable '''
        # if not self.optimize_rules:
        #     params_to_train += list(self.robot_actions.parameters())
        #     params_to_train += list(self.robot_delta_states.parameters())
        #     params_to_train += list(self.robot_init_states.parameters())
        #     params_to_train += list(self.robot_glb_rotation.parameters())
        #     params_to_train += list(self.robot_glb_trans.parameters())
        #     params_to_train += list(self.robot_states.parameters())
        
        # ''' Add params for expanded points '''
        # ## prams to train ## ## params to train ## ##  # 
        # # params_to_train += list(self.mano_expanded_actuator_delta_offset_nex.parameters())
        # # params_to_train += list(self.mano_expanded_actuator_friction_forces.parameters())
        # # params_to_train += list(self.robot_actuator_friction_forces.parameters())
        
        # if self.optimize_rules:
        #     params_to_train += list(self.other_bending_network.parameters())
        
        ## scaling constants ##
        
        ''' Scaling constants '''
        self.mano_mult_const_after_cent = 3.
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        print(f"mano_to_dyn_corr_pts_idxes: {self.mano_to_dyn_corr_pts_idxes.size()}")
        
        
        # maxx robo pts ##
        self.maxx_robo_pts = 25.
        self.minn_robo_pts = -15.
        self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        self.mult_const_after_cent = 0.5437551664260203
        
        self.mano_mult_const_after_cent = 3. # mult 
        
        if 'model.mano_mult_const_after_cent' in self.conf:
            self.mano_mult_const_after_cent = self.conf['model.mano_mult_const_after_cent']
            
        
        # ## redmax hand ##
        # if self.hand_type == "redmax_hand":
        #     self.maxx_robo_pts = 25.
        #     self.minn_robo_pts = -15.
        #     self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        #     self.mult_const_after_cent = 0.5437551664260203 ## 
        # else:
        #     self.minn_robo_pts = -0.1
        #     self.maxx_robo_pts = 0.2
        #     self.extent_robo_pts = self.maxx_robo_pts - self.minn_robo_pts
        #     self.mult_const_after_cent = 0.437551664260203 ## should modify
        # ## for grab ##
        # self.mult_const_after_cent = self.mult_const_after_cent / 3. * 0.9507
        
        
        
        # robot_states_ori, robot_glb_trans_ori, robot_glb_rotation_ori # 
        # robot_glb_rotation, robot_glb_trans #
        self.robot_init_states_ori = self.robot_init_states.weight.data.clone()
        self.robot_glb_rotation_ori = self.robot_glb_rotation.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        self.robot_actions_ori = self.robot_actions.weight.data.clone()
        self.robot_glb_trans_ori = self.robot_glb_trans.weight.data.clone()
        self.robot_states_ori = self.robot_states.weight.data.clone()
        self.robot_delta_states_ori = self.robot_delta_states.weight.data.clone()
        
        self.robot_states_sv = self.robot_states.weight.data.clone()
        
        self.nn_ts = self.nn_timesteps - 1

        ''' Set actions for the redmax simulation and add parameters to params-to-train '''
        # # robot actions #
        # self.redmax_robot_actions = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=22,
        # ).cuda()
        # torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        
        # self.redmax_robot_states = nn.Embedding(
        #     num_embeddings=num_steps, embedding_dim=22,
        # ).cuda()
        
        # params_to_train = []
        # params_to_train += list(self.redmax_robot_actions.parameters())
        
        # timestep_to_active_mesh_wo_glb_from_states, timestep_to_active_mesh_from_states #
        self.timestep_to_active_mesh_wo_glb_from_states = {}
        self.timestep_to_active_mesh_from_states = {}
        # states -> get states -> only update the acitons #
        with torch.no_grad():
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                ''' Get rotations, translations, and actions of the current robot '''
                cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                
                self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts)
                # robo_links_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                # self.robot_agent.set_init_states_target_value(robo_links_states)
                cur_robo_visual_pts = self.robot_agent.get_init_state_visual_pts()
                
                self.timestep_to_active_mesh_wo_glb_from_states[cur_ts] = cur_robo_visual_pts.detach().clone()
                
                
                if not self.use_scaled_robot_hand:
                    ### transform the visual pts ###
                    cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                    cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                    cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                    
                
                cur_rot = cur_robo_glb_rot
                cur_trans = cur_robo_glb_trans
                
                
                # timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                # timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                ### transform by the glboal transformation and the translation ###
                cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0) 
                self.timestep_to_active_mesh_from_states[cur_ts] = cur_robo_visual_pts.clone() # robo visual pts #
                
        ## 
        self.timestep_to_active_mesh = self.timestep_to_active_mesh_from_states.copy()
        
        self.iter_step = 0
        self.validate_mesh_robo()

        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)

        
        ''' Set the redmax robot actions and the action optimizer ''' 
        nn_iters_opt_redmax_actions = 2000000
        
        nn_substeps = 10
        
        # nn_substeps = 200
        
        ## redmax
        self.redmax_robot_actions = nn.Embedding(
            num_embeddings=num_steps * nn_substeps, embedding_dim=60, #  22,
        ).cuda()
        torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        # params_to_train += list(self.redmax_robot_actions.parameters())
        params_to_train_actions = []
        params_to_train_actions += list(self.redmax_robot_actions.parameters())
        
        ''' TODO: load redmax robot actions here ''' 
        if 'model.load_redmax_robot_actions_fn' in self.conf and len(self.conf['model.load_redmax_robot_actions_fn']) > 0:
            
            load_redmax_robot_actions_fn = self.conf['model.load_redmax_robot_actions_fn'] # 
            print(f"Loading redmax actions from {load_redmax_robot_actions_fn}")
            redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
            self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])
            
            self.ref_from_prev_opt_actions = True
        else:
            self.ref_from_prev_opt_actions = False
        
        ###### get the original redmax robot actions ####### ### weight.data ###
        ori_redmax_robot_actions = self.redmax_robot_actions.weight.data.detach().clone() #### ## ##
        
        # self.actions_optimizer = torch.optim.Adam(params_to_train_actions, lr=self.learning_rate) #
        # self.actions_optimizer = torch.optim.Adam(params_to_train_actions, 0.000001)
        # self.actions_optimizer = torch.optim.Adam(params_to_train_actions, 0.000005)
        # self.actions_optimizer = torch.optim.Adam(params_to_train_actions, 0.00001)
        self.actions_optimizer = torch.optim.Adam(params_to_train_actions, 0.000001)
        ''' Set the redmax robot actions and the action optimizer ''' 
        
        
        ### Constraint set ###
        self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        ### 
        self.timestep_to_active_mesh = {}
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts #
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        
        self.timestep_to_active_mesh_w_delta_states = {}
        
        timestep_to_tot_rot = {}
        timestep_to_tot_trans = {}
        
        self.timestep_to_active_mesh_wo_glb = {}
        self.tot_visual_pts = []
        
        
        # states -> get states -> only update the acitons #
        with torch.no_grad():
            for _ in range(2):
                redmax_sim.reset(backward_flag = True)
                for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                    ''' Get rotations, translations, and actions of the current robot '''
                    cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                    cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                    cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    # robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    
                    
                    if cur_ts == 0:
                        
                        ## init states ## 
                        # robot_init_state = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                        # redmax_q_init = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
                        
                        links_init_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)
                        # self.robot_agent.set_init_states_target_value(links_init_states)
                        # cur_visual_pts = self.robot_agent.get_init_state_visual_pts()
                        # cur_visual_pts = self.timestep_to_active_mesh_wo_glb[cur_ts]
                        redmax_q_init = links_init_states[self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()
                        
                        # redmax_q_init = np.zeros((redmax_ndof_r,), dtype=np.float32)
                        # redmax_q_init = self.robot_init_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
                        redmax_q_init = redmax_q_init[:redmax_ndof_r]
                        redmax_sim.set_q_init(redmax_q_init)
                        
                        # if i_iter % 100 == 0 or (i_iter == nn_iters_opt_redmax_actions - 1):
                            
                        cur_state = torch.from_numpy(redmax_q_init).float().cuda() ### sim_q of the redmax_sim
                        # self.robot_agent.active_robot.set_delta_state_and_update_v2(cur_state, 0) ## delta states ##
                        cur_state = torch.cat(
                            [torch.zeros((2,), dtype=torch.float32).cuda(), cur_state], dim=0 ## cur state ##
                        )
                        self.real_nroot_robot_agent.active_robot.set_delta_state_and_update_v2(cur_state, 0, use_real_act_joint=True)
                        
                        ### cur_robo_visual_pts and ##
                        cur_robo_visual_pts, robo_visual_pts_link_idxes = self.real_nroot_robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                        
                        
                        self.tot_visual_pts.append(cur_robo_visual_pts.clone())
                        
                    else:
                        
                        for i_sub_step in range(nn_substeps):
                            cur_tot_substeps = (cur_ts - 1) * nn_substeps + i_sub_step
                            
                            ### redmax robot actions ###
                            redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substeps).squeeze(0)
                            
                            redmax_u = ori_redmax_robot_actions[cur_tot_substeps].detach().cpu().numpy()
                            
                            # redmax_u = redmax_actions.detach().cpu().numpy()
                            redmax_u = redmax_u[: redmax_ndof_u]
                            
                            if i_sub_step == nn_substeps - 1:
                                print(f"i_ts: {cur_ts}, redmax_u: {redmax_u}")
                            
                            redmax_sim.set_u(redmax_u) # sim
                            # tot_redmax_actions.append(redmax_actions)
                            ''' set and set the virtual forces ''' ## redmax_sim
                            virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                            redmax_sim.set_forces(virtual_force) ### virtual forces ###

                            redmax_sim.forward(1, verbose = False)
                            redmax_sim_q = redmax_sim.get_q() # get q ### get q ## ## get q ## ## 
                            # if i_sub_step == nn_substeps - 1:
                            #     robo_intermediates_states.append(redmax_sim_q)
                            #     df_dqs.append(2 * (redmax_sim_q - robo_states[cur_ts]))
                            #     cur_diff_qs = np.sum((redmax_sim_q - robo_states[cur_ts]) ** 2)
                            #     tot_diff_qs.append(cur_diff_qs.item())
                            # else:
                            #     df_dqs.append(np.zeros((redmax_sim_q.shape[0],)))
                        
                        # if i_iter % 100 == 0 or (i_iter == nn_iters_opt_redmax_actions - 1):
                            # cur_delta_state = redmax_sim_q - (robo_intermediates_states[-1 - 1] if cur_ts > 1 else redmax_q_init)
                            # cur_delta_state = torch.from_numpy(cur_delta_state).float().cuda()
                            # states = {}
                            # states['delta_glb_rot']  = torch.eye(3, dtype=torch.float32).cuda()
                            # states['delta_glb_trans'] = torch.zeros((3,), dtype=torch.float32).cuda()
                            # states['link_states'] = cur_delta_state
                            
                            # self.robot_agent.set_and_update_states(states=states, cur_timestep=cur_ts - 1)    
                            # cur_visual_pts = self.timestep_to_active_mesh_wo_glb[cur_ts]
                            
                        cur_state = torch.from_numpy(redmax_sim_q).float().cuda() ### sim_q of the redmax_sim ## 
                        cur_state = torch.cat(
                            [torch.zeros((2,), dtype=torch.float32).cuda(), cur_state], dim=0
                        )
                        self.real_nroot_robot_agent.active_robot.set_delta_state_and_update_v2(cur_state, 0, use_real_act_joint=True) ## delta states ##
                        # self.robot_states.weight.data[cur_ts, :] = self.robot_agent.get_joint_state(cur_ts - 1, self.robot_states.weight.data[cur_ts, :])
                        # self.robot_states_sv[cur_ts, : ] = self.robot_agent.get_joint_state(cur_ts - 1, self.robot_states_sv[cur_ts, :])
                    
                        cur_robo_visual_pts, robo_visual_pts_link_idxes = self.real_nroot_robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                    
                        self.tot_visual_pts.append(cur_robo_visual_pts.clone())
                    
                    # self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts)
                    # # robo_links_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                    # # self.robot_agent.set_init_states_target_value(robo_links_states)
                    # cur_robo_visual_pts = self.robot_agent.get_init_state_visual_pts()
                    
                    self.timestep_to_active_mesh_wo_glb[cur_ts] = cur_robo_visual_pts.detach().clone()
                    
                    # ### transform the visual pts ###
                    # cur_robo_visual_pts = (cur_robo_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                    # cur_robo_visual_pts = cur_robo_visual_pts * 2. -1.
                    # cur_robo_visual_pts = cur_robo_visual_pts * self.mult_const_after_cent # mult_const #
                    
                    
                    cur_rot = cur_robo_glb_rot
                    cur_trans = cur_robo_glb_trans
                    
                    
                    timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                    timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                    
                    
                    ### transform by the glboal transformation and the translation ###
                    cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0) ## transformed pts ## 
                    
                    
                    self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts.clone() # robo visual pts #
                    
        ### validate mesh robo ###
        self.iter_step = 1
        self.validate_mesh_robo()
        
        self.tot_visual_pts = torch.stack(self.tot_visual_pts, dim=0).detach().cpu().numpy()
        self.validate_mesh_robo_c()
        
        ''' Set redmax robot actions '''
        # params_to_train_actions = []
        # self.redmax_robot_actions = nn.Embedding(
        #     num_embeddings=num_steps * nn_substeps, embedding_dim=22,
        # ).cuda()
        # torch.nn.init.zeros_(self.redmax_robot_actions.weight)
        # params_to_train_actions += list(self.redmax_robot_actions.parameters()) ## validate 
        
        # self.actions_optimizer = torch.optim.Adam(params_to_train_actions, lr=self.learning_rate)
        
        params_to_train_kines = []
        # opt_robo_states, opt_robo_glb_trans, opt_robo_glb_rot #
        if self.opt_robo_states:
            params_to_train_kines += list(self.robot_delta_states.parameters())
            # robot_actions ## get the robot actions ### ## params to train kinematics ## ## runner 
            params_to_train_kines += list(self.robot_actions.parameters())
        if self.opt_robo_glb_trans:
            params_to_train_kines += list(self.robot_glb_trans.parameters())
        if self.opt_robo_glb_rot:
            params_to_train_kines += list(self.robot_glb_rotation.parameters())
        
        
        # can tray the optimizer ## ## mano states ##
        if self.use_LBFGS:
            self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
        else:
            self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
        
        if self.optimize_rules:
            params_to_train_kines = []
            print("optimizing rules!")
            params_to_train_kines += list(self.other_bending_network.parameters())
            if self.use_LBFGS:
                self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=self.learning_rate)
            else:
                self.kines_optimizer = torch.optim.Adam(params_to_train_kines, lr=self.learning_rate)
            # self.kines_optimizer = torch.optim.LBFGS(params_to_train_kines, lr=1e-2)
        
        ## policy and the controller ##hand_test_routine_2_light_color_wtime_active_passive/wmask_reverse_value_totviews_tag_forces_rule_v13__hieoptrotorules_penalty_friction_ct_01_thres_0001_avg_optrobo_nglbtrans_reg001_manorules_projk_0_nsp_res_sqrstiff_preprojfri_projk_0_optrules_diffh_subs_10_/checkpoints/redmax_robot_actions_ckpt_001900.pth"
        # if len(load_redmax_robot_actions_fn) > 0:
        #     redmax_robot_actions_ckpt = torch.load(load_redmax_robot_actions_fn, map_location=self.device, )
            # self.redmax_robot_actions.load_state_dict(redmax_robot_actions_ckpt['redmax_robot_actions'])

        ''' prepare for keeping the original global rotations, trans, and states '''
        # ori_mano_robot_glb_rot = self.mano_robot_glb_rotation.weight.data.clone()
        # ori_mano_robot_glb_trans = self.mano_robot_glb_trans.weight.data.clone()
        # ori_mano_robot_delta_states = self.mano_robot_delta_states.weight.data.clone()
        
        
        for i_iter in tqdm(range(100000)):
            tot_losses = []
            
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            # timestep #
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # # # #
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_passive_normals = {}
            self.ts_to_passive_normals = {}
            self.ts_to_passive_pts = {}
            self.ts_to_contact_force_d = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            self.ts_to_redmax_states = {}
            self.ts_to_mano_rhand_meshes = {}
            
            self.ts_to_act_opt_pts_woglb = {}
            
            redmax_sim.reset(backward_flag = True)
            
            tot_grad_qs = []
            
            robo_intermediates_states = []
            
            tot_penetration_depth = []
            
            robo_actions_diff_loss = []
            mano_tracking_loss = []
            
            
            penetration_forces = None
            sampled_visual_pts_link_idxes = None
            
            # for cur_ts in range(self.nn_ts):
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                tot_redmax_actions = []
                self.free_def_bending_weight = 0.0

                cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                
                if cur_ts == 0:
                    ''' Set init q of the redmax '''
                    redmax_q_init = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)[self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()[:redmax_ndof_r]
                    redmax_sim.set_q_init(redmax_q_init)
                    self.ts_to_redmax_states[cur_ts] = redmax_q_init.copy()
                    redmax_sim_q = redmax_q_init.copy()
                else:
                    for i_sub_step in range(nn_substeps):
                        cur_tot_substep = (cur_ts - 1) * nn_substeps + i_sub_step
                        redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substep).squeeze(0)
                        redmax_u = redmax_actions.detach().cpu().numpy() ## set redmax actions 
                        redmax_u = redmax_u[: redmax_ndof_u]
                        redmax_sim.set_u(redmax_u) # sim
                        
                        virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                        redmax_sim.set_forces(virtual_force) ### virtual forces ###

                        redmax_sim.forward(1, verbose = False)
                        tot_redmax_actions.append(redmax_actions)
                
                    redmax_sim_q = redmax_sim.get_q()
                robo_intermediates_states.append(redmax_sim_q)
                
                
                redmax_robot_states_th = torch.from_numpy(redmax_sim_q).float().cuda()
                redmax_robot_states_th.requires_grad = True
                redmax_robot_states_th.requires_grad_ = True
                        
                
                act_robot_states = torch.cat(
                    [torch.zeros((2,), dtype=torch.float32).cuda(), redmax_robot_states_th], dim=0 ### act robot states
                )
                self.real_nroot_robot_agent.active_robot.set_delta_state_and_update_v2(act_robot_states, 0, use_real_act_joint=True) 
                
                cur_robo_visual_pts, robo_visual_pts_link_idxes = self.real_nroot_robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                
                # ts_to_act_opt_pts_woglb, timestep_to_active_mesh_wo_glb_from_states
                self.ts_to_act_opt_pts_woglb[cur_ts] = cur_robo_visual_pts.detach()
                
                if self.ref_from_prev_opt_actions:
                    saved_robo_visual_pts = self.timestep_to_active_mesh_wo_glb[cur_ts]
                else:
                    saved_robo_visual_pts = self.timestep_to_active_mesh_wo_glb_from_states[cur_ts]
                ### calculate distance ### ## saved robot pts ###
                dist_cur_robo_visual_pts_w_saved_robot_pts = torch.sum(
                    (cur_robo_visual_pts - saved_robo_visual_pts) ** 2, dim=-1
                )
                dist_cur_robo_visual_pts_w_saved_robot_pts = dist_cur_robo_visual_pts_w_saved_robot_pts.mean()
                
                diff_hand_tracking = dist_cur_robo_visual_pts_w_saved_robot_pts * 100.0
                
                ## timestep ##
                
                ### transform by the glboal transformation and the translation ###
                cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0) ## transformed pts ## 
                
                #### timestep_to_active_mesh, ts_to_mano_rhand_meshes ####
                self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts # .clone() # robo visual pts #
                
                self.ts_to_mano_rhand_meshes[cur_ts] = self.rhand_verts[cur_ts]
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                # ### diff hand tracking ### ## diff hand tracking ##
                # mano_tracking_loss.append(diff_hand_tracking.detach().cpu().item())
                
                self.kines_optimizer.zero_grad()
                diff_hand_tracking.backward(retain_graph=True)
                self.kines_optimizer.step()

                
                ''' redmax robot action grads ''' 
                if cur_ts > 0 and  (redmax_robot_states_th.grad is not None):
                    # print(f"cur_ts: {cur_ts} with gradients")
                    redmax_robot_states_grad = redmax_robot_states_th.grad.data
                    redmax_robot_states_grad_np = redmax_robot_states_grad.detach().cpu().numpy()
                    redmax_sim.backward_info.set_flags(False, False, False, True) 
                    redmax_robot_states_grad_np_tot = np.zeros((cur_ts * nn_substeps * redmax_ndof_r,), dtype=np.float32)
                    redmax_robot_states_grad_np_tot[-redmax_ndof_r:] = redmax_robot_states_grad_np[:redmax_ndof_r]
                    # redmax_robot_states_grad_np_tot = np.concatenate(
                    #     [np.zeros((redmax_ndof_r * (cur_ts - 1),), dtype=np.float32), redmax_robot_states_grad_np[:redmax_ndof_r]], axis=0
                    # )
                    # print(f"redmax_robot_states_grad_np_tot: {redmax_robot_states_grad_np_tot.shape}")
                    redmax_sim.backward_info.df_dq = redmax_robot_states_grad_np_tot
                    redmax_sim.backward_info.df_du = np.zeros(redmax_ndof_u * cur_ts * nn_substeps) 
                    # print(f"Start backarding")
                    #### redmax_sim ####
                    redmax_sim.backward() # redmax sim ##
                    # print(f"After backawarding") ## get backward df_du ##
                    redmax_sim_robot_action_grad = np.copy(redmax_sim.backward_results.df_du)
                    # redmax_sim #
                    redmax_sim_robot_action_grad_th = torch.from_numpy(redmax_sim_robot_action_grad).float().cuda()[ -redmax_ndof_u * nn_substeps: ]
                    
                    tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) ## nn_forward_steps x nn_us_dim #
                    redmax_sim_robot_action_grad_th = redmax_sim_robot_action_grad_th.contiguous().view(nn_substeps, -1).contiguous()
                    surrogate_loss = torch.sum(
                        tot_redmax_actions[:, :redmax_sim_robot_action_grad_th.size(1)] * redmax_sim_robot_action_grad_th, dim=-1
                    )
                    
                    surrogate_loss = surrogate_loss.mean()
                    ''' redmax robot action grads ''' 
                    
                    
                    
                    ''' get states and actions differences '''
                    robot_actions_diff = torch.sum(
                        (ori_redmax_robot_actions - self.redmax_robot_actions.weight) ** 2, dim=-1
                    )
                    robot_actions_diff = robot_actions_diff.mean()
                    
                    
                    
                    robot_states_actions_diff_loss = robot_actions_diff # + robot_rotation_diff + robot_trans_diff
                    
                    
                    robo_actions_diff_loss.append(robot_states_actions_diff_loss.item())
                    
                    surrogate_loss = surrogate_loss + robot_actions_diff * self.robot_actions_diff_coef

                    ## only actions optimization -> so it would not lead to unsynchronized phenoemna heere ##
                    self.actions_optimizer.zero_grad()
                    surrogate_loss.backward()
                    self.actions_optimizer.step()
                # 
                
                ### states diff loss ###
                # robot_states_actions_diff_loss = transformation_diff_loss.item()
                # robo_actions_diff_loss.append(robot_states_actions_diff_loss)

                self.update_learning_rate() #
                # tot_losses.append(loss.detach().item()) # total losses # # total losses # 
                # tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                # tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                tot_losses.append(diff_hand_tracking.detach().item())
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            
            
            if i_iter % self.report_freq == 0:
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'i_iter: {} iter:{:8>d} loss = {} lr = {}'.format(i_iter, self.iter_step, tot_losses, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                ''' Dump to the file '''
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                # print(f"spring_ks_values: { self.other_bending_network.optimizable_spring_ks.weight.data.detach().cpu().numpy() }")
            
            ## save ckpt ##
            if i_iter == 0 or (i_iter % self.ckpt_sv_freq == 0):
                self.save_checkpoint(tag="hand_only") 
            
            # self.validate_mesh_robo_a()
            if i_iter % self.val_mesh_freq == 0:
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                self.validate_mesh_robo_redmax_acts(i_iter=i_iter)
            
            
            
                
        
        
        self.iter_step = 0


        for i_iter in tqdm(range(100000)):
            tot_losses = []
            tot_tracking_loss = []
            
            # timestep #
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # # # #
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_passive_normals = {}
            self.ts_to_passive_normals = {}
            self.ts_to_passive_pts = {}
            self.ts_to_contact_force_d = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            self.ts_to_redmax_states = {}
            self.ts_to_mano_rhand_meshes = {}
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16] # pure mano hand # ## pure mano hand ##
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            
            redmax_sim.reset(backward_flag = True)
            
            tot_grad_qs = []
            
            robo_intermediates_states = []
            
            tot_penetration_depth = []
            
            robo_actions_diff_loss = []
            mano_tracking_loss = []
            
            
            penetration_forces = None
            sampled_visual_pts_link_idxes = None
            
            ### 
            # init global transformations ##
            # cur_ts_redmax_delta_rotations = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda()
            # cur_ts_redmax_delta_rotations = torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda()
            # cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda()
            
            # for cur_ts in range(self.nn_ts):
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                tot_redmax_actions = []
                # actions = {}

                self.free_def_bending_weight = 0.0

                cur_robo_glb_rot = self.robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                cur_robo_glb_rot = cur_robo_glb_rot / torch.clamp(torch.norm(cur_robo_glb_rot, dim=-1, p=2), min=1e-7)
                cur_robo_glb_rot = dyn_model_act.quaternion_to_matrix(cur_robo_glb_rot) # mano glboal rotations #
                cur_robo_glb_trans = self.robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                
                # robo_links_states = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                # self.robot_agent.set_init_states_target_value(robo_links_states)
                
                
                
                if cur_ts == 0:
                    ''' Set init q of the redmax '''
                    # redmax_q_init = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0).detach().cpu().numpy()
                    # redmax_q_init = redmax_q_init[:redmax_ndof_r]
                    # self.robot_agent.active_robot.act_joint_idxes
                    redmax_q_init = self.robot_states(torch.zeros((1,), dtype=torch.long).cuda() ).squeeze(0)[self.robot_agent.active_robot.act_joint_idxes].detach().cpu().numpy()[:redmax_ndof_r]
                    
                    # redmax_q_init
                    # redmax_q_init = np.zeros((redmax_ndof_r, ), dtype=np.float32)
                    # 
                    redmax_sim.set_q_init(redmax_q_init)
                    # redmax_sim.forward(1, verbose = False)
                    # redmax_q_init_th = torch.from_numpy(redmax_q_init).float().cuda()
                    # self.redmax_robot_states.weight.data[cur_ts, :redmax_q_init_th.size(0)] = redmax_q_init_th[:]
                    
                    self.ts_to_redmax_states[cur_ts] = redmax_q_init.copy()
                    redmax_sim_q = redmax_q_init.copy()
                else:
                    for i_sub_step in range(nn_substeps):
                        cur_tot_substep = (cur_ts - 1) * nn_substeps + i_sub_step
                        redmax_actions = self.redmax_robot_actions(torch.zeros((1,), dtype=torch.long).cuda() + cur_tot_substep).squeeze(0)
                        redmax_u = redmax_actions.detach().cpu().numpy() ## set redmax actions 
                        redmax_u = redmax_u[: redmax_ndof_u]
                        redmax_sim.set_u(redmax_u) # sim
                        
                        virtual_force = np.zeros((redmax_ndof_m,), dtype=np.float32) # virutla forces # 
                        redmax_sim.set_forces(virtual_force) ### virtual forces ###

                        
                        redmax_sim.forward(1, verbose = False)
                        tot_redmax_actions.append(redmax_actions)
                
                    ## redmax sim q ##
                    redmax_sim_q = redmax_sim.get_q()
                robo_intermediates_states.append(redmax_sim_q)
                
                
                redmax_robot_states_th = torch.from_numpy(redmax_sim_q).float().cuda()
                redmax_robot_states_th.requires_grad = True
                redmax_robot_states_th.requires_grad_ = True
                # cur_delta_state = redmax_robot_states_th - (torch.from_numpy(robo_intermediates_states[-1 - 1]).float().cuda() if cur_ts > 1 else torch.from_numpy(redmax_q_init).float().cuda())
                
                # expressive motion #
                # self.real_nroot_robot_agent.active_robot.set_delta_state_and_update_v2(cur_state, 0, use_real_act_joint=True) ## delta states ##
                #         # self.robot_states.weight.data[cur_ts, :] = self.robot_agent.get_joint_state(cur_ts - 1, self.robot_states.weight.data[cur_ts, :])
                #         # self.robot_states_sv[cur_ts, : ] = self.robot_agent.get_joint_state(cur_ts - 1, self.robot_states_sv[cur_ts, :])
                    
                        
                
                act_robot_states = torch.cat(
                    [torch.zeros((2,), dtype=torch.float32).cuda(), redmax_robot_states_th], dim=0 ### act robot states
                )
                self.real_nroot_robot_agent.active_robot.set_delta_state_and_update_v2(act_robot_states, 0, use_real_act_joint=True) 
                
                cur_robo_visual_pts, robo_visual_pts_link_idxes = self.real_nroot_robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                
                # self.real_nroot_robot_agent.active_robot.set_delta_state_and_update_v2(cur_state, 0, use_real_act_joint=True) ## delta states ##
                # # self.robot_states.weight.data[cur_ts, :] = self.robot_agent.get_joint_state(cur_ts - 1, self.robot_states.weight.data[cur_ts, :])
                # # self.robot_states_sv[cur_ts, : ] = self.robot_agent.get_joint_state(cur_ts - 1, self.robot_states_sv[cur_ts, :])
            
                # cur_visual_pts, robo_visual_pts_link_idxes = self.real_nroot_robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
        
                # tate_vals = self.robot_agent.get_joint_state( cur_ts, state_vals, link_name_to_link_struct)
                # self.robot_states.weight.data[cur_ts, :] = self.robot_agent.get_joint_state(cur_ts, self.robot_states.weight.data[cur_ts, :])
                
                # robo_delta_states = self.robot_delta_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                # self.robot_agent.active_robot.set_delta_state_and_update_v2(robo_delta_states, cur_ts) ## delta states ##
                #### get the visual pts ####
                # cur_verts, joint_idxes =  get_init_state_visual_pts(expanded_pts=False, ret_joint_idxes=True)
                # cur_robo_visual_pts, robo_visual_pts_link_idxes = self.robot_agent.get_init_state_visual_pts(ret_joint_idxes=True)
                
                # timestep_to_active_mesh_wo_glb #  ##
                if self.ref_from_prev_opt_actions:
                    saved_robo_visual_pts = self.timestep_to_active_mesh_wo_glb[cur_ts]
                else:
                    saved_robo_visual_pts = self.timestep_to_active_mesh_wo_glb_from_states[cur_ts]
                ### calculate distance ### ## saved robot pts ###
                dist_cur_robo_visual_pts_w_saved_robot_pts = torch.sum(
                    (cur_robo_visual_pts - saved_robo_visual_pts) ** 2, dim=-1
                )
                dist_cur_robo_visual_pts_w_saved_robot_pts = dist_cur_robo_visual_pts_w_saved_robot_pts.mean()
                
                
                
                cur_rot = cur_robo_glb_rot
                cur_trans = cur_robo_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                ## timestep ##
                
                ### transform by the glboal transformation and the translation ###
                cur_robo_visual_pts = torch.matmul(cur_robo_glb_rot, cur_robo_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_robo_glb_trans.unsqueeze(0) ## transformed pts ## 
                
                
                # if self.optim_sim_model_params_from_mano:
                #     self.timestep_to_active_mesh[cur_ts] = 
                
                self.timestep_to_active_mesh[cur_ts] = cur_robo_visual_pts # .clone() # robo visual pts #
                
                self.ts_to_mano_rhand_meshes[cur_ts] = self.rhand_verts[cur_ts]
                # ragged_dist = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # dist_transformed_expanded_visual_pts_to_ori_visual_pts = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                # diff_cur_states_to_ref_states = torch.zeros((1,), dtype=torch.float32).cuda().mean()
                
                
                self.free_def_bending_weight = 0.0
                # self.free_def_bending_weight = 0.5
                
                
                
                if contact_pairs_set is None:
                    self.contact_pairs_set = None
                else:
                    self.contact_pairs_set = contact_pairs_set.copy()
                
                
                if self.optim_sim_model_params_from_mano:
                    # print(f"Using mano mesh models for optimization")
                    act_mesh_pts_dict_to_model = self.ts_to_mano_rhand_meshes
                else:
                    act_mesh_pts_dict_to_model = self.timestep_to_active_mesh

                
                contact_pairs_set = self.other_bending_network.forward2( input_pts_ts=cur_ts, timestep_to_active_mesh=act_mesh_pts_dict_to_model, timestep_to_passive_mesh=self.timestep_to_passive_mesh, timestep_to_passive_mesh_normals=self.timestep_to_passive_mesh_normals, friction_forces=self.robot_actuator_friction_forces, sampled_verts_idxes=None, reference_mano_pts=None, fix_obj=False, contact_pairs_set=contact_pairs_set)
                
                

 
                if self.train_with_forces_to_active and (not self.use_mano_inputs): 
                    # penetration_forces #
                    if torch.sum(self.other_bending_network.penetrating_indicator.float()) > 0.5:
                        net_penetrating_forces = self.other_bending_network.penetrating_forces
                        net_penetrating_points = self.other_bending_network.penetrating_points

                        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
                        self.timestep_to_penetration_points[cur_ts] = net_penetrating_points.detach().cpu().numpy()
                        self.timestep_to_penetration_points_forces[cur_ts] = net_penetrating_forces.detach().cpu().numpy()
                        
                        
                        ### transform the visual pts ###
                        # cur_visual_pts = (cur_visual_pts - self.minn_robo_pts) / self.extent_robo_pts
                        # cur_visual_pts = cur_visual_pts * 2. - 1.
                        # cur_visual_pts = cur_visual_pts * self.mult_const_after_cent # mult_const #
                        
                        # sampled_visual_pts_joint_idxes = robo_visual_pts_link_idxes[finger_sampled_idxes][self.other_bending_network.penetrating_indicator]
                        # print(f"robo_visual_pts_link_idxes: {robo_visual_pts_link_idxes.size()}")
                        sampled_visual_pts_link_idxes = robo_visual_pts_link_idxes[self.other_bending_network.penetrating_indicator]
                        
                        net_penetrating_forces = torch.matmul(
                            cur_rot.transpose(1, 0), net_penetrating_forces.transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_forces = net_penetrating_forces / self.mult_const_after_cent
                        net_penetrating_forces = net_penetrating_forces / 2
                        net_penetrating_forces = net_penetrating_forces * self.extent_robo_pts
                        
                        net_penetrating_points = torch.matmul(
                            cur_rot.transpose(1, 0), (net_penetrating_points - cur_trans.unsqueeze(0)).transpose(1, 0)
                        ).transpose(1, 0)
                        net_penetrating_points = net_penetrating_points / self.mult_const_after_cent
                        net_penetrating_points = (net_penetrating_points + 1.) / 2. # penetrating points #
                        net_penetrating_points = (net_penetrating_points * self.extent_robo_pts) + self.minn_robo_pts
                        
                        penetration_forces = {
                            'penetration_forces': net_penetrating_forces,
                            'penetration_forces_points': net_penetrating_points
                        }
                        
                        # link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        
                    else:
                        penetration_forces = None
                        sampled_visual_pts_link_idxes = None
                        # link_maximal_contact_forces = torch.zeros((redmax_ndof_r, 6), dtype=torch.float32).cuda()
                        
                if contact_pairs_set is not None:
                    self.contact_pairs_sets[cur_ts] = contact_pairs_set.copy()
                
                # contact force d 
                self.ts_to_contact_passive_normals[cur_ts] = self.other_bending_network.tot_contact_passive_normals.detach().cpu().numpy()
                self.ts_to_passive_pts[cur_ts] = self.other_bending_network.cur_passive_obj_verts.detach().cpu().numpy()
                self.ts_to_passive_normals[cur_ts] = self.other_bending_network.cur_passive_obj_ns.detach().cpu().numpy()
                self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
                self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
                if self.other_bending_network.penalty_based_friction_forces is not None:
                    self.ts_to_penalty_disp_pts[cur_ts] = self.other_bending_network.penalty_based_friction_forces.detach().cpu().numpy()
                

                tot_penetration_depth.append(self.other_bending_network.penetrating_depth_penalty.detach().item())
                
                
                
                ## loss optimizaed transformations ##
                tracking_loss = self.compute_loss_optimized_transformations_v2(cur_ts + 1, cur_ts + 1)
                tot_tracking_loss.append(tracking_loss.detach().cpu().item())

                
                # loss = tracking_loss + self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                # # diff_redmax_visual_pts_with_ori_visual_pts.backward()
                penetraton_penalty = self.other_bending_network.penetrating_depth_penalty * self.penetrating_depth_penalty_coef
                
                
                # # print(f"after calculating losses....") # #
                
                # robot_states_ori, robot_glb_trans_ori, robot_glb_rotation_ori # 
                # # # ori_redmax_robot_actions, ori_redmax_glb_rot, ori_redmax_glb_trans #
                robot_rotation_diff = torch.sum(
                    (self.robot_glb_rotation_ori - self.robot_glb_rotation.weight) ** 2, dim=-1
                )
                robot_rotation_diff = robot_rotation_diff.mean()
                
                robot_trans_diff = torch.sum(
                    (self.robot_glb_trans_ori - self.robot_glb_trans.weight) ** 2, dim=-1
                )
                robot_trans_diff = robot_trans_diff.mean()
                
                robot_states_diff = torch.sum(
                    (self.robot_states_ori - self.robot_states.weight) ** 2, dim=-1
                )
                robot_states_diff = robot_states_diff.mean()
                
                robot_delta_states_diff = torch.sum(
                    (self.robot_delta_states_ori - self.robot_delta_states.weight) ** 2, dim=-1
                )
                robot_delta_states_diff = robot_delta_states_diff.mean()
                
                transformation_diff_loss = robot_rotation_diff + robot_trans_diff + robot_states_diff + robot_delta_states_diff
                
                
                ''' Kinematics proj loss ''' 
                kinematics_proj_loss = tracking_loss * self.loss_scale_coef  + transformation_diff_loss * 0.01 + penetraton_penalty
                
                if self.drive_robot == 'actions': 
                    kinematics_proj_loss = kinematics_proj_loss + dist_cur_robo_visual_pts_w_saved_robot_pts * 100.0 ### prev optimized loss ? ##
                    # kinematics_proj_loss =  dist_cur_robo_visual_pts_w_saved_robot_pts
                
                loss = kinematics_proj_loss
                
                diff_hand_tracking = dist_cur_robo_visual_pts_w_saved_robot_pts
                
                ### diff hand tracking ###
                mano_tracking_loss.append(diff_hand_tracking.detach().cpu().item())
                
                self.kines_optimizer.zero_grad()
                
                try:
                    kinematics_proj_loss.backward(retain_graph=True)
                    if self.use_LBFGS:
                        self.kines_optimizer.step(evaluate_tracking_loss) # 
                    else:
                        self.kines_optimizer.step()
                except:
                    pass

                ## try to find the bug and try to improve your code here ##
                # tracking_loss.backward(retain_graph=True) # try to solve problems ##
                if self.use_LBFGS:
                    self.other_bending_network.reset_timestep_to_quantities(cur_ts)
                
                
                ''' redmax robot action grads ''' 
                if cur_ts > 0 and  (redmax_robot_states_th.grad is not None):
                    # print(f"cur_ts: {cur_ts} with gradients")
                    redmax_robot_states_grad = redmax_robot_states_th.grad.data
                    redmax_robot_states_grad_np = redmax_robot_states_grad.detach().cpu().numpy()
                    redmax_sim.backward_info.set_flags(False, False, False, True) 
                    redmax_robot_states_grad_np_tot = np.zeros((cur_ts * nn_substeps * redmax_ndof_r,), dtype=np.float32)
                    redmax_robot_states_grad_np_tot[-redmax_ndof_r:] = redmax_robot_states_grad_np[:redmax_ndof_r]
                    # redmax_robot_states_grad_np_tot = np.concatenate(
                    #     [np.zeros((redmax_ndof_r * (cur_ts - 1),), dtype=np.float32), redmax_robot_states_grad_np[:redmax_ndof_r]], axis=0
                    # )
                    # print(f"redmax_robot_states_grad_np_tot: {redmax_robot_states_grad_np_tot.shape}")
                    redmax_sim.backward_info.df_dq = redmax_robot_states_grad_np_tot
                    redmax_sim.backward_info.df_du = np.zeros(redmax_ndof_u * cur_ts * nn_substeps) 
                    # print(f"Start backarding")
                    redmax_sim.backward()
                    # print(f"After backawarding") ## get backward df_du ##
                    redmax_sim_robot_action_grad = np.copy(redmax_sim.backward_results.df_du)
                    # redmax_sim #
                    redmax_sim_robot_action_grad_th = torch.from_numpy(redmax_sim_robot_action_grad).float().cuda()[ -redmax_ndof_u * nn_substeps: ]
                    
                    tot_redmax_actions = torch.stack(tot_redmax_actions, dim=0) ## nn_forward_steps x nn_us_dim #
                    redmax_sim_robot_action_grad_th = redmax_sim_robot_action_grad_th.contiguous().view(nn_substeps, -1).contiguous()
                    surrogate_loss = torch.sum(
                        tot_redmax_actions[:, :redmax_sim_robot_action_grad_th.size(1)] * redmax_sim_robot_action_grad_th, dim=-1
                    )
                    
                    surrogate_loss = surrogate_loss.mean()
                    ''' redmax robot action grads ''' 
                    
                    
                    
                    ''' get states and actions differences '''
                    robot_actions_diff = torch.sum(
                        (ori_redmax_robot_actions - self.redmax_robot_actions.weight) ** 2, dim=-1
                    )
                    robot_actions_diff = robot_actions_diff.mean()
                    
                    
                    
                    robot_states_actions_diff_loss = robot_actions_diff # + robot_rotation_diff + robot_trans_diff
                    
                    
                    robo_actions_diff_loss.append(robot_states_actions_diff_loss.item())
                    
                    surrogate_loss = surrogate_loss + robot_actions_diff * self.robot_actions_diff_coef

                    ## only actions optimization -> so it would not lead to unsynchronized phenoemna heere ##
                    self.actions_optimizer.zero_grad()
                    surrogate_loss.backward()
                    self.actions_optimizer.step()
                # 
                
                
                
                ### states diff loss ###
                robot_states_actions_diff_loss = transformation_diff_loss.item()
                robo_actions_diff_loss.append(robot_states_actions_diff_loss)

                
                tot_losses.append(loss.detach().item()) # total losses # # total losses # 
                # tot_penalty_dot_forces_normals.append(cur_penalty_dot_forces_normals.detach().item())
                # tot_penalty_friction_constraint.append(cur_penalty_friction_constraint.detach().item())
                
                self.iter_step += 1

                self.writer.add_scalar('Loss/loss', loss, self.iter_step)

                # if (i_iter == 0) or self.iter_step % self.save_freq == 0:
                #     self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                self.update_learning_rate() ## update learning rate ##
                
                torch.cuda.empty_cache()
                
            
            
            ## save ckpt ##
            if i_iter == 0 or (i_iter % self.ckpt_sv_freq == 0):
                self.save_checkpoint() 
            
            ''' Get nn_forward_ts and backward through the actions for updating '''
            
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            tot_penetration_depth = sum(tot_penetration_depth) / float(len(tot_penetration_depth))
            robo_actions_diff_loss = sum(robo_actions_diff_loss) / float(len(robo_actions_diff_loss))
            mano_tracking_loss = sum(mano_tracking_loss) / float(len(mano_tracking_loss))
            
            if i_iter % self.report_freq == 0:
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} tracking_loss = {} mano_tracking_loss = {} penetration_depth = {} actions_diff_loss = {} lr={}'.format(self.iter_step, tot_losses, tot_tracking_loss, mano_tracking_loss, tot_penetration_depth, robo_actions_diff_loss, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                ''' Dump to the file '''
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')
                # spring_ks_values, inertia_div_factor
                # print(f"spring_ks_values: {self.other_bending_network.spring_ks_values.weight.data.detach().cpu().numpy()}, inertia_div_factor: {self.other_bending_network.inertia_div_factor.weight.data.detach().cpu().numpy()}")
                # optimizable_spring_ks #
                print(f"spring_ks_values: { self.other_bending_network.optimizable_spring_ks.weight.data.detach().cpu().numpy() }")
            
            # self.validate_mesh_robo_a()
            if i_iter % self.val_mesh_freq == 0:
                self.validate_mesh_robo()
                ### test for contact infos ###
                self.validate_contact_info_robo()
                
            
            torch.cuda.empty_cache()
    
    
    
    ''' GRAB & TACO clips; MANO dynamic hand ''' 
    def train_dyn_mano_model_states(self, ): # train dyn mano model states #
        
        # chagne # # mano notjmano but the mano ---> optimize the mano delta states? #
        ### the real robot actions from mano model rules ### # logging #
        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))
        self.update_learning_rate() # update learning rrate # 
        # robot actions ##
        
        nn_timesteps = self.timestep_to_passive_mesh.size(0)
        self.nn_timesteps = nn_timesteps
        num_steps = self.nn_timesteps
        
        # load --- and can load other states as well ##
        ''' Load the robot hand '''
        # model_path = self.conf['model.sim_model_path'] # 
        # robot_agent = dyn_model_act.RobotAgent(xml_fn=model_path, args=None)
        # self.robot_agent = robot_agent
        # robo_init_verts = self.robot_agent.robot_pts
        # robo_sampled_verts_idxes_fn = "robo_sampled_verts_idxes.npy"
        # if os.path.exists(robo_sampled_verts_idxes_fn):
        #     sampled_verts_idxes = np.load("robo_sampled_verts_idxes.npy")
        #     sampled_verts_idxes = torch.from_numpy(sampled_verts_idxes).long().cuda()
        # else:
        #     n_sampling = 1000
        #     pts_fps_idx = data_utils.farthest_point_sampling(robo_init_verts.unsqueeze(0), n_sampling=n_sampling)
        #     sampled_verts_idxes = pts_fps_idx
        #     np.save(robo_sampled_verts_idxes_fn, sampled_verts_idxes.detach().cpu().numpy())
        # self.robo_hand_faces = self.robot_agent.robot_faces
        # self.sampled_verts_idxes = sampled_verts_idxes
        ''' Load the robot hand '''
        
        ## load the robot hand ##
        
        ''' Load robot hand in DiffHand simulator '''
        # redmax_sim = redmax.Simulation(model_path)
        # redmax_sim.reset(backward_flag = True) # redmax_sim -- 
        # # ### redmax_ndof_u, redmax_ndof_r ### #
        # redmax_ndof_u = redmax_sim.ndof_u
        # redmax_ndof_r = redmax_sim.ndof_r
        # redmax_ndof_m = redmax_sim.ndof_m ### ndof_m ### # redma # x_sim 
        
        
        ''' Load the mano hand ''' # dynamic mano hand jin it #
        model_path_mano = self.conf['model.mano_sim_model_path']
        # mano_agent = dyn_model_act_mano_deformable.RobotAgent(xml_fn=model_path_mano) # robot #
        mano_agent = dyn_model_act_mano.RobotAgent(xml_fn=model_path_mano) ## model path mano ## # 
        self.mano_agent = mano_agent
        # ''' Load the mano hand '''
        self.robo_hand_faces = self.mano_agent.robot_faces
        # if self.use_mano_hand_for_test:
        # self.robo_hand_faces = self.hand_faces
        
        nn_substeps = 10
        
        mano_nn_substeps = 1
        # mano_nn_substeps = 10 # 
        self.mano_nn_substeps = mano_nn_substeps
        
        # self.hand_faces # hand faces #
        
        ''' Expnad the current visual points ''' 
        # expanded_visual_pts = self.mano_agent.active_robot.expand_visual_pts()
        # self.expanded_visual_pts_nn = expanded_visual_pts.size(0)
        # expanded_visual_pts_npy = expanded_visual_pts.detach().cpu().numpy()
        # expanded_visual_pts_sv_fn = "expanded_visual_pts.npy"
        # np.save(expanded_visual_pts_sv_fn, expanded_visual_pts_npy)
        # # ''' Expnad the current visual points '''  #  # differentiate through the simulator? # # 
        
        params_to_train = [] # params to train #
        ### robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans ###
        
        ''' Define MANO robot actions, delta_states, init_states, frictions, and others '''
        # self.mano_robot_actions = nn.Embedding(
        #     num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        # ).cuda()
        # torch.nn.init.zeros_(self.mano_robot_actions.weight)
        # # params_to_train += list(self.robot_actions.parameters())
        
        # self.mano_robot_delta_states = nn.Embedding(
        #     num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        # ).cuda()
        # torch.nn.init.zeros_(self.mano_robot_delta_states.weight)
        # # params_to_train += list(self.robot_delta_states.parameters())
        
        # self.mano_robot_init_states = nn.Embedding(
        #     num_embeddings=1, embedding_dim=60,
        # ).cuda()
        # torch.nn.init.zeros_(self.mano_robot_init_states.weight)
        # # params_to_train += list(self.robot_init_states.parameters())
        
        self.mano_robot_glb_rotation = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=4
        ).cuda()
        self.mano_robot_glb_rotation.weight.data[:, 0] = 1.
        self.mano_robot_glb_rotation.weight.data[:, 1:] = 0.
        params_to_train += list(self.mano_robot_glb_rotation.parameters())
        
        
        self.mano_robot_glb_trans = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=3
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_glb_trans.weight)
        params_to_train += list(self.mano_robot_glb_trans.parameters())   
        # 
        self.mano_robot_states = nn.Embedding(
            num_embeddings=num_steps * mano_nn_substeps, embedding_dim=60,
        ).cuda()
        torch.nn.init.zeros_(self.mano_robot_states.weight)
        # self.mano_robot_states.weight.data[0, :] = self.mano_robot_init_states.weight.data[0, :].clone()
        params_to_train += list(self.mano_robot_states.parameters())
        
        
        ''' Load optimized MANO hand actions and states '''
        # ### laod optimized init actions ####
        if 'model.load_optimized_init_actions' in self.conf and len(self.conf['model.load_optimized_init_actions']) > 0: 
            print(f"[MANO] Loading optimized init transformations from {self.conf['model.load_optimized_init_actions']}")
            cur_optimized_init_actions_fn = self.conf['model.load_optimized_init_actions']
            # cur_optimized_init_actions = # optimized init states 
            optimized_init_actions_ckpt = torch.load(cur_optimized_init_actions_fn, map_location=self.device, )
            
            if 'mano_robot_states' in optimized_init_actions_ckpt:
                self.mano_robot_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_states'])
            self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_trans'])
            self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_rotation'])
            # self.mano_robot_init_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_init_states'])
            
            
            # if optimized_init_actions_ckpt['mano_robot_glb_rotation']['weight'].size(0) == num_steps:
            #     # weight nn_steps x xxx # 
            #     for i_cur_step in range(num_steps):
            #         self.mano_robot_glb_rotation.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 1) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_glb_rotation']['weight'][i_cur_step: i_cur_step + 1, :]
                    
            #         #### set mano glb rotations ###
            #         if i_cur_step < num_steps - 1:
            #             for i_mano_substep in range(mano_nn_substeps):
                            
            #                 cur_substep_trans = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step, :] + (optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step + 1, :] - optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step, :]) / float(mano_nn_substeps) * float(i_mano_substep)
            #                 self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps + i_mano_substep, :] = cur_substep_trans
            #         else:
            #             self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 2) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step: i_cur_step + 1, :]
            #         # self.mano_robot_glb_trans.weight.data[(i_cur_step + ) * mano_nn_substeps + i_mano_substep, :] = cur_substep_trans
                    
                    
            #         ### 
            #         # self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps: i_cur_step * mano_nn_substeps + 1] = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step: i_cur_step + 1, :]
                    
                    
            #         # self.mano_robot_glb_trans.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 1) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_glb_trans']['weight'][i_cur_step: i_cur_step + 1, :]
                    
            #         ### delta states trans ###
            #         #### delta states --- the current timestteps  ###
            #         # for i_substep in range(mano_nn_substeps):
                    
            #         self.mano_robot_delta_states.weight.data[i_cur_step * mano_nn_substeps: i_cur_step * mano_nn_substeps + 1] = optimized_init_actions_ckpt['mano_robot_delta_states']['weight'][i_cur_step: i_cur_step + 1, :]
            #         #### delta states --- set to zero ###
            #         self.mano_robot_delta_states.weight.data[i_cur_step * mano_nn_substeps + 1: (i_cur_step + 1) * mano_nn_substeps] = 0. # optimized_init_actions_ckpt['mano_robot_delta_states']['weight'][i_cur_step: i_cur_step + 1, :]
                    
            #         # if self.use_mano_hand_for_test:
                        
                    
                    
            #         # self.mano_robot_delta_states.weight.data[i_cur_step * mano_nn_substeps: (i_cur_step + 1) * mano_nn_substeps] = optimized_init_actions_ckpt['mano_robot_delta_states']['weight'][i_cur_step: i_cur_step + 1, :]
            # else:
            #     self.mano_robot_glb_rotation.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_rotation'])
            #     if 'robot_delta_states' in optimized_init_actions_ckpt:
            #         self.mano_robot_delta_states.load_state_dict(optimized_init_actions_ckpt['mano_robot_delta_states'])
            #     if 'mano_robot_actions' in optimized_init_actions_ckpt:
            #         self.mano_robot_actions.load_state_dict(optimized_init_actions_ckpt['mano_robot_actions'])
            #     # self.mano_robot_actuator_friction_forces.load_state_dict(optimized_init_actions_ckpt['robot_actuator_friction_forces'])
            #     self.mano_robot_glb_trans.load_state_dict(optimized_init_actions_ckpt['mano_robot_glb_trans'])
        else:
            optimized_init_actions_ckpt = None

        mano_glb_trans_np_data = self.mano_robot_glb_trans.weight.data.detach().cpu().numpy()
        mano_glb_rotation_np_data = self.mano_robot_glb_rotation.weight.data.detach().cpu().numpy()
        mano_states_np_data = self.mano_robot_states.weight.data.detach().cpu().numpy()
        
        if optimized_init_actions_ckpt is not None and 'object_transl' in optimized_init_actions_ckpt:
            object_transl = optimized_init_actions_ckpt['object_transl'].detach().cpu().numpy()
            object_global_orient = optimized_init_actions_ckpt['object_global_orient'].detach().cpu().numpy()
        


        
        
        ''' Scaling constants '''
        self.mano_mult_const_after_cent = 0.9507 #  1.0
        
        if 'model.mano_mult_const_after_cent' in self.conf:
            self.mano_mult_const_after_cent = self.conf['model.mano_mult_const_after_cent']
        
        mano_to_dyn_corr_pts_idxes_fn = "/home/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        if not os.path.exists(mano_to_dyn_corr_pts_idxes_fn):
            mano_to_dyn_corr_pts_idxes_fn = "/data/xueyi/diffsim/NeuS/rsc/mano/nearest_dyn_verts_idxes.npy"
        self.mano_to_dyn_corr_pts_idxes = np.load(mano_to_dyn_corr_pts_idxes_fn, allow_pickle=True)
        self.mano_to_dyn_corr_pts_idxes = torch.from_numpy(self.mano_to_dyn_corr_pts_idxes).long().cuda() 
        
        print(f"mano_to_dyn_corr_pts_idxes: {self.mano_to_dyn_corr_pts_idxes.size()}")
        

        self.nn_ts = self.nn_timesteps
        
        
        
        ''' Set actions for the redmax simulation and add parameters to params-to-train '''
        
        # params_to_train = []
        # params_to_train += list(self.redmax_robot_actions.parameters())
        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)
        # init_rot = R.random().as_quat()
        # init_rot = torch.from_numpy(init_rot).float().cuda()
        # self.robot_glb_rotation.weight.data[0, :] = init_rot[:] # init rot
        
        
        
        # ### Constraint set ###
        # self.robot_hand_states_only_allowing_neg = torch.tensor( [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16], dtype=torch.long).cuda()
        
        
        self.timestep_to_active_mesh = {}
        # ref_expanded_visual_pts, minn_idx_expanded_visual_pts_to_link_pts #
        # minn_idx_expanded_visual_pts_to_link_pts #
        self.timestep_to_expanded_visual_pts = {}
        self.timestep_to_active_mesh_opt_ours_sim = {}
        self.timestep_to_active_mesh_w_delta_states = {}
        
          
        self.iter_step = 0


        self.minn_tracking_loss = 1e27
        
        for i_iter in tqdm(range(100000)):
            tot_losses = []
            tot_tracking_loss = []
            
            # timestep 
            # self.timestep_to_active_mesh = {}
            self.timestep_to_posed_active_mesh = {}
            self.timestep_to_posed_mano_active_mesh = {}
            self.timestep_to_mano_active_mesh = {}
            self.timestep_to_corr_mano_pts = {}
            # self.timestep_to_
            timestep_to_tot_rot = {}
            timestep_to_tot_trans = {}
            
            correspondence_pts_idxes = None
            # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
            self.timestep_to_raw_active_meshes = {}
            self.timestep_to_penetration_points = {}
            self.timestep_to_penetration_points_forces = {}
            self.joint_name_to_penetration_forces_intermediates = {}
            
            
            self.ts_to_contact_force_d = {}
            self.ts_to_penalty_frictions = {}
            self.ts_to_penalty_disp_pts = {}
            self.ts_to_redmax_states = {}
            # constraitns for states # 
            # with 17 dimensions on the states; [3, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16]
            
            contact_pairs_set = None
            self.contact_pairs_sets = {}
            
            # redmax_sim.reset(backward_flag = True)
            
            # tot_grad_qs = []
            
            robo_intermediates_states = []
            
            tot_penetration_depth = []
            
            robo_actions_diff_loss = []
            mano_tracking_loss = []
            
            # init global transformations ##
            # cur_ts_redmax_delta_rotations = torch.tensor([1., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_delta_rotations = torch.tensor([0., 0., 0., 0.], dtype=torch.float32).cuda()
            cur_ts_redmax_robot_trans = torch.zeros((3,), dtype=torch.float32).cuda()
            
            # for cur_ts in range(self.nn_ts):
            for cur_ts in range(self.nn_ts * self.mano_nn_substeps):
                tot_redmax_actions = []
                
                
                actions = {}
                
                self.free_def_bending_weight = 0.0

                # mano_robot_glb_rotation, mano_robot_glb_trans, mano_robot_delta_states #
                cur_glb_rot = self.mano_robot_glb_rotation(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                cur_glb_rot = cur_glb_rot + cur_ts_redmax_delta_rotations
                
                cur_glb_rot = cur_glb_rot / torch.clamp(torch.norm(cur_glb_rot, dim=-1, p=2), min=1e-7)
                # cur_glb_rot_quat = cur_glb_rot.clone()
                
                cur_glb_rot = dyn_model_act.quaternion_to_matrix(cur_glb_rot) # mano glboal rotations #
                cur_glb_trans = self.mano_robot_glb_trans(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                
                # cur_ts_redmax_delta_rotations, cur_ts_redmax_robot_trans # 
                # cur_ts_delta_rot, cur_ts_redmax_robot_trans #
                # # print(f"ts: {cur_ts}, cur_ts_redmax_delta_rotations: {cur_ts_redmax_delta_rotations}, cur_glb_rot: {cur_glb_rot_quat}")
                # cur_ts_delta_rot = dyn_model_act.quaternion_to_matrix(cur_ts_redmax_delta_rotations) # mano glboa
                # # cur_ts_delta_trans = 
                
                # # # cur_ts_delta_rot, cur_ts_redmax_robot_trans # # # 
                ## simulatio pa
                # parameters # filed s -> fricitonal forces # --- 
                # cur_glb_rot = torch.matmul(cur_ts_delta_rot, cur_glb_rot)
                cur_glb_trans = cur_glb_trans + cur_ts_redmax_robot_trans # 

                link_cur_states = self.mano_robot_states(torch.zeros((1,), dtype=torch.long).cuda() + cur_ts).squeeze(0)
                self.mano_agent.set_init_states_target_value(link_cur_states)
                
                
                cur_visual_pts = self.mano_agent.get_init_state_visual_pts() 


                cur_visual_pts = cur_visual_pts * self.mano_mult_const_after_cent
                
                
                cur_rot = cur_glb_rot
                cur_trans = cur_glb_trans
                
                timestep_to_tot_rot[cur_ts] = cur_rot.detach()
                timestep_to_tot_trans[cur_ts] = cur_trans.detach()
                
                ## 
                ### transform by the glboal transformation and the translation ###
                cur_visual_pts = torch.matmul(cur_rot, cur_visual_pts.contiguous().transpose(1, 0).contiguous()).transpose(1, 0).contiguous() + cur_trans.unsqueeze(0) 
                
                
                
                # diff_redmax_visual_pts_with_ori_visual_pts = torch.sum(
                #     (cur_visual_pts[sampled_verts_idxes] - self.timestep_to_active_mesh_opt_ours_sim[cur_ts].detach()) ** 2, dim=-1
                # )
                # diff_redmax_visual_pts_with_ori_visual_pts = diff_redmax_visual_pts_with_ori_visual_pts.mean()
                
                # if self.use_mano_hand_for_test:
                #     self.timestep_to_active_mesh[cur_ts] = self.rhand_verts[cur_ts] # .detach()
                # else:
                self.timestep_to_active_mesh[cur_ts] = cur_visual_pts
                self.timestep_to_raw_active_meshes[cur_ts] = cur_visual_pts.detach().cpu().numpy()
                
                
                
                
                cur_kine_rhand_verts = self.rhand_verts[cur_ts // mano_nn_substeps]
                cur_dyn_visual_pts_to_mano_verts = cur_visual_pts[self.mano_to_dyn_corr_pts_idxes]
                diff_hand_tracking = torch.mean(
                    torch.sum((cur_kine_rhand_verts - cur_dyn_visual_pts_to_mano_verts) ** 2, dim=-1)
                )
                    
                
                
                self.optimizer.zero_grad()
                loss = diff_hand_tracking
                loss.backward(retain_graph=True)
                self.optimizer.step()
                
                # diff_hand_tracking # diff hand ## 
                mano_tracking_loss.append(diff_hand_tracking.detach().cpu().item())
                
                
                
                tot_losses.append(loss.detach().item()) # total losses # # total losses # 


                self.writer.add_scalar('Loss/loss', loss, self.iter_step)

                self.iter_step += 1
                self.update_learning_rate() ## update learning rate ##
                
                torch.cuda.empty_cache()
                
            
            ''' Get nn_forward_ts and backward through the actions for updating '''
            
            if (i_iter % self.ckpt_sv_freq) == 0:
                self.save_checkpoint() # a smart solution for them ? # # save checkpoint #
                
            
            tot_losses = sum(tot_losses) / float(len(tot_losses))
            # tot_penalty_dot_forces_normals = sum(tot_penalty_dot_forces_normals) / float(len(tot_penalty_dot_forces_normals))
            # tot_penalty_friction_constraint = sum(tot_penalty_friction_constraint) / float(len(tot_penalty_friction_constraint))
            # tot_tracking_loss = sum(tot_tracking_loss) / float(len(tot_tracking_loss))
            # tot_penetration_depth = sum(tot_penetration_depth) / float(len(tot_penetration_depth))
            # robo_actions_diff_loss = sum(robo_actions_diff_loss) / float(len(robo_actions_diff_loss))
            mano_tracking_loss = sum(mano_tracking_loss) / float(len(mano_tracking_loss))

            
            # if tot_losses < self.minn_tracking_loss:
            #     self.minn_tracking_loss = tot_losses
            #     self.save_checkpoint(tag="best")
                
                
            if i_iter % self.report_freq == 0:
                logs_sv_fn = os.path.join(self.base_exp_dir, 'log.txt')
                
                cur_log_sv_str = 'iter:{:8>d} loss = {} mano_tracking_loss = {} lr={}'.format(self.iter_step, tot_losses, mano_tracking_loss, self.optimizer.param_groups[0]['lr'])
                
                print(cur_log_sv_str)
                ''' Dump to the file '''
                with open(logs_sv_fn, 'a') as log_file:
                    log_file.write(cur_log_sv_str + '\n')

            # self.validate_mesh_robo_a()
            if i_iter % self.val_mesh_freq == 0:
                # self.validate_mesh_robo()
                self.validate_mesh_robo_e()
                
                ### test for contact infos ###
                # self.validate_contact_info_robo()
                
            
            torch.cuda.empty_cache()
    
    
    def update_learning_rate(self):
        if self.iter_step < self.warm_up_end: # warm up end and the w
            learning_factor = self.iter_step / self.warm_up_end
        else:
            alpha = self.learning_rate_alpha
            progress = (self.iter_step - self.warm_up_end) / (self.end_iter - self.warm_up_end)
            learning_factor = (np.cos(np.pi * progress) + 1.0) * 0.5 * (1 - alpha) + alpha

        ## g in self.
        for g in self.optimizer.param_groups:
            g['lr'] = self.learning_rate * learning_factor

    ## backup files ##
    def file_backup(self):
        dir_lis = self.conf['general.recording']
        os.makedirs(os.path.join(self.base_exp_dir, 'recording'), exist_ok=True)
        for dir_name in dir_lis:
            cur_dir = os.path.join(self.base_exp_dir, 'recording', dir_name)
            os.makedirs(cur_dir, exist_ok=True)
            files = os.listdir(dir_name)
            for f_name in files:
                if f_name[-3:] == '.py':
                    copyfile(os.path.join(dir_name, f_name), os.path.join(cur_dir, f_name))

        copyfile(self.conf_path, os.path.join(self.base_exp_dir, 'recording', 'config.conf'))

    def load_checkpoint(self, checkpoint_name):
        checkpoint = torch.load(os.path.join(self.base_exp_dir, 'checkpoints', checkpoint_name), map_location=self.device)
        self.nerf_outside.load_state_dict(checkpoint['nerf'])
        for i_obj in range(len(self.sdf_network)):
            self.sdf_network[i_obj].load_state_dict(checkpoint['sdf_network_fine'][i_obj])
            self.bending_network[i_obj].load_state_dict(checkpoint['bending_network_fine'][i_obj])
        # self.sdf_network.load_state_dict(checkpoint['sdf_network_fine'])
        self.deviation_network.load_state_dict(checkpoint['variance_network_fine'])
        self.color_network.load_state_dict(checkpoint['color_network_fine'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.iter_step = checkpoint['iter_step']

        logging.info('End')
        
    
    # laod checkpoint 
    def load_checkpoint_via_fn(self, checkpoint_fn):
        # ji
        
        checkpoint = torch.load(checkpoint_fn, map_location=self.device, )
        
        self.other_bending_network.load_state_dict(checkpoint['dyn_model'], strict=False)
        # self.other_bending_network.load_state_dict(checkpoint['dyn_model'])
        
        logging.info(f"checkpoint with sdf_net and bending_net loaded from {checkpoint_fn}")
        # self.optimizer.load_state_dict(checkpoint['optimizer'])
        # self.iter_step = checkpoint['iter_step']

        logging.info('End')
        
    def load_checkpoint_prev_delta(self, delta_mesh_checkpoint_fn):
        delta_mesh_checkpoint = torch.load(delta_mesh_checkpoint_fn, map_location=self.device)
        self.prev_sdf_network.load_state_dict(delta_mesh_checkpoint['sdf_network_fine'])
        logging.info(f"delta_mesh checkpoint loaded from {delta_mesh_checkpoint_fn}")
    
    def save_checkpoint_delta_states(self, ):
        checkpoint = {
            'robot_delta_states': self.robot_delta_states.state_dict()
        }
        ckpt_sv_root_folder = os.path.join(self.base_exp_dir, 'checkpoints')
        os.makedirs(ckpt_sv_root_folder, exist_ok=True)
        ckpt_sv_fn = os.path.join(ckpt_sv_root_folder, 'robo_delta_states_ckpt_{:0>6d}.pth'.format(self.iter_step))
        
        torch.save(checkpoint, ckpt_sv_fn)
    
    # redmax_robot_actions
    def save_checkpoint_redmax_robot_actions(self, ):
        checkpoint = {
            'redmax_robot_actions': self.redmax_robot_actions.state_dict()
        }
        ckpt_sv_root_folder = os.path.join(self.base_exp_dir, 'checkpoints')
        os.makedirs(ckpt_sv_root_folder, exist_ok=True)
        ckpt_sv_fn = os.path.join(ckpt_sv_root_folder, 'redmax_robot_actions_ckpt_{:0>6d}.pth'.format(self.iter_step))
        
        torch.save(checkpoint, ckpt_sv_fn)

    def save_checkpoint(self, tag="", niter=False):
        # checkpoint #
        
        ### save model weights ###
        checkpoint = {
            'dyn_model': self.other_bending_network.state_dict()
        }
        # if self.mode == '':
        #     checkpoint['robot_actions'] = self.robot_actions.state_dict()
            # robot_actions = self.robot_actions.
        # robot_actions, 
        # robot_actions, robot_init_states, robot_glb_rotation, robot_actuator_friction_forces, robot_glb_trans
        if self.mode in ['train_actions_from_model_rules', 'train_mano_actions_from_model_rules', 'train_actions_from_mano_model_rules', 'train_real_robot_actions_from_mano_model_rules', 'train_real_robot_actions_from_mano_model_rules_diffhand', 'train_real_robot_actions_from_mano_model_rules_diffhand_fortest', 'train_finger_retargeting', 'train_real_robot_actions_from_mano_model_rules_manohand_fortest', 'train_real_robot_actions_from_mano_model_rules_manohand_fortest_states', 'train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states_res_world', 'train_dyn_mano_model_states', 'train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states_grab', 'train_expanded_set_motions', 'train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab', 'train_expanded_set_motions_retar', "train_finger_kinematics_retargeting_arctic_twohands", "train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_arctic_twohands", "train_redmax_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab", "train_manip_acts_params"]:
            try:    
                checkpoint['robot_actions'] = self.robot_actions.state_dict()    
            except:
                pass
            
            try:    
                checkpoint['robot_actions_lft'] = self.robot_actions_lft.state_dict()    
            except:
                pass
            try:
                checkpoint['robot_init_states'] = self.robot_init_states.state_dict()    
            except:
                pass
            try:
                checkpoint['robot_glb_rotation'] = self.robot_glb_rotation.state_dict()    
            except:
                pass
            
            try:
                checkpoint['robot_glb_rotation_lft'] = self.robot_glb_rotation_lft.state_dict()    
            except:
                pass
            
            try:
                checkpoint['robot_actuator_friction_forces'] = self.robot_actuator_friction_forces.state_dict()    
            except:
                pass
            
            try:
                checkpoint['robot_actuator_friction_forces'] = self.robot_actuator_friction_forces.state_dict() 
            except:
                pass
            
            try:
                checkpoint['robot_glb_trans'] = self.robot_glb_trans.state_dict() 
            except:
                pass
            # try:
            #     checkpoint['robot_delta_states'] = self.robot_delta_states.state_dict()
            # except:
            #     pass
            
            try:
                checkpoint['robot_delta_angles'] = self.robot_delta_angles.state_dict()
            except:
                pass
            
            try:
                checkpoint['robot_delta_trans'] = self.robot_delta_trans.state_dict()
            except:
                pass
            
            # try:
            #     checkpoint['robot_glb_trans_lft'] = self.robot_glb_trans_lft.state_dict() 
            # except:
            #     pass
            
            try:
                checkpoint['robot_glb_trans_lft'] = self.robot_glb_trans_lft.state_dict() 
            except:
                pass
            try:
                checkpoint['robot_delta_states'] = self.robot_delta_states.state_dict()
            except:
                pass
            
            
            if self.mode in ['train_actions_from_mano_model_rules']:
                
                checkpoint['expanded_actuator_friction_forces'] = self.expanded_actuator_friction_forces.state_dict() 
            # robot_delta_states 
            
            try:
                checkpoint['expanded_actuator_delta_offset'] = self.expanded_actuator_delta_offset.state_dict()
            except:
                pass
            
            # mano_expanded_actuator_friction_forces, mano_expanded_actuator_delta_offset # 
            try:
                checkpoint['mano_expanded_actuator_friction_forces'] = self.mano_expanded_actuator_friction_forces.state_dict()
            except:
                pass
            try:
                checkpoint['mano_expanded_actuator_delta_offset'] = self.mano_expanded_actuator_delta_offset.state_dict()
            except:
                pass
            
            # mano_expanded_actuator_delta_offset_nex
            try:
                checkpoint['mano_expanded_actuator_delta_offset_nex'] = self.mano_expanded_actuator_delta_offset_nex.state_dict()
            except:
                pass
            
            # mano_robot_glb_rotation, mano_robot_glb_trans, mano_robot_init_states, mano_robot_delta_states #
            try:
                checkpoint['mano_robot_glb_rotation'] = self.mano_robot_glb_rotation.state_dict()
            except:
                pass
            
            try:
                checkpoint['mano_robot_glb_trans'] = self.mano_robot_glb_trans.state_dict()
            except:
                pass
            
            try:
                checkpoint['mano_robot_init_states'] = self.mano_robot_init_states.state_dict()
            except:
                pass
            
            try:
                checkpoint['mano_robot_delta_states'] = self.mano_robot_delta_states.state_dict()
            except:
                pass
        
            try:
                checkpoint['mano_robot_states'] = self.mano_robot_states.state_dict()
            except:
                pass
            
            try: # redmax_robot_actions
                checkpoint['redmax_robot_actions'] = self.redmax_robot_actions.state_dict()
            except:
                pass
            
            # residual_controller, residual_dynamics_model # 
            try:
                checkpoint['residual_controller'] = self.residual_controller.state_dict()
            except:
                pass
            
            try:
                checkpoint['residual_dynamics_model'] = self.residual_dynamics_model.state_dict()
            except:
                pass
            
            # robot_states
            try:
                checkpoint['robot_states'] = self.robot_states.state_dict()
            except:
                pass
            
            try:
                checkpoint['robot_states_sv_from_act'] = self.robot_states_sv
            except:
                pass
        
            try:
                checkpoint['robot_states_lft'] = self.robot_states_lft.state_dict()
            except:
                pass
            
            try:
                checkpoint['robot_delta_states_lft'] = self.robot_delta_states_lft.state_dict()
            except:
                pass
                
            # robot_delta_glb_trans, robot_delta_glb_trans_lft
            try:
                checkpoint['robot_delta_glb_trans'] = self.robot_delta_glb_trans.state_dict()
            except:
                pass
        
            try:
                checkpoint['robot_delta_glb_trans_lft'] = self.robot_delta_glb_trans_lft.state_dict()
            except:
                pass
            
            # # object_global_orient, object_transl #
            try:
                checkpoint['object_transl'] = self.object_transl
                checkpoint['object_global_orient'] = self.object_global_orient
            except:
                pass
            
            try:
                # optimized_quat
                cur_optimized_quat = np.stack(self.optimized_quat, axis=0)
                cur_optimized_trans = np.stack(self.optimized_trans, axis=0)
                checkpoint['optimized_quat'] = cur_optimized_quat
                checkpoint['optimized_trans'] = cur_optimized_trans
            except:
                pass
                
            
        
        print(f"Saving checkpoint with keys {checkpoint.keys()}")
        os.makedirs(os.path.join(self.base_exp_dir, 'checkpoints'), exist_ok=True)
        
        ckpt_sv_fn = os.path.join(self.base_exp_dir, 'checkpoints', 'ckpt_{:0>6d}.pth'.format(self.iter_step))
        if len(tag) > 0:
            if tag == "best": ## 
                ckpt_sv_fn = os.path.join(self.base_exp_dir, 'checkpoints', 'ckpt_{}.pth'.format(tag))
            else:
                if niter: ### 
                    ckpt_sv_fn = os.path.join(self.base_exp_dir, 'checkpoints', 'ckpt_{}.pth'.format(tag))
                else:
                    # ckpt_sv_fn = os.path.join(self.base_exp_dir, 'checkpoints', 'ckpt_{}_{}.pth'.format(self.iter_step, tag))
                    ckpt_sv_fn = os.path.join(self.base_exp_dir, 'checkpoints', 'ckpt_{}_{}.pth'.format(self.iter_step, tag))
        
        torch.save(checkpoint, ckpt_sv_fn)
        
        
        
        
        bending_net_save_values = self.other_bending_network.save_values
        os.makedirs(os.path.join(self.base_exp_dir, 'miscs'), exist_ok=True)
        bending_net_save_values_sv_fn = os.path.join(self.base_exp_dir, 'miscs', 'bending_net_save_values_{:0>6d}.npy'.format(self.iter_step))
        np.save(bending_net_save_values_sv_fn, bending_net_save_values) 
        return ckpt_sv_fn
    
    
    def validate_mesh_robo(self, ): # validate mesh robo # ## validate the #
        def merge_meshes(verts_list, faces_list):
            tot_verts_nn = 0
            merged_verts = []
            merged_faces = []
            for i_mesh in range(len(verts_list)):
                merged_verts.append(verts_list[i_mesh])
                merged_faces.append(faces_list[i_mesh] + tot_verts_nn) # 
                tot_verts_nn += verts_list[i_mesh].shape[0]
            merged_verts = np.concatenate(merged_verts, axis=0)
            merged_faces = np.concatenate(merged_faces, axis=0)
            return merged_verts, merged_faces
        
        # self.hand_faces, self.obj_faces # # hand faces #
        mano_hand_faces_np = self.hand_faces.detach().cpu().numpy()
        hand_faces_np = self.robo_hand_faces.detach().cpu().numpy()
        
        if self.use_mano_inputs:
            hand_faces_np = mano_hand_faces_np
            
        if self.optim_sim_model_params_from_mano:
            hand_faces_np = mano_hand_faces_np
        
        obj_faces_np = self.obj_faces.detach().cpu().numpy()
        
        
        if self.other_bending_network.canon_passive_obj_verts is None:
            init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
            init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
        else:
            init_passive_obj_verts = self.other_bending_network.canon_passive_obj_verts.detach().cpu().numpy()
            init_passive_obj_verts_center = torch.zeros((3, )).cuda().detach().cpu().numpy()
        
        # init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
        # init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        ts_to_obj_quaternion = {}
        ts_to_obj_rot_mtx = {}
        ts_to_obj_trans = {}
        ts_to_hand_obj_verts = {}
        # for i_ts in range(1, self.nn_timesteps - 1, 10):
        for i_ts in range(0, (self.nn_ts - 1) * self.mano_nn_substeps, 1):
            if self.optim_sim_model_params_from_mano:
                cur_hand_mesh = self.rhand_verts[i_ts] # .detach().cpu().numpy()
            else:
                cur_hand_mesh = self.timestep_to_active_mesh[i_ts]
            
            if i_ts % self.mano_nn_substeps == 0:
                cur_mano_rhand = self.rhand_verts[i_ts // self.mano_nn_substeps].detach().cpu().numpy()
            cur_rhand_verts = cur_hand_mesh # [: cur_hand_mesh.size(0) // 2]
            # cur_lhand_verts = cur_hand_mesh # [cur_hand_mesh.size(0) // 2:]
            cur_rhand_verts_np = cur_rhand_verts.detach().cpu().numpy()
            # cur_lhand_verts_np = cur_lhand_verts.detach().cpu().numpy()
            if i_ts not in self.other_bending_network.timestep_to_optimizable_rot_mtx: # to optimizable rot mtx #
                cur_pred_rot_mtx = np.eye(3, dtype=np.float32)
                cur_pred_trans = np.zeros((3,), dtype=np.float32)
            else:
                cur_pred_rot_mtx = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
                cur_pred_trans = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            cur_transformed_obj = np.matmul(cur_pred_rot_mtx, (init_passive_obj_verts - init_passive_obj_verts_center).T).T + init_passive_obj_verts_center + np.reshape(cur_pred_trans, (1, 3))
            
            ## the training 
            if self.mode in ['train_finger_retargeting']:
                cur_transformed_obj = self.obj_pcs[i_ts].detach().cpu().numpy()

            if i_ts not in self.other_bending_network.timestep_to_optimizable_quaternion:
                ts_to_obj_quaternion[i_ts] = np.zeros((4,), dtype=np.float32)
                ts_to_obj_quaternion[i_ts][0] = 1. # ## quaternion ## # 
                ts_to_obj_rot_mtx[i_ts] = np.eye(3, dtype=np.float32)
                ts_to_obj_trans[i_ts] = np.zeros((3,),  dtype=np.float32)
            else:
                ts_to_obj_quaternion[i_ts] = self.other_bending_network.timestep_to_optimizable_quaternion[i_ts].detach().cpu().numpy()
                ts_to_obj_rot_mtx[i_ts] = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
                ts_to_obj_trans[i_ts] = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            # 
            ts_to_hand_obj_verts[i_ts] = (cur_rhand_verts_np, cur_transformed_obj) # not correct.... #
            # merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_transformed_obj, cur_mano_rhand], [hand_faces_np, obj_faces_np, mano_hand_faces_np])
            if i_ts % 10 == 0:
                # print(f"exporting meshes i_ts: {i_ts}, cur_hand_verts_np: {cur_rhand_verts_np.shape}, hand_faces_np: {hand_faces_np.shape}")
                merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_transformed_obj], [hand_faces_np, obj_faces_np])
                maxx_hand_faces, minn_hand_faces = np.max(hand_faces_np), np.min(hand_faces_np)
                maxx_obj_faces, minn_obj_faces = np.max(obj_faces_np), np.min(obj_faces_np)
                
                # print(f"cur_rhand_verts_np: {cur_rhand_verts_np.shape}, cur_transformed_obj: {cur_transformed_obj.shape}, maxx_hand_faces: {maxx_hand_faces}, minn_hand_faces: {minn_hand_faces}, maxx_obj_faces: {maxx_obj_faces}, minn_obj_faces: {minn_obj_faces}")
                mesh = trimesh.Trimesh(merged_verts, merged_faces)
                mesh_sv_fn = '{:0>8d}_ts_{:0>3d}.ply'.format(self.iter_step, i_ts)
                mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                
                if i_ts % self.mano_nn_substeps == 0:
                    ### overlayed with the mano mesh ###
                    merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_transformed_obj, cur_mano_rhand], [hand_faces_np, obj_faces_np, mano_hand_faces_np])
                    mesh = trimesh.Trimesh(merged_verts, merged_faces)
                    mesh_sv_fn = '{:0>8d}_ts_{:0>3d}_wmano.ply'.format(self.iter_step, i_ts)
                    mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                
                
        ts_sv_dict = {
            'ts_to_obj_quaternion': ts_to_obj_quaternion,
            'ts_to_obj_rot_mtx': ts_to_obj_rot_mtx,
            'ts_to_obj_trans': ts_to_obj_trans,
            'ts_to_hand_obj_verts': ts_to_hand_obj_verts
        }
        
        if self.mode not in ['train_finger_retargeting']:
            hand_obj_verts_faces_sv_dict = {
                'ts_to_hand_obj_verts': ts_to_hand_obj_verts,
                'hand_faces': hand_faces_np,
                'obj_faces': obj_faces_np
            }
            
            hand_obj_verts_faces_sv_dict_sv_fn = 'hand_obj_verts_faces_sv_dict_{:0>8d}.npy'.format(self.iter_step)
            hand_obj_verts_faces_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, hand_obj_verts_faces_sv_dict_sv_fn)
            np.save(hand_obj_verts_faces_sv_dict_sv_fn, hand_obj_verts_faces_sv_dict)
            # collision detection and # hand obj verts faces sv dict #
        try:
            timestep_to_mano_active_mesh = {ts: self.timestep_to_mano_active_mesh[ts].detach().cpu().numpy() for ts in self.timestep_to_mano_active_mesh}
            mano_sv_dict_sv_fn = 'mano_act_pts_{:0>8d}.npy'.format(self.iter_step)
            mano_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, mano_sv_dict_sv_fn)
            np.save(mano_sv_dict_sv_fn, timestep_to_mano_active_mesh)
            
            timestep_to_robot_mesh = {ts: self.timestep_to_active_mesh[ts].detach().cpu().numpy() for ts in self.timestep_to_active_mesh}
            rhand_verts = self.rhand_verts.detach().cpu().numpy()
            #  ts_to_robot_fingers, ts_to_mano_fingers
            retar_sv_dict = {
                'timestep_to_robot_mesh': timestep_to_robot_mesh,
                'rhand_verts': rhand_verts,
                'ts_to_robot_fingers': self.ts_to_robot_fingers, 
                'ts_to_mano_fingers': self.ts_to_mano_fingers,
            }
            retar_sv_fn =  'retar_info_dict_{:0>8d}.npy'.format(self.iter_step)
            retar_sv_fn = os.path.join(mesh_sv_root_dir, retar_sv_fn)
            np.save(retar_sv_fn, retar_sv_dict)
            # mano_sv_dict_sv_fn = 'mano_act_pts_{:0>8d}.npy'.format(self.iter_step)
            # mano_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, mano_sv_dict_sv_fn)
            # np.save(mano_sv_dict_sv_fn, timestep_to_mano_active_mesh)
            
            # timestep_to_corr_mano_pts #
            # timestep_to_mano_active_mesh #
            timestep_to_corr_mano_pts = {ts: self.timestep_to_corr_mano_pts[ts].detach().cpu().numpy() for ts in self.timestep_to_corr_mano_pts}
            mano_sv_dict_sv_fn = 'corr_mano_act_pts_{:0>8d}.npy'.format(self.iter_step)
            mano_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, mano_sv_dict_sv_fn)
            np.save(mano_sv_dict_sv_fn, timestep_to_corr_mano_pts)
        except:
            pass
        sv_dict_sv_fn = '{:0>8d}.npy'.format(self.iter_step)
        sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, sv_dict_sv_fn)
        np.save(sv_dict_sv_fn, ts_sv_dict)
        
        try:
            robo_intermediates_states_np = self.robo_intermediates_states.numpy()
            robo_intermediates_states_sv_fn = f"robo_intermediates_states_{self.iter_step}.npy"
            robo_intermediates_states_sv_fn = os.path.join(mesh_sv_root_dir, robo_intermediates_states_sv_fn)
            np.save(robo_intermediates_states_sv_fn, robo_intermediates_states_np) # 
        except:
            pass
    
    def validate_mesh_robo_redmax_acts(self, i_iter, tag=None):
        #### timestep_to_active_mesh, ts_to_mano_rhand_meshes ####
        #### # ts_to_act_opt_pts_woglb, timestep_to_active_mesh_wo_glb_from_states
        optimized_redmax_meshes = {
            ts: self.ts_to_act_opt_pts_woglb[ts].detach().cpu().numpy() for ts in self.ts_to_act_opt_pts_woglb
        }
        states_optimized_meshes = {
            ts: self.timestep_to_active_mesh_wo_glb_from_states[ts].detach().cpu().numpy() for ts in self.timestep_to_active_mesh_wo_glb_from_states
        }
        act_optimized_sv_dict = {
            'optimized_redmax_meshes': optimized_redmax_meshes,
            'states_optimized_meshes': states_optimized_meshes,
        }
        
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        if (tag is None) or (len(tag) == 0):
            act_optimized_sv_dict_fn = f"act_optimized_sv_dict_{i_iter}.npy"
        else:
            act_optimized_sv_dict_fn = f"act_optimized_sv_dict_{i_iter}_tag_{tag}.npy"
        act_optimized_sv_dict_fn = os.path.join(mesh_sv_root_dir, act_optimized_sv_dict_fn)
        np.save(act_optimized_sv_dict_fn, act_optimized_sv_dict) #
        print(f"Redmax acts optimized info saved to {act_optimized_sv_dict_fn}")
    
    # # ts_to_target_sim_active_meshes, ts_to_target_sim_obj_quat, ts_to_target_sim_obj_trans #
    def validate_mesh_robo_d(self, ):
        init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
        init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
        
        ts_to_active_mesh = {
            ts: self.ts_to_target_sim_active_meshes[ts].detach().cpu().numpy() for ts in self.ts_to_target_sim_active_meshes
        }
        ts_to_passive_mesh = {}
        for ts in self.ts_to_target_sim_active_meshes:
            cur_pred_quat = self.ts_to_target_sim_obj_quat[ts]
            cur_pred_trans = self.ts_to_target_sim_obj_trans[ts].detach().cpu().numpy()
            
            cur_pred_rot_mtx = fields.quaternion_to_matrix(cur_pred_quat).detach().cpu().numpy()
            
            ## cur pred trnas ##
            # cur_pred_trans = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            cur_transformed_obj = np.matmul(cur_pred_rot_mtx, (init_passive_obj_verts - init_passive_obj_verts_center).T).T + init_passive_obj_verts_center + np.reshape(cur_pred_trans, (1, 3))
            ts_to_passive_mesh[ts] = cur_transformed_obj
            
        obj_faces_np = self.obj_faces.detach().cpu().numpy()
        hand_faces_np = self.robo_hand_faces.detach().cpu().numpy() # # 
        
        # obj faces; hand faces; ts to active meshes; ts to passive meshes #
        
        sv_dict = {
            'ts_to_active_mesh': ts_to_active_mesh, 'ts_to_passive_mesh': ts_to_passive_mesh, 'obj_faces': obj_faces_np, 'hand_faces': hand_faces_np
        }
        
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        ts_to_active_mesh_sv_fn = '{:0>8d}_ts_to_target_sim_active_mesh.npy'.format(self.iter_step)
        ts_to_active_mesh_sv_fn = os.path.join(mesh_sv_root_dir, ts_to_active_mesh_sv_fn)
        np.save(ts_to_active_mesh_sv_fn, sv_dict)
        

    def validate_mesh_robo_a(self, ):
        ts_to_active_mesh = {
            ts: self.timestep_to_active_mesh[ts][self.sampled_verts_idxes].detach().cpu().numpy() for ts in self.timestep_to_active_mesh
        }
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        ts_to_active_mesh_sv_fn = '{:0>8d}_ts_to_active_mesh.ply'.format(self.iter_step)
        ts_to_active_mesh_sv_fn = os.path.join(mesh_sv_root_dir, ts_to_active_mesh_sv_fn)
        np.save(ts_to_active_mesh_sv_fn, ts_to_active_mesh)
        
    def validate_mesh_robo_b(self, ):
        ts_to_active_mesh = {
            ts: self.cur_ts_to_optimized_visual_pts[ts] for ts in self.cur_ts_to_optimized_visual_pts
        }
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        ts_to_active_mesh_sv_fn = '{:0>8d}_ts_to_opt_act_pts.ply'.format(self.iter_step)
        ts_to_active_mesh_sv_fn = os.path.join(mesh_sv_root_dir, ts_to_active_mesh_sv_fn)
        np.save(ts_to_active_mesh_sv_fn, ts_to_active_mesh)
    
    
    def save_redmax_actions(self, ):
        
        # tot_redmax_actions
        redmax_act_sv_folder = os.path.join(self.base_exp_dir, 'checkpoint')
        os.makedirs(redmax_act_sv_folder, exist_ok=True)
        redmax_act_sv_fn = '{:0>8d}_redmax_act.npy'.format(self.iter_step)
        redmax_act_sv_fn = os.path.join(redmax_act_sv_folder, redmax_act_sv_fn)
        
        
        np.save(redmax_act_sv_fn, self.tot_redmax_actions.detach().cpu().numpy())
        
    
    def validate_mesh_robo_c(self, ):
        ts_to_active_mesh = {
            ts: self.tot_visual_pts[ts] for ts in range(self.tot_visual_pts.shape[0])
        }
        
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        ts_to_active_mesh_sv_fn = '{:0>8d}_ts_to_opt_intermediate_act_pts.npy'.format(self.iter_step)
        ts_to_active_mesh_sv_fn = os.path.join(mesh_sv_root_dir, ts_to_active_mesh_sv_fn)
        np.save(ts_to_active_mesh_sv_fn, ts_to_active_mesh)
        
        ts_to_ref_act_mesh = {
            ts: self.timestep_to_active_mesh_wo_glb[ts].detach().cpu().numpy() for ts in self.timestep_to_active_mesh_wo_glb
        }
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        ts_to_ref_act_mesh_sv_fn = '{:0>8d}_ts_to_ref_act_pts.npy'.format(self.iter_step)
        ts_to_ref_act_mesh_sv_fn = os.path.join(mesh_sv_root_dir, ts_to_ref_act_mesh_sv_fn)
        np.save(ts_to_ref_act_mesh_sv_fn, ts_to_ref_act_mesh)
        

    
    
    def validate_mesh_robo_e(self, ): # validate mesh robo # ## validate the #
        def merge_meshes(verts_list, faces_list):
            tot_verts_nn = 0
            merged_verts = []
            merged_faces = []
            for i_mesh in range(len(verts_list)):
                merged_verts.append(verts_list[i_mesh])
                merged_faces.append(faces_list[i_mesh] + tot_verts_nn) # 
                tot_verts_nn += verts_list[i_mesh].shape[0]
            merged_verts = np.concatenate(merged_verts, axis=0)
            merged_faces = np.concatenate(merged_faces, axis=0)
            return merged_verts, merged_faces
        
        # one single hand or # not very easy to 
        # self.hand_faces, self.obj_faces # # j
        mano_hand_faces_np = self.hand_faces.detach().cpu().numpy()
        hand_faces_np = self.robo_hand_faces.detach().cpu().numpy() # ### robot faces # 
        
        if self.use_mano_inputs:
            hand_faces_np = mano_hand_faces_np
        
        obj_faces_np = self.obj_faces.detach().cpu().numpy()
        
        
        # init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
        # init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        
        for i_ts in range(0, self.nn_ts, 1):
            cur_mano_rhand = self.rhand_verts[i_ts].detach().cpu().numpy()
            cur_hand_verts = self.timestep_to_active_mesh[i_ts].detach().cpu().numpy()
            # cur_hand_faces = self.
            obj_verts = self.timestep_to_passive_mesh[i_ts].detach().cpu().numpy()
            obj_faces = obj_faces_np
            hand_faces = hand_faces_np
            if i_ts % 10 == 0:
                merged_verts, merged_faces = merge_meshes([cur_hand_verts, obj_verts], [hand_faces, obj_faces])
                ## ## merged 
                mesh = trimesh.Trimesh(merged_verts, merged_faces)
                
                mesh_sv_fn = '{:0>8d}_ts_{:0>3d}.ply'.format(self.iter_step, i_ts)
                mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                
                ## i_ts % ## 
                # if i_ts % self.mano_nn_substeps == 0: ## 
                ### overlayed with the mano mesh ### ## 
                merged_verts, merged_faces = merge_meshes([cur_hand_verts, obj_verts, cur_mano_rhand], [hand_faces_np, obj_faces_np, mano_hand_faces_np])
                mesh = trimesh.Trimesh(merged_verts, merged_faces)
                mesh_sv_fn = '{:0>8d}_ts_{:0>3d}_wmano.ply'.format(self.iter_step, i_ts)
                mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                
                cur_hand_mesh = trimesh.Trimesh(cur_hand_verts, hand_faces)
                mesh_sv_fn = 'dyn_mano_hand_{:0>8d}_ts_{:0>3d}.obj'.format(self.iter_step, i_ts)
                cur_hand_mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                
                cur_obj_mesh = trimesh.Trimesh(obj_verts, obj_faces_np)
                mesh_sv_fn = 'obj_{:0>8d}_ts_{:0>3d}.obj'.format(self.iter_step, i_ts)
                cur_obj_mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                
                cur_mano_hand_mesh = trimesh.Trimesh(cur_mano_rhand, mano_hand_faces_np)
                mesh_sv_fn = 'kine_mano_hand_{:0>8d}_ts_{:0>3d}.obj'.format(self.iter_step, i_ts)
                cur_mano_hand_mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
        ## cur ts and the mano meshes ##
    
    
    def validate_mesh_robo_g(self, ): # validate mesh robo # ## validate the #
        def merge_meshes(verts_list, faces_list):
            tot_verts_nn = 0
            merged_verts = []
            merged_faces = []
            for i_mesh in range(len(verts_list)):
                merged_verts.append(verts_list[i_mesh])
                merged_faces.append(faces_list[i_mesh] + tot_verts_nn) # 
                tot_verts_nn += verts_list[i_mesh].shape[0]
            merged_verts = np.concatenate(merged_verts, axis=0)
            merged_faces = np.concatenate(merged_faces, axis=0)
            return merged_verts, merged_faces
        
        # one single hand or # not very easy to 
        # self.hand_faces, self.obj_faces 
        mano_hand_faces_np = self.hand_faces.detach().cpu().numpy()
        # hand_faces_np = self.robo_hand_faces.detach().cpu().numpy() # ### robot faces # 
        
        if self.use_mano_inputs: ## mano inputs ## ## validate mesh robo g ##
            hand_faces_np = mano_hand_faces_np
        
        obj_faces_np = self.obj_faces.detach().cpu().numpy()
        
        
        if self.other_bending_network.canon_passive_obj_verts is None:
            init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
            init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
        else:
            init_passive_obj_verts = self.other_bending_network.canon_passive_obj_verts.detach().cpu().numpy()
            init_passive_obj_verts_center = torch.zeros((3, )).cuda().detach().cpu().numpy()
        
        # init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
        # init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        ts_to_obj_quaternion = {}
        ts_to_obj_rot_mtx = {}
        ts_to_obj_trans = {}
        ts_to_hand_obj_verts = {}
        ts_to_transformed_obj_pts = {}
        # for i_ts in range(1, self.nn_timesteps - 1, 10):
        for i_ts in range(0, (self.nn_ts - 1) * self.mano_nn_substeps, 1):
            cur_hand_mesh = self.timestep_to_active_mesh[i_ts]
            if i_ts % self.mano_nn_substeps == 0:
                cur_mano_rhand = self.rhand_verts[i_ts // self.mano_nn_substeps].detach().cpu().numpy()
            cur_rhand_verts = cur_hand_mesh # [: cur_hand_mesh.size(0) // 2]
            # cur_lhand_verts = cur_hand_mesh # [cur_hand_mesh.size(0) // 2:]
            cur_rhand_verts_np = cur_rhand_verts.detach().cpu().numpy()
            # cur_lhand_verts_np = cur_lhand_verts.detach().cpu().numpy()
            if i_ts not in self.other_bending_network.timestep_to_optimizable_rot_mtx: # to optimizable rot mtx #
                cur_pred_rot_mtx = np.eye(3, dtype=np.float32)
                cur_pred_trans = np.zeros((3,), dtype=np.float32)
            else:
                cur_pred_rot_mtx = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
                cur_pred_trans = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            cur_transformed_obj = np.matmul(cur_pred_rot_mtx, (init_passive_obj_verts - init_passive_obj_verts_center).T).T + init_passive_obj_verts_center + np.reshape(cur_pred_trans, (1, 3))

            if i_ts not in self.other_bending_network.timestep_to_optimizable_quaternion:
                ts_to_obj_quaternion[i_ts] = np.zeros((4,), dtype=np.float32)
                ts_to_obj_quaternion[i_ts][0] = 1. # ## quaternion ## # 
                ts_to_obj_rot_mtx[i_ts] = np.eye(3, dtype=np.float32)
                ts_to_obj_trans[i_ts] = np.zeros((3,),  dtype=np.float32)
            else:
                ts_to_obj_quaternion[i_ts] = self.other_bending_network.timestep_to_optimizable_quaternion[i_ts].detach().cpu().numpy()
                ts_to_obj_rot_mtx[i_ts] = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
                ts_to_obj_trans[i_ts] = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            # 
            ts_to_hand_obj_verts[i_ts] = (cur_rhand_verts_np, cur_transformed_obj) # not correct.... #
            # tostotrans
            # merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_transformed_obj, cur_mano_rhand], [hand_faces_np, obj_faces_np, mano_hand_faces_np])
            # if i_ts % 10 == 0:
            #     # print(f"exporting meshes i_ts: {i_ts}, cur_hand_verts_np: {cur_rhand_verts_np.shape}, hand_faces_np: {hand_faces_np.shape}")
            #     merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_transformed_obj], [hand_faces_np, obj_faces_np])
            #     mesh = trimesh.Trimesh(merged_verts, merged_faces)
            #     mesh_sv_fn = '{:0>8d}_ts_{:0>3d}.ply'.format(self.iter_step, i_ts)
            #     mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                
            #     if i_ts % self.mano_nn_substeps == 0:
            #         ### overlayed with the mano mesh ###
            #         merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_transformed_obj, cur_mano_rhand], [hand_faces_np, obj_faces_np, mano_hand_faces_np])
            #         mesh = trimesh.Trimesh(merged_verts, merged_faces)
            #         mesh_sv_fn = '{:0>8d}_ts_{:0>3d}_wmano.ply'.format(self.iter_step, i_ts)
            #         mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                
        try:
            timestep_to_anchored_mano_pts = self.timestep_to_anchored_mano_pts
        except:
            timestep_to_anchored_mano_pts = {}
        
        try:
            timestep_to_raw_active_meshes = self.timestep_to_raw_active_meshes
        except:
            timestep_to_raw_active_meshes = {}
        
        try:
            ts_to_dyn_mano_pts = self.ts_to_dyn_mano_pts
        except:
            ts_to_dyn_mano_pts = {}
        ts_sv_dict = {
            'ts_to_obj_quaternion': ts_to_obj_quaternion,
            'ts_to_obj_rot_mtx': ts_to_obj_rot_mtx,
            'ts_to_obj_trans': ts_to_obj_trans,
            'ts_to_hand_obj_verts': ts_to_hand_obj_verts, # hand vertices, obj vertices #
            'obj_faces_np': obj_faces_np,
            'timestep_to_anchored_mano_pts': timestep_to_anchored_mano_pts, 
            'timestep_to_raw_active_meshes': timestep_to_raw_active_meshes, # ts to raw active meshes ##
            'ts_to_dyn_mano_pts': ts_to_dyn_mano_pts,
        }
        
        # if self.mode not in ['train_finger_retargeting', 'train_expanded_set_motions']:
        #     hand_obj_verts_faces_sv_dict = {
        #         'ts_to_hand_obj_verts': ts_to_hand_obj_verts,
        #         'hand_faces': hand_faces_np,
        #         'obj_faces': obj_faces_np
        #     }
              
        #     # verts faces #
        #     hand_obj_verts_faces_sv_dict_sv_fn = 'hand_obj_verts_faces_sv_dict_{:0>8d}.npy'.format(self.iter_step)
        #     hand_obj_verts_faces_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, hand_obj_verts_faces_sv_dict_sv_fn)
        #     np.save(hand_obj_verts_faces_sv_dict_sv_fn, hand_obj_verts_faces_sv_dict)
        #     # collision detection and # hand obj verts faces sv dict #
        
        timestep_to_mano_active_mesh = {ts: self.timestep_to_active_mesh[ts].detach().cpu().numpy() for ts in self.timestep_to_active_mesh}
        mano_sv_dict_sv_fn = 'mano_act_pts_{:0>8d}.npy'.format(self.iter_step)
        mano_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, mano_sv_dict_sv_fn)
        np.save(mano_sv_dict_sv_fn, timestep_to_mano_active_mesh)
            
        
        
        # try: # timestep to mnao active mesh #
        #     timestep_to_mano_active_mesh = {ts: self.timestep_to_mano_active_mesh[ts].detach().cpu().numpy() for ts in self.timestep_to_mano_active_mesh}
        #     mano_sv_dict_sv_fn = 'mano_act_pts_{:0>8d}.npy'.format(self.iter_step)
        #     mano_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, mano_sv_dict_sv_fn)
        #     np.save(mano_sv_dict_sv_fn, timestep_to_mano_active_mesh)
            
        #     # timestep_to_corr_mano_pts # 
        #     # timestep_to_mano_active_mesh # 
        #     timestep_to_corr_mano_pts = {ts: self.timestep_to_corr_mano_pts[ts].detach().cpu().numpy() for ts in self.timestep_to_corr_mano_pts}
        #     mano_sv_dict_sv_fn = 'corr_mano_act_pts_{:0>8d}.npy'.format(self.iter_step)
        #     mano_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, mano_sv_dict_sv_fn)
        #     np.save(mano_sv_dict_sv_fn, timestep_to_corr_mano_pts)
        # except:
        #     pass
        sv_dict_sv_fn = 'retar_info_dict_{:0>8d}.npy'.format(self.iter_step)
        sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, sv_dict_sv_fn)
        np.save(sv_dict_sv_fn, ts_sv_dict)
    
    def validate_mesh_robo_h(self, ): # validate mesh robo # ## validate the #
        def merge_meshes(verts_list, faces_list):
            tot_verts_nn = 0
            merged_verts = []
            merged_faces = []
            for i_mesh in range(len(verts_list)):
                merged_verts.append(verts_list[i_mesh])
                merged_faces.append(faces_list[i_mesh] + tot_verts_nn) # 
                tot_verts_nn += verts_list[i_mesh].shape[0]
            merged_verts = np.concatenate(merged_verts, axis=0)
            merged_faces = np.concatenate(merged_faces, axis=0)
            return merged_verts, merged_faces
        
        # one single hand or # not very easy to ## no
        # self.hand_faces, self.obj_faces
        mano_hand_faces_np = self.hand_faces.detach().cpu().numpy()
        hand_faces_np = self.robo_hand_faces.detach().cpu().numpy() # ### robot faces # 
        
        if self.use_mano_inputs:
            hand_faces_np = mano_hand_faces_np
            
        if self.optim_sim_model_params_from_mano:
            hand_faces_np = mano_hand_faces_np
        else:
            hand_faces_np_left  =  self.robot_agent_left.robot_faces.detach().cpu().numpy()
        
        obj_faces_np = self.obj_faces.detach().cpu().numpy()
        
        
        if self.other_bending_network.canon_passive_obj_verts is None:
            init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
            # init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
            init_passive_obj_verts_center = torch.zeros((3, )).cuda().detach().cpu().numpy()
        else:
            init_passive_obj_verts = self.other_bending_network.canon_passive_obj_verts.detach().cpu().numpy()
            init_passive_obj_verts_center = torch.zeros((3, )).cuda().detach().cpu().numpy()
        
        # init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
        # init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        ts_to_obj_quaternion = {}
        ts_to_obj_rot_mtx = {}
        ts_to_obj_trans = {}
        ts_to_hand_obj_verts = {}
        # for i_ts in range(1, self.nn_timesteps - 1, 10):
        for i_ts in range(0, (self.nn_ts - 1) * self.mano_nn_substeps, 1):
            if self.optim_sim_model_params_from_mano:
                cur_hand_mesh = self.rhand_verts[i_ts] # .detach().cpu().numpy()
            else:
                cur_hand_mesh = self.ts_to_act_rgt[i_ts]
            
            # cur_rhand_mano_gt, cur_lhand_mano_gt, gt_obj_pcs, re_trans_obj_pcs # 
            cur_rhand_mano_gt = self.rhand_verts[i_ts].detach().cpu().numpy()
            cur_lhand_mano_gt = self.lhand_verts[i_ts].detach().cpu().numpy() # 
            gt_obj_pcs = self.timestep_to_passive_mesh[i_ts].detach().cpu().numpy() # 
            re_trans_obj_pcs = self.re_transformed_obj_verts[i_ts].detach().cpu().numpy() # 
            
            # ts_to_mano_rhand_meshes
            if self.optim_sim_model_params_from_mano:
                cur_hand_mesh_left = self.lhand_verts[i_ts] ## retargeting ##
                cur_hand_mesh_faces_left = self.hand_faces.detach().cpu().numpy()
            else:
                cur_hand_mesh_left = self.ts_to_act_lft[i_ts]
                cur_hand_mesh_faces_left = self.robot_agent_left.robot_faces.detach().cpu().numpy()
            
            # cur_hand_mesh_left = self.ts_to_act_lft[i_ts]
            cur_hand_mesh_left_np = cur_hand_mesh_left.detach().cpu().numpy()
            
            if i_ts % self.mano_nn_substeps == 0:
                cur_mano_rhand = self.rhand_verts[i_ts // self.mano_nn_substeps].detach().cpu().numpy()
            cur_rhand_verts = cur_hand_mesh # [: cur_hand_mesh.size(0) // 2]
            # cur_lhand_verts = cur_hand_mesh # [cur_hand_mesh.size(0) // 2:]
            cur_rhand_verts_np = cur_rhand_verts.detach().cpu().numpy()
            # cur_lhand_verts_np = cur_lhand_verts.detach().cpu().numpy()
            if i_ts not in self.other_bending_network.timestep_to_optimizable_rot_mtx: # to optimizable rot mtx #
                cur_pred_rot_mtx = np.eye(3, dtype=np.float32)
                cur_pred_trans = np.zeros((3,), dtype=np.float32)
            else:
                cur_pred_rot_mtx = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
                cur_pred_trans = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            cur_transformed_obj = np.matmul(cur_pred_rot_mtx, (init_passive_obj_verts - init_passive_obj_verts_center).T).T + init_passive_obj_verts_center + np.reshape(cur_pred_trans, (1, 3))

            if i_ts not in self.other_bending_network.timestep_to_optimizable_quaternion:
                ts_to_obj_quaternion[i_ts] = np.zeros((4,), dtype=np.float32)
                ts_to_obj_quaternion[i_ts][0] = 1. # ## quaternion ## # 
                ts_to_obj_rot_mtx[i_ts] = np.eye(3, dtype=np.float32)
                ts_to_obj_trans[i_ts] = np.zeros((3,),  dtype=np.float32)
            else:
                ts_to_obj_quaternion[i_ts] = self.other_bending_network.timestep_to_optimizable_quaternion[i_ts].detach().cpu().numpy()
                ts_to_obj_rot_mtx[i_ts] = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
                ts_to_obj_trans[i_ts] = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            # 
            ts_to_hand_obj_verts[i_ts] = (cur_rhand_verts_np, cur_hand_mesh_left_np, cur_transformed_obj) # not correct.... #
            # cur_transformed_obj = self.timestep_to_passive_mesh[i_ts].detach().cpu().numpy()
            # merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_transformed_obj, cur_mano_rhand], [hand_faces_np, obj_faces_np, mano_hand_faces_np])
            if i_ts % 10 == 0:
                # print(f"exporting meshes i_ts: {i_ts}, cur_hand_verts_np: {cur_rhand_verts_np.shape}, hand_faces_np: {hand_faces_np.shape}")
                # print(f"cur_rhand_verts_np: {cur_rhand_verts_np.shape}, cur_hand_mesh_left_np: {cur_hand_mesh_left_np.shape}, cur_transformed_obj: {cur_transformed_obj.shape}, hand_faces_np: {hand_faces_np.shape}, obj_faces_np: {obj_faces_np.shape}")
                merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_hand_mesh_left_np, cur_transformed_obj], [hand_faces_np, cur_hand_mesh_faces_left,  obj_faces_np])
                # print(f"merged_verts: {merged_verts.shape}, merged_faces: {merged_faces.shape}")
                mesh = trimesh.Trimesh(merged_verts, merged_faces)
                mesh_sv_fn = '{:0>8d}_ts_{:0>3d}.ply'.format(self.iter_step, i_ts)
                mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                
                if i_ts % self.mano_nn_substeps == 0:
                    ### overlayed with the mano mesh ###
                    merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_hand_mesh_left_np, cur_transformed_obj, cur_mano_rhand], [hand_faces_np, cur_hand_mesh_faces_left, obj_faces_np, mano_hand_faces_np])
                    mesh = trimesh.Trimesh(merged_verts, merged_faces)
                    mesh_sv_fn = '{:0>8d}_ts_{:0>3d}_wmano.ply'.format(self.iter_step, i_ts)
                    mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                    
                    ### overlayed with the mano mesh ###
                    merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_hand_mesh_left_np, cur_mano_rhand], [hand_faces_np, cur_hand_mesh_faces_left, mano_hand_faces_np]) # 
                    mesh = trimesh.Trimesh(merged_verts, merged_faces)
                    mesh_sv_fn = '{:0>8d}_ts_{:0>3d}_onlyhand.ply'.format(self.iter_step, i_ts)
                    mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                    
                    # cur_rhand_mano_gt, cur_lhand_mano_gt, gt_obj_pcs, re_trans_obj_pcs # 
                    merged_verts, merged_faces = merge_meshes([cur_rhand_mano_gt, cur_lhand_mano_gt, gt_obj_pcs], [mano_hand_faces_np, mano_hand_faces_np, obj_faces_np])
                    mesh  = trimesh.Trimesh(merged_verts, merged_faces)
                    mesh_sv_fn = '{:0>8d}_ts_{:0>3d}_gt_mano_obj.ply'.format(self.iter_step, i_ts)
                    mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                    
                    merged_verts, merged_faces = merge_meshes([cur_rhand_mano_gt, cur_lhand_mano_gt, re_trans_obj_pcs], [mano_hand_faces_np, mano_hand_faces_np, obj_faces_np])
                    mesh  = trimesh.Trimesh(merged_verts, merged_faces)
                    mesh_sv_fn = '{:0>8d}_ts_{:0>3d}_gt_mano_retrans_obj.ply'.format(self.iter_step, i_ts)
                    mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
                    
                
                
        ts_sv_dict = {
            'ts_to_obj_quaternion': ts_to_obj_quaternion,
            'ts_to_obj_rot_mtx': ts_to_obj_rot_mtx,
            'ts_to_obj_trans': ts_to_obj_trans,
            'ts_to_hand_obj_verts': ts_to_hand_obj_verts,
            'hand_faces_np': hand_faces_np,
            'cur_hand_mesh_faces_left': cur_hand_mesh_faces_left,
            'obj_faces_np': obj_faces_np
        }
        
        # if self.mode not in ['train_finger_retargeting']:
        #     hand_obj_verts_faces_sv_dict = {
        #         'ts_to_hand_obj_verts': ts_to_hand_obj_verts,
        #         'hand_faces': hand_faces_np,
        #         'obj_faces': obj_faces_np
        #     }
            
        #     hand_obj_verts_faces_sv_dict_sv_fn = 'hand_obj_verts_faces_sv_dict_{:0>8d}.npy'.format(self.iter_step)
        #     hand_obj_verts_faces_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, hand_obj_verts_faces_sv_dict_sv_fn)
        #     np.save(hand_obj_verts_faces_sv_dict_sv_fn, hand_obj_verts_faces_sv_dict)
        #     # collision detection and # hand obj verts faces sv dict #
        # try:
        #     timestep_to_mano_active_mesh = {ts: self.timestep_to_mano_active_mesh[ts].detach().cpu().numpy() for ts in self.timestep_to_mano_active_mesh}
        #     mano_sv_dict_sv_fn = 'mano_act_pts_{:0>8d}.npy'.format(self.iter_step)
        #     mano_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, mano_sv_dict_sv_fn)
        #     np.save(mano_sv_dict_sv_fn, timestep_to_mano_active_mesh)
            
        #     timestep_to_robot_mesh = {ts: self.timestep_to_active_mesh[ts].detach().cpu().numpy() for ts in self.timestep_to_active_mesh}
        #     rhand_verts = self.rhand_verts.detach().cpu().numpy()
        #     #  ts_to_robot_fingers, ts_to_mano_fingers
        #     retar_sv_dict = {
        #         'timestep_to_robot_mesh': timestep_to_robot_mesh,
        #         'rhand_verts': rhand_verts,
        #         'ts_to_robot_fingers': self.ts_to_robot_fingers, 
        #         'ts_to_mano_fingers': self.ts_to_mano_fingers,
        #     }
        #     retar_sv_fn =  'retar_info_dict_{:0>8d}.npy'.format(self.iter_step)
        #     retar_sv_fn = os.path.join(mesh_sv_root_dir, retar_sv_fn)
        #     np.save(retar_sv_fn, retar_sv_dict)
        #     # mano_sv_dict_sv_fn = 'mano_act_pts_{:0>8d}.npy'.format(self.iter_step)
        #     # mano_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, mano_sv_dict_sv_fn)
        #     # np.save(mano_sv_dict_sv_fn, timestep_to_mano_active_mesh)
            
        #     # timestep_to_corr_mano_pts # 
        #     # timestep_to_mano_active_mesh # 
        #     timestep_to_corr_mano_pts = {ts: self.timestep_to_corr_mano_pts[ts].detach().cpu().numpy() for ts in self.timestep_to_corr_mano_pts}
        #     mano_sv_dict_sv_fn = 'corr_mano_act_pts_{:0>8d}.npy'.format(self.iter_step)
        #     mano_sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, mano_sv_dict_sv_fn)
        #     np.save(mano_sv_dict_sv_fn, timestep_to_corr_mano_pts)
        # except:
        #     pass
        sv_dict_sv_fn = '{:0>8d}.npy'.format(self.iter_step)
        sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, sv_dict_sv_fn)
        np.save(sv_dict_sv_fn, ts_sv_dict)
        
        # try:
        #     robo_intermediates_states_np = self.robo_intermediates_states.numpy()
        #     robo_intermediates_states_sv_fn = f"robo_intermediates_states_{self.iter_step}.npy"
        #     robo_intermediates_states_sv_fn = os.path.join(mesh_sv_root_dir, robo_intermediates_states_sv_fn)
        #     np.save(robo_intermediates_states_sv_fn, robo_intermediates_states_np) # 
        # except:
        #     pass
    
    
    def validate_contact_info_robo(self, ): # validate mesh robo #
        def merge_meshes(verts_list, faces_list):
            tot_verts_nn = 0
            merged_verts = []
            merged_faces = []
            for i_mesh in range(len(verts_list)):
                merged_verts.append(verts_list[i_mesh])
                merged_faces.append(faces_list[i_mesh] + tot_verts_nn) # and you
                tot_verts_nn += verts_list[i_mesh].shape[0]
            merged_verts = np.concatenate(merged_verts, axis=0)
            merged_faces = np.concatenate(merged_faces, axis=0)
            return merged_verts, merged_faces
        
        # one single hand or # not very easy to 
        # self.hand_faces, self.obj_faces # # j
        # mano_hand_faces_np = self.hand_faces.detach().cpu().numpy()
        # hand_faces_np = self.robo_hand_faces.detach().cpu().numpy() # ### robot faces # 
        # obj_faces_np = self.obj_faces.detach().cpu().numpy()
        init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
        init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        # ts_to_obj_quaternion = {}
        # ts_to_obj_rot_mtx = {}
        # ts_to_obj_trans = {}
        # ts_to_hand_obj_verts = {}
        ts_to_obj_verts = {}
        # for i_ts in range(1, self.nn_timesteps - 1, 10):
        for i_ts in range(0,( self.nn_ts - 1) * self.mano_nn_substeps, 1):
            cur_hand_mesh = self.timestep_to_active_mesh[i_ts]
            # cur_mano_rhand = self.rhand_verts[i_ts].detach().cpu().numpy() # 
            # cur_rhand_verts = cur_hand_mesh # [: cur_hand_mesh.size(0) // 2]
            # cur_lhand_verts = cur_hand_mesh # [cur_hand_mesh.size(0) // 2:]
            # cur_rhand_verts_np = cur_rhand_verts.detach().cpu().numpy()
            # cur_lhand_verts_np = cur_lhand_verts.detach().cpu().numpy()
            if i_ts not in self.other_bending_network.timestep_to_optimizable_rot_mtx:
                cur_pred_rot_mtx = np.eye(3, dtype=np.float32)
                cur_pred_trans = np.zeros((3,), dtype=np.float32)
            else:
                cur_pred_rot_mtx = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
                cur_pred_trans = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            cur_transformed_obj = np.matmul(cur_pred_rot_mtx, (init_passive_obj_verts - init_passive_obj_verts_center).T).T + init_passive_obj_verts_center + np.reshape(cur_pred_trans, (1, 3))

            # if i_ts not in self.other_bending_network.timestep_to_optimizable_quaternion:
            #     ts_to_obj_quaternion[i_ts] = np.zeros((4,), dtype=np.float32)
            #     ts_to_obj_quaternion[i_ts][0] = 1. # ## quaternion ## #
            #     ts_to_obj_rot_mtx[i_ts] = np.eye(3, dtype=np.float32)
            #     ts_to_obj_trans[i_ts] = np.zeros((3,),  dtype=np.float32)
            # else: # 
            #     ts_to_obj_quaternion[i_ts] = self.other_bending_network.timestep_to_optimizable_quaternion[i_ts].detach().cpu().numpy()
            #     ts_to_obj_rot_mtx[i_ts] = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
            #     ts_to_obj_trans[i_ts] = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            # 
            ts_to_obj_verts[i_ts] = cur_transformed_obj # cur_transformed_obj #
            # ts_to_hand_obj_verts[i_ts] = (cur_rhand_verts_np, cur_transformed_obj)
            # # merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_transformed_obj, cur_mano_rhand], [hand_faces_np, obj_faces_np, mano_hand_faces_np])
            # if i_ts % 10 == 0:
            #     merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_transformed_obj], [hand_faces_np, obj_faces_np])
            #     mesh = trimesh.Trimesh(merged_verts, merged_faces)
            #     mesh_sv_fn = '{:0>8d}_ts_{:0>3d}.ply'.format(self.iter_step, i_ts)
            #     mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
            
        # joint torque # joint torque #
        cur_sv_penetration_points_fn = os.path.join(self.base_exp_dir, "meshes", f"penetration_points_{self.iter_step}.npy")
        # timestep_to_raw_active_meshes, timestep_to_penetration_points, timestep_to_penetration_points_forces
        cur_timestep_to_accum_acc = {
            ts: self.other_bending_network.timestep_to_accum_acc[ts].detach().cpu().numpy() for ts in self.other_bending_network.timestep_to_accum_acc
        }
        # save penetration points # joint
        cur_sv_penetration_points = {
            'timestep_to_raw_active_meshes': self.timestep_to_raw_active_meshes,
            'timestep_to_penetration_points': self.timestep_to_penetration_points,
            'timestep_to_penetration_points_forces': self.timestep_to_penetration_points_forces,
            'ts_to_obj_verts': ts_to_obj_verts,
            'cur_timestep_to_accum_acc': cur_timestep_to_accum_acc
        }
        np.save(cur_sv_penetration_points_fn, cur_sv_penetration_points)
        
        # joint_name_to_penetration_forces_intermediates
        cur_sv_joint_penetration_intermediates_fn = os.path.join(self.base_exp_dir, "meshes", f"joint_penetration_intermediates_{self.iter_step}.npy")
        np.save(cur_sv_joint_penetration_intermediates_fn, self.joint_name_to_penetration_forces_intermediates)
        
        
        # [upd_contact_active_point_pts, upd_contact_point_position, (upd_contact_active_idxes, upd_contact_passive_idxes), (upd_contact_frame_rotations, upd_contact_frame_translations)]
        tot_contact_pairs_set = {}
        
        for ts in self.contact_pairs_sets:
            cur_contact_pairs_set = self.contact_pairs_sets[ts]
            if isinstance(cur_contact_pairs_set, dict):
                tot_contact_pairs_set[ts] = {
                    'upd_contact_active_idxes': cur_contact_pairs_set['contact_active_idxes'].detach().cpu().numpy(),
                    'upd_contact_passive_pts': cur_contact_pairs_set['contact_passive_pts'].detach().cpu().numpy(), # in contact indexes #
                }
            else:
                [upd_contact_active_point_pts, upd_contact_point_position, (upd_contact_active_idxes, upd_contact_passive_idxes), (upd_contact_frame_rotations, upd_contact_frame_translations)] = cur_contact_pairs_set
                tot_contact_pairs_set[ts] = {
                    'upd_contact_active_idxes': upd_contact_active_idxes.detach().cpu().numpy(),
                    'upd_contact_passive_idxes': upd_contact_passive_idxes.detach().cpu().numpy(), # in contact indexes #
                    
                }
        # sampled_verts_idxes
        tot_contact_pairs_set['sampled_verts_idxes'] = self.sampled_verts_idxes.detach().cpu().numpy()
        tot_contact_pairs_set_fn = os.path.join(self.base_exp_dir, "meshes", f"tot_contact_pairs_set_{self.iter_step}.npy")
        np.save(tot_contact_pairs_set_fn, tot_contact_pairs_set)
        
        # self.ts_to_contact_force_d[cur_ts] = self.other_bending_network.contact_force_d.detach().cpu().numpy()
        #         self.ts_to_penalty_frictions[cur_ts] = self.other_bending_network.penalty_friction_tangential_forces.detach().cpu().numpy()
            
        contact_forces_dict = {
            'ts_to_contact_force_d': self.ts_to_contact_force_d, 
            'ts_to_penalty_frictions': self.ts_to_penalty_frictions,
            'ts_to_penalty_disp_pts': self.ts_to_penalty_disp_pts,
            'cur_timestep_to_accum_acc': cur_timestep_to_accum_acc,
            'ts_to_passive_normals': self.ts_to_passive_normals,
            'ts_to_passive_pts': self.ts_to_passive_pts,
            'ts_to_contact_passive_normals': self.ts_to_contact_passive_normals, ## the normal directions of the inc contact apssive object points #
        }
        contact_forces_dict_sv_fn = os.path.join(self.base_exp_dir, "meshes", f"contact_forces_dict_{self.iter_step}.npy")
        np.save(contact_forces_dict_sv_fn, contact_forces_dict)
        
        # [upd_contact_active_point_pts, upd_contact_point_position, (upd_contact_active_idxes, upd_contact_passive_idxes), (upd_contact_frame_rotations, upd_contact_frame_translations)] = self.contact_pairs_set
        # contact_pairs_info = {
        #     'upd_contact_active_idxes': upd_contact_active_idxes.detach().cpu().numpy()
        # }
        
        
        
    # train_robot_actions_from_mano_model_rules
    def validate_mesh_expanded_pts(self, ):
        # one single hand or
        # self.hand_faces, self.obj_faces #
        mano_hand_faces_np = self.hand_faces.detach().cpu().numpy()
        # validate_mesh_expanded_pts = self.faces.detach().cpu().numpy() # ### robot faces # 
        # obj_faces_np = self.obj_faces.detach().cpu().numpy()
        init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
        init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        ts_to_obj_quaternion = {}
        ts_to_obj_rot_mtx = {}
        ts_to_obj_trans = {}
        ts_to_transformed_obj = {}
        ts_to_active_pts = {}
        ts_to_mano_hand_verts = {}
        # for i_ts in range(1, self.nn_timesteps - 1, 10):
        for i_ts in range(0, self.nn_ts, 10):
            cur_hand_mesh = self.timestep_to_active_mesh[i_ts]
            cur_mano_rhand = self.rhand_verts[i_ts].detach().cpu().numpy()
            cur_rhand_verts = cur_hand_mesh # [: cur_hand_mesh.size(0) // 2]
            # cur_lhand_verts = cur_hand_mesh # [cur_hand_mesh.size(0) // 2:]
            cur_rhand_verts_np = cur_rhand_verts.detach().cpu().numpy()
            # cur_lhand_verts_np = cur_lhand_verts.detach().cpu().numpy()
            if i_ts not in self.other_bending_network.timestep_to_optimizable_rot_mtx:
                cur_pred_rot_mtx = np.eye(3, dtype=np.float32)
                cur_pred_trans = np.zeros((3,), dtype=np.float32)
            else:
                cur_pred_rot_mtx = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
                cur_pred_trans = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            cur_transformed_obj = np.matmul(cur_pred_rot_mtx, (init_passive_obj_verts - init_passive_obj_verts_center).T).T + init_passive_obj_verts_center + np.reshape(cur_pred_trans, (1, 3))

            if i_ts not in self.other_bending_network.timestep_to_optimizable_quaternion:
                ts_to_obj_quaternion[i_ts] = np.zeros((4,), dtype=np.float32)
                ts_to_obj_quaternion[i_ts][0] = 1. # ## quaternion ## # 
                ts_to_obj_rot_mtx[i_ts] = np.eye(3, dtype=np.float32)
                ts_to_obj_trans[i_ts] = np.zeros((3,),  dtype=np.float32)
            else:
                ts_to_obj_quaternion[i_ts] = self.other_bending_network.timestep_to_optimizable_quaternion[i_ts].detach().cpu().numpy()
                ts_to_obj_rot_mtx[i_ts] = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
                ts_to_obj_trans[i_ts] = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            # 
            ts_to_transformed_obj[i_ts] = cur_transformed_obj # .detach().cpu().numpy()
            ts_to_active_pts[i_ts] = cur_hand_mesh.detach().cpu().numpy()
            ts_to_mano_hand_verts[i_ts] = cur_mano_rhand
            # merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_transformed_obj, cur_mano_rhand], [hand_faces_np, obj_faces_np, mano_hand_faces_np])
            # mesh = trimesh.Trimesh(merged_verts, merged_faces)
            # mesh_sv_fn = '{:0>8d}_ts_{:0>3d}.ply'.format(self.iter_step, i_ts)
            # mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn)) # mesh_sv_fn #
        ts_sv_dict = {
            'ts_to_obj_quaternion': ts_to_obj_quaternion,
            'ts_to_obj_rot_mtx': ts_to_obj_rot_mtx,
            'ts_to_obj_trans': ts_to_obj_trans,
            'ts_to_transformed_obj': ts_to_transformed_obj,
            'ts_to_active_pts': ts_to_active_pts,
            'ts_to_mano_hand_verts': ts_to_mano_hand_verts,
            'hand_faces': mano_hand_faces_np,
            # 'fobj_faces': obj_faces_np,
        }
        sv_dict_sv_fn = '{:0>8d}.npy'.format(self.iter_step)
        sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, sv_dict_sv_fn)
        np.save(sv_dict_sv_fn, ts_sv_dict)
    
    def validate_mesh_mano(self, ):
        def merge_meshes(verts_list, faces_list):
            tot_verts_nn = 0
            merged_verts = []
            merged_faces = []
            for i_mesh in range(len(verts_list)):
                merged_verts.append(verts_list[i_mesh])
                merged_faces.append(faces_list[i_mesh] + tot_verts_nn) # and you
                tot_verts_nn += verts_list[i_mesh].shape[0]
            merged_verts = np.concatenate(merged_verts, axis=0)
            merged_faces = np.concatenate(merged_faces, axis=0)
            return merged_verts, merged_faces
        
        # one single hand or
        # self.hand_faces, self.obj_faces #
        mano_hand_faces_np = self.hand_faces.detach().cpu().numpy()
        hand_faces_np = self.faces.detach().cpu().numpy() # ### robot faces # 
        obj_faces_np = self.obj_faces.detach().cpu().numpy()
        init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
        init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        ts_to_obj_quaternion = {}
        ts_to_obj_rot_mtx = {}
        ts_to_obj_trans = {}
        # for i_ts in range(1, self.nn_timesteps - 1, 10):
        for i_ts in range(0, self.nn_ts, 10):
            cur_hand_mesh = self.timestep_to_active_mesh[i_ts]
            cur_mano_rhand = self.rhand_verts[i_ts].detach().cpu().numpy()
            cur_rhand_verts = cur_hand_mesh # [: cur_hand_mesh.size(0) // 2]
            # cur_lhand_verts = cur_hand_mesh # [cur_hand_mesh.size(0) // 2:]
            cur_rhand_verts_np = cur_rhand_verts.detach().cpu().numpy()
            # cur_lhand_verts_np = cur_lhand_verts.detach().cpu().numpy()
            if i_ts not in self.other_bending_network.timestep_to_optimizable_rot_mtx:
                cur_pred_rot_mtx = np.eye(3, dtype=np.float32)
                cur_pred_trans = np.zeros((3,), dtype=np.float32)
            else:
                cur_pred_rot_mtx = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
                cur_pred_trans = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            cur_transformed_obj = np.matmul(cur_pred_rot_mtx, (init_passive_obj_verts - init_passive_obj_verts_center).T).T + init_passive_obj_verts_center + np.reshape(cur_pred_trans, (1, 3))

            if i_ts not in self.other_bending_network.timestep_to_optimizable_quaternion:
                ts_to_obj_quaternion[i_ts] = np.zeros((4,), dtype=np.float32)
                ts_to_obj_quaternion[i_ts][0] = 1. # ## quaternion ## # 
                ts_to_obj_rot_mtx[i_ts] = np.eye(3, dtype=np.float32)
                ts_to_obj_trans[i_ts] = np.zeros((3,),  dtype=np.float32)
            else:
                ts_to_obj_quaternion[i_ts] = self.other_bending_network.timestep_to_optimizable_quaternion[i_ts].detach().cpu().numpy()
                ts_to_obj_rot_mtx[i_ts] = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
                ts_to_obj_trans[i_ts] = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            # 
            merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_transformed_obj, cur_mano_rhand], [hand_faces_np, obj_faces_np, mano_hand_faces_np])
            mesh = trimesh.Trimesh(merged_verts, merged_faces)
            mesh_sv_fn = '{:0>8d}_ts_{:0>3d}.ply'.format(self.iter_step, i_ts)
            mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
        ts_sv_dict = {
            'ts_to_obj_quaternion': ts_to_obj_quaternion,
            'ts_to_obj_rot_mtx': ts_to_obj_rot_mtx,
            'ts_to_obj_trans': ts_to_obj_trans,
        }
        sv_dict_sv_fn = '{:0>8d}.npy'.format(self.iter_step)
        sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, sv_dict_sv_fn)
        np.save(sv_dict_sv_fn, ts_sv_dict)
        

    def validate_mesh(self, world_space=False, resolution=64, threshold=0.0):
        def merge_meshes(verts_list, faces_list):
            tot_verts_nn = 0
            merged_verts = []
            merged_faces = []
            for i_mesh in range(len(verts_list)):
                merged_verts.append(verts_list[i_mesh])
                merged_faces.append(faces_list[i_mesh] + tot_verts_nn) # and you
                tot_verts_nn += verts_list[i_mesh].shape[0]
            merged_verts = np.concatenate(merged_verts, axis=0)
            merged_faces = np.concatenate(merged_faces, axis=0)
            return merged_verts, merged_faces
        
        
        
        if self.train_multi_seqs:
            # seq_idx = torch.randint(low=0, high=len(self.rhand_verts), size=(1,)).item()
            seq_idx = self.seq_idx
            cur_hand_faces = self.hand_faces[seq_idx]
            cur_obj_faces = self.obj_faces[seq_idx]
            timestep_to_passive_mesh = self.obj_verts[seq_idx]
            timestep_to_active_mesh = self.hand_verts[seq_idx]
        else:
            cur_hand_faces = self.hand_faces
            cur_obj_faces = self.obj_faces
            timestep_to_passive_mesh = self.timestep_to_passive_mesh
            timestep_to_active_mesh = self.timestep_to_active_mesh
            
        
        # one single hand or
        # self.hand_faces, self.obj_faces #
        # hand_faces_np = self.hand_faces.detach().cpu().numpy()
        # obj_faces_np = self.obj_faces.detach().cpu().numpy()
        # init_passive_obj_verts = self.timestep_to_passive_mesh[0].detach().cpu().numpy()
        
        hand_faces_np = cur_hand_faces.detach().cpu().numpy()
        obj_faces_np = cur_obj_faces.detach().cpu().numpy()
        init_passive_obj_verts = timestep_to_passive_mesh[0].detach().cpu().numpy()
        init_passive_obj_verts_center = np.mean(init_passive_obj_verts, axis=0, keepdims=True)
        mesh_sv_root_dir = os.path.join(self.base_exp_dir, 'meshes')
        os.makedirs(mesh_sv_root_dir, exist_ok=True)
        ts_to_obj_quaternion = {}
        ts_to_obj_rot_mtx = {}
        ts_to_obj_trans = {}
        for i_ts in range(1, self.n_timesteps, 10):
            # cur_hand_mesh = self.timestep_to_active_mesh[i_ts]
            cur_hand_mesh = timestep_to_active_mesh[i_ts]
            cur_rhand_verts = cur_hand_mesh[: cur_hand_mesh.size(0) // 2]
            cur_lhand_verts = cur_hand_mesh[cur_hand_mesh.size(0) // 2: ]
            cur_rhand_verts_np = cur_rhand_verts.detach().cpu().numpy()
            cur_lhand_verts_np = cur_lhand_verts.detach().cpu().numpy()
            cur_pred_rot_mtx = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
            cur_pred_trans = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            cur_transformed_obj = np.matmul(cur_pred_rot_mtx, (init_passive_obj_verts - init_passive_obj_verts_center).T).T + init_passive_obj_verts_center + np.reshape(cur_pred_trans, (1, 3))

            ts_to_obj_quaternion[i_ts] = self.other_bending_network.timestep_to_optimizable_quaternion[i_ts].detach().cpu().numpy()
            ts_to_obj_rot_mtx[i_ts] = self.other_bending_network.timestep_to_optimizable_rot_mtx[i_ts].detach().cpu().numpy()
            ts_to_obj_trans[i_ts] = self.other_bending_network.timestep_to_optimizable_total_def[i_ts].detach().cpu().numpy()
            # 
            merged_verts, merged_faces = merge_meshes([cur_rhand_verts_np, cur_lhand_verts_np, cur_transformed_obj], [hand_faces_np, hand_faces_np, obj_faces_np])
            mesh = trimesh.Trimesh(merged_verts, merged_faces)
            mesh_sv_fn = '{:0>8d}_ts_{:0>3d}.ply'.format(self.iter_step, i_ts)
            mesh.export(os.path.join(mesh_sv_root_dir, mesh_sv_fn))
        ts_sv_dict = {
            'ts_to_obj_quaternion': ts_to_obj_quaternion,
            'ts_to_obj_rot_mtx': ts_to_obj_rot_mtx,
            'ts_to_obj_trans': ts_to_obj_trans,
        }
        sv_dict_sv_fn = '{:0>8d}.npy'.format(self.iter_step)
        sv_dict_sv_fn = os.path.join(mesh_sv_root_dir, sv_dict_sv_fn)
        np.save(sv_dict_sv_fn, ts_sv_dict)
        
    

if __name__ == '__main__':
    print('Hello Wooden')

    torch.set_default_tensor_type('torch.cuda.FloatTensor')

    FORMAT = "[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s"
    logging.basicConfig(level=logging.DEBUG, format=FORMAT)

    parser = argparse.ArgumentParser()
    parser.add_argument('--conf', type=str, default='./confs/base.conf')
    parser.add_argument('--mode', type=str, default='train')
    parser.add_argument('--mcube_threshold', type=float, default=0.0)
    parser.add_argument('--is_continue', default=False, action="store_true")
    parser.add_argument('--gpu', type=int, default=0)
    parser.add_argument('--case', type=str, default='')

    args = parser.parse_args()

    torch.cuda.set_device(args.gpu)
    runner = Runner(args.conf, args.mode, args.case, args.is_continue)

    bending_net_type = runner.conf['model.bending_net_type']
    

    if args.mode == 'train':
        runner.train()
    # elif args.mode == 'train_def': # 
    #     runner.train_def()
    # elif args.mode == 'train_from_model_rules': # from model rules #
    #     runner.train_from_model_rules() # 
    # elif args.mode == 'train_sdf_from_model_rules':
    #     runner.train_sdf_from_model_rules()
    # elif args.mode == 'train_actions_from_model_rules':
    #     runner.train_actions_from_model_rules()
    # elif args.mode == 'train_mano_actions_from_model_rules':
    #     runner.train_mano_actions_from_model_rules() ##
    # elif args.mode == 'train_actions_from_mano_model_rules':
    #     # runner.train_actions_from_mano_model_rules()
    #     # runner.train_robot_actions_from_mano_model_rules()
    #     # runner.train_robot_actions_from_mano_model_rules_v2()
        
    #         # runner.train_robot_actions_from_mano_model_rules_v4()
    #     # else:
    #     runner.train_robot_actions_from_mano_model_rules_v3() ##
    elif args.mode == 'train_real_robot_actions_from_mano_model_rules': ##
        # runner.train_real_robot_actions_from_mano_model_rules() ##
        # runner.train_real_robot_actions_from_mano_model_rules_v2() ##
        if bending_net_type in ["active_force_field_v15", 'active_force_field_v16']:
            runner.train_real_robot_actions_from_mano_model_rules_v4()
        elif bending_net_type in ["active_force_field_v17"]:
            runner.train_real_robot_actions_from_mano_model_rules_v5() ### train the mano model rules v5 ##
        else: 
            runner.train_real_robot_actions_from_mano_model_rules_v3() ##
        # runner.train_real_robot_actions_from_mano_model_rules_v4() ## train the model rules v4 ##
    
    elif args.mode ==  'train_real_robot_actions_from_mano_model_rules_diffhand':
        runner.train_real_robot_actions_from_mano_model_rules_v5_diffhand() ### trai
    
    elif args.mode ==  'train_real_robot_actions_from_mano_model_rules_diffhand_fortest':
        # train_real_robot_actions_from_mano_model_rules_v5_diffhand_fortest
        runner.train_real_robot_actions_from_mano_model_rules_v5_diffhand_fortest()
    
    elif args.mode == 'train_real_robot_actions_from_mano_model_rules_manohand_fortest':
        # runner.
        runner.train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest()
        # pass
    elif args.mode == 'train_real_robot_actions_from_mano_model_rules_manohand_fortest_states':
        runner.train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states()
    # elif args.mode == "train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states_res_world":
    #     runner.train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states_res_world()
    
    # elif args.mode == "train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states_res_rl":
    #     runner.train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states_res_rl() ## res rl for test states ##
    
    elif args.mode == "train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states_grab":
        runner.train_real_robot_actions_from_mano_model_rules_v5_manohand_fortest_states_grab()
    
    ########## Dynamic MANO optimization ##########
    elif args.mode == "train_dyn_mano_model_states":
        runner.train_dyn_mano_model_states() # 
        
    # ########## Retargeting -- Expanded set motion ##########
    # elif args.mode == "train_expanded_set_motions":
    #     runner.train_expanded_set_motions()
    
    # ########## Retargeting -- Expanded set motion retargeting ##########
    # elif args.mode == "train_expanded_set_motions_retar":
    #     runner.train_expanded_set_motions_retargeting()
    
    # ########## Retargeting -- GRAB & TACO ##########
    # elif args.mode == 'train_finger_retargeting':
    #     runner.train_finger_kinematics_retargeting()
    
    # ########## Retargeting -- ARCTIC ##########
    # elif args.mode == "train_finger_kinematics_retargeting_arctic_twohands":
    #     runner.train_finger_kinematics_retargeting_arctic_twohands()
        
    ########## GRAB & TACO ##########
    elif args.mode == "train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab":
        # runner.train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab()
        if runner.use_multi_stages: # try the actons 
            runner.train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab_multi_stages()
        else:
            runner.train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_grab()
    
    
    elif args.mode == "train_diffhand_model":
        runner.train_diffhand_model()
        
    elif args.mode == "train_manip_acts_params":
        if runner.use_multi_stages:
            runner.train_manip_acts_params_curriculum()
        else:
            runner.train_manip_acts_params()
    
    # ########## ARCTIC ##########
    # elif args.mode == "train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_arctic_twohands":
    #     if runner.use_multi_stages: # try the
    #         runner.train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_arctic_twohands_multi_stages()
    #     else:
    #         runner.train_real_robot_actions_from_mano_model_rules_v5_shadowhand_fortest_states_arctic_twohands()
    
    # elif args.mode == "train_diffhand_model":
    #     runner.train_diffhand_model()
    
    # ''' Step 1: train diffhand model via kinematics articulated states '''
    # elif args.mode == "train_diffhand_model":
    #     runner.train_diffhand_model()
        
        
    # ''' Step 2: train manip acts and parameters via a contact model curriculum '''
    # elif args.mode == "train_manip_acts_params":
    #     if runner.use_multi_stages:
    #         runner.train_manip_acts_params_curriculum()
    #     else:
    #         runner.train_manip_acts_params()
        
